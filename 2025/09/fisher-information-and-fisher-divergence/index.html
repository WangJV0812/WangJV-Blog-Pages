<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fisher Information and Fisher Divergence | WangJV Blog</title><meta name=keywords content="数学,概率"><meta name=description content="在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。

Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。
Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。

下面则给出一些不那么直观的，数学形式上的解释。
1. Statistical Manifold
和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。
那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。
2. Score Function
对于一个受到参数 $\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \theta)$，我们可以定义其 Score Function：
$$
\begin{gather}
s_\theta(x, \theta) = \nabla_\theta \log q(x; \theta)\\
s_x(x, \theta) = \nabla_x \log q(x; \theta)\\
\end{gather}
$$对于 score function，我们可以从两个 level 理解它。
首先，直观的、几何的讲，对于 score function $s_x(x, \theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\theta(x, \theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\theta$ 的概率密度对数增长最快的方向。"><meta name=author content="WangJV"><link rel=canonical href=https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/><link crossorigin=anonymous href=https://wangjv0812.cn/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://wangjv0812.cn/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wangjv0812.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangjv0812.cn/favicon-32x32.png><link rel=apple-touch-icon href=https://wangjv0812.cn/apple-touch-icon.png><link rel=mask-icon href=https://wangjv0812.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},chtml:{scale:1,minScale:.5,matchFontHeight:!1,displayAlign:"center",displayIndent:"0",mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},svg:{scale:1,minScale:.5,mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},options:{enableMenu:!0,menuOptions:{settings:{zoom:"Click"}}},loader:{load:["ui/safe","a11y/assistive-mml"]},startup:{ready(){MathJax.startup.defaultReady();const e=new ResizeObserver(e=>{MathJax.typesetPromise()});e.observe(document.body)}}},window.innerWidth<=768&&(MathJax.chtml=MathJax.chtml||{},MathJax.chtml.scale=.9)</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><style>.MathJax{outline:0}@media(max-width:768px){.MathJax{font-size:90%!important}.MathJax_Display{overflow-x:auto;overflow-y:hidden;padding:0!important;margin:1em 0!important}.MathJax_CHTML{line-height:1.2!important}}mjx-container[jax=CHTML][display=true]{overflow-x:auto;overflow-y:hidden;padding:1px 0}</style><meta property="og:url" content="https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/"><meta property="og:site_name" content="WangJV Blog"><meta property="og:title" content="Fisher Information and Fisher Divergence"><meta property="og:description" content="在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。
Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。 Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。 下面则给出一些不那么直观的，数学形式上的解释。
1. Statistical Manifold 和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。
那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。
2. Score Function 对于一个受到参数 $\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \theta)$，我们可以定义其 Score Function：
$$ \begin{gather} s_\theta(x, \theta) = \nabla_\theta \log q(x; \theta)\\ s_x(x, \theta) = \nabla_x \log q(x; \theta)\\ \end{gather} $$对于 score function，我们可以从两个 level 理解它。
首先，直观的、几何的讲，对于 score function $s_x(x, \theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\theta(x, \theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\theta$ 的概率密度对数增长最快的方向。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-13T17:13:56+08:00"><meta property="article:modified_time" content="2025-09-13T17:13:56+08:00"><meta property="article:tag" content="数学"><meta property="article:tag" content="概率"><meta property="og:image" content="https://wangjv0812.cn/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wangjv0812.cn/"><meta name=twitter:title content="Fisher Information and Fisher Divergence"><meta name=twitter:description content="在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。

Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。
Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。

下面则给出一些不那么直观的，数学形式上的解释。
1. Statistical Manifold
和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。
那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。
2. Score Function
对于一个受到参数 $\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \theta)$，我们可以定义其 Score Function：
$$
\begin{gather}
s_\theta(x, \theta) = \nabla_\theta \log q(x; \theta)\\
s_x(x, \theta) = \nabla_x \log q(x; \theta)\\
\end{gather}
$$对于 score function，我们可以从两个 level 理解它。
首先，直观的、几何的讲，对于 score function $s_x(x, \theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\theta(x, \theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\theta$ 的概率密度对数增长最快的方向。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wangjv0812.cn/posts/"},{"@type":"ListItem","position":2,"name":"Fisher Information and Fisher Divergence","item":"https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fisher Information and Fisher Divergence","name":"Fisher Information and Fisher Divergence","description":"在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。\nFisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。 Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。 下面则给出一些不那么直观的，数学形式上的解释。\n1. Statistical Manifold 和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\\sigma^2, \\mu$ 控制。那么所有的高斯分布的参数 $\\sigma^2, \\mu$ 所构成的空间便形成一个 “统计流形”。\n那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。\n2. Score Function 对于一个受到参数 $\\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \\theta)$，我们可以定义其 Score Function：\n$$ \\begin{gather} s_\\theta(x, \\theta) = \\nabla_\\theta \\log q(x; \\theta)\\\\ s_x(x, \\theta) = \\nabla_x \\log q(x; \\theta)\\\\ \\end{gather} $$对于 score function，我们可以从两个 level 理解它。\n首先，直观的、几何的讲，对于 score function $s_x(x, \\theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \\theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \\theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \\theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\\theta(x, \\theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\\theta$ 的概率密度对数增长最快的方向。\n","keywords":["数学","概率"],"articleBody":"在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。\nFisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。 Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。 下面则给出一些不那么直观的，数学形式上的解释。\n1. Statistical Manifold 和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\\sigma^2, \\mu$ 控制。那么所有的高斯分布的参数 $\\sigma^2, \\mu$ 所构成的空间便形成一个 “统计流形”。\n那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。\n2. Score Function 对于一个受到参数 $\\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \\theta)$，我们可以定义其 Score Function：\n$$ \\begin{gather} s_\\theta(x, \\theta) = \\nabla_\\theta \\log q(x; \\theta)\\\\ s_x(x, \\theta) = \\nabla_x \\log q(x; \\theta)\\\\ \\end{gather} $$对于 score function，我们可以从两个 level 理解它。\n首先，直观的、几何的讲，对于 score function $s_x(x, \\theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \\theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \\theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \\theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\\theta(x, \\theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\\theta$ 的概率密度对数增长最快的方向。\n但是这个直观的几何理解并没有解释为什么其中包含一个 $\\log$ 的形式，为什么一定是 $\\nabla \\log$ 而非 $\\nabla$。如果你的数学直觉比较强的化可能已经意识到了，这关联到了对于一个概率分布 信息 的衡量。但是如果想要深刻的理解这个问题，涉及到 信息几何 和 微分几何方面的知识。我会尽量在工科数学的范围内给出一些直观的解释。我们不妨先思考一下，直接使用 $\\nabla_xq(x)$ 为什么不够好？不妨思考一个简单的 高斯分布。我们分别在高概率位置 $x_1$ 和 低概率位置 $x_2$ 采样，计算 $\\nabla_\\theta q(x_1)$ 和 $\\nabla_\\theta q(x_2)$。那么自然的：\n对于 $x_1，q(x_1)$ 很大，它的梯度 $\\nabla_\\theta q(x_1, \\theta)$ 可能也比较大。 对于 $x_2，q(x_2)$ 几乎为零，它的梯度 $\\nabla_\\theta q(x_2, \\theta)$ 也几乎为零。 这导致尾部的、意外的、低概率的数据点 $x_2$ 对模型参数 $\\theta$ 几乎没有贡献。但是这是完全违背了统计直觉。从信息论角度分析，意外的、小概率的事件包含更多的信息，它们应该对参数调整产生更大的影响才对。信息几何告诉我们，由参数 $\\theta$ 定义的概率分布族（例如，所有可能的高斯分布）构成了一个空间，但这不是一个平坦的欧几里得空间，而是一个弯曲的统计流形。此时如果直接使用欧式几何的距离定义并不能争取的反应在参数流形上的距离关系。我们不加证明的给出结论，通过 “信息化转化算子” $\\log$，抵消了概率变化导致的对参数空间的 “扭曲” 影响。\n简而言之，从信息几何的角度出发，$\\log$ 的加入将原本非平直的参数空间 “压平” 了。这样让在参数空间上直接通过类似于欧氏距离的方式度量分布成为了可能。$\\log$ 是我们对抗参数空间内在扭曲性、将其“拉平”以便于我们理解和优化的第一步，也是最关键的一步。\n3. Fisher Information Matrix 我们已经理解了 Score Function，Fisher Information Matrix 则是定义在参数 Score Function $s_\\theta(x, \\theta)$ 的协方差矩阵：\n$$ \\begin{gather} I(\\theta) = \\mathbb{E}_{x\\sim q(x, \\theta)}\\big[ s_\\theta(x, \\theta) s_\\theta(x, \\theta)^\\top \\big] \\end{gather} $$但是这个形式不够直观，我们不妨将其变为一个更加常见的形式。但是展具体的证明前，需要先给出一个引理的证明：\nLemma 1. $\\mathbb E_{x\\sim q(x, \\theta)}\\big[s_\\theta(x, \\theta)\\big] = 0$\n有:\n$$ \\begin{aligned} \u0026\\nabla_\\theta \\log q(x, \\theta) = \\frac{\\nabla_\\theta q(x, \\theta)}{q(x, \\theta)}\\\\ \\longrightarrow \u0026 q(x, \\theta) \\nabla_\\theta \\log q(x, \\theta) = \\nabla_\\theta q(x, \\theta)\\\\ \\end{aligned} $$可以推出：\n$$ \\begin{gather} \\begin{aligned} \\mathbb E_{x\\sim q(x, \\theta)}\\big[s_\\theta(x, \\theta)] \u0026= \\int q(x, \\theta) \\nabla_\\theta \\log q(x, \\theta) dx\\\\ \u0026= \\int \\nabla_\\theta q(x, \\theta) dx\\\\ \\end{aligned} \\end{gather} $$在温和的条件下，可以交换 $\\nabla$ 和 $\\int$，有：\n$$ \\begin{gather} \\begin{aligned} \\mathbb E_{x\\sim q(x, \\theta)}\\big[s_\\theta(x, \\theta)] \u0026= \\nabla_\\theta \\int q(x, \\theta) dx\\\\ \u0026= \\nabla_\\theta 1\\\\ \u0026= 0 \\end{aligned} \\end{gather} $$那么从 Lemma 1. 出发，等式两边增加梯度算子 $\\nabla_\\theta$：\n$$ \\begin{gather} \\int q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\nabla_\\theta \\int q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\int \\nabla_\\theta \\bigg(q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) \\bigg)dx = 0\\\\ \\int \\nabla_\\theta q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) + q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\int \\nabla_\\theta q(x, \\theta)\\nabla_\\theta \\log q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) + q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\end{gather} $$分配积分符号，有：\n$$ \\begin{gather} \\int \\nabla_\\theta q(x, \\theta)\\nabla_\\theta \\log q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx + \\int q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\end{gather} $$其中有：\n$$ \\begin{gather} \\int \\nabla_\\theta q(x, \\theta)\\nabla_\\theta \\log q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx = \\mathbb{E}_{x\\sim q(x, \\theta)}(s(x, \\theta)s(x, \\theta)^\\top)\\\\ \\int q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = \\mathbb{E}_{x\\sim q(x, \\theta)}\\big[\\nabla^2_\\theta \\log q(x, \\theta) \\big] \\end{gather} $$证毕。有结论：\n$$ \\begin{gather}\\boxed{ \\begin{aligned} I(\\theta) \u0026= \\mathbb{E}_{x\\sim q(x, \\theta)}\\bigg[s_\\theta(x, \\theta)s_\\theta(x, \\theta)^\\top\\bigg]\\\\ \u0026= \\mathbb{E}_{x\\sim q(x, \\theta)}\\bigg[\\nabla^2_\\theta \\log q(x, \\theta) \\bigg] \\end{aligned} }\\end{gather} $$我们知道，Fisher Information Matrix 本质上是概率分布 $q(x, \\theta)$ 对参数 $\\theta$ 的 Hessian 矩阵。对于有一定的场论基础的同学肯定已经意识到了，Hessian 矩阵衡量了一个标量场在 $\\theta$ 处的曲率。我们不妨分类讨论：\n对于对角线上的元素： $$ I(\\theta)_{ii} = \\mathbb{E}_{x\\sim q(x, \\theta)} \\bigg[ \\frac{\\partial^2 }{\\partial \\theta_{ii}} \\log q(x, \\theta) \\bigg] $$这代表了对数似然函数在 $\\theta_i$ 这个参数轴方向上的平均曲率。如果 $I(\\theta)_{ii}$ 很大，说明对数函数（信息函数）$\\log q(x, \\theta)$ 在 $\\theta_{i}$ 方向上是很陡峭的。这意味着只要 $\\theta_i$ 发生一丁点变化，$\\log q(x, \\theta)$ 都会发生很大的变化。这说明数据对 $\\theta_i$ 的约束是很强的，我们可以很精确的估计出 $\\theta_i$。该参数的估计方差会很小\n非对角线的元素： $$ I(\\theta)_{ij} = \\mathbb{E}_{x\\sim q(x, \\theta)} \\bigg[ \\frac{\\partial^2 }{\\partial \\theta_i \\partial \\theta_j} \\log q(x, \\theta) \\bigg] $$这代表了 $\\theta_i 和 \\theta_j$ 之间发生 “混淆的可能性”。几何的讲，$I(\\theta)_{ij}$ 代表了在局部，当 $\\theta_j$ 方向移动时，对数似然函数在 $\\theta_i$ 方向上的斜率是如何变化的。\n如果 $I(\\theta)_{ij} = 0$，这是很理想的情况，此时 $\\theta_i$ 和 $\\theta_j$ 是相互独立的，改变 $\\theta_j$ 的值，不会影响对 $\\theta_i$ 的最优估计 如果 $I(\\theta)_{ij} \\neq 0$，这意味着 $\\theta_i$ 和 $\\theta_j$ 之间是存在混淆的。这代表如果我们修正了 $\\theta_i$ 的值，$\\theta_j$ 如果不变化的话，此时已经不是最优了。如果 $I(\\theta)_{ij} \u003e 0$，说明 $\\theta_i$ 和 $\\theta_j$ 是负相关的，如果增加了 $\\theta_i$，需要适当的减小 $\\theta_j$ 来达到最优。相反，如果 $I(\\theta)_{ij} \u003c 0$，说明 $\\theta_i$ 和 $\\theta_j$ 是正相关的，如果增加了 $\\theta_i$，需要适当的增加 $\\theta_j$ 来达到最优。 总而言之，$I(\\theta)$ 是定义在参数流形上的一个度量张量。它是一个局部量，在特定参数点 $\\hat{I(\\theta)}$ 上，衡量了整个概率分布 $p(x, \\theta)$ 对于参数 $\\theta$ 无穷小变化的敏感程度。\n3.1. 信息几何角度理解 下面我们给出一些更高视角下关于 Fisher Information Matrix 的思考。你会惊讶的发现，和上面的几何的观点不谋而合。\n对于一个统计流形（Statistical Manifold），对于其上两个十分接近的分布 $p(x\\mid \\theta)$ 和 $p(x\\mid \\theta + d\\theta)$，不妨用 KL 散度评估两个分布之间的 “距离”。那么有：\n$$ \\begin{gather} \\begin{aligned} D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta)) \u0026= \\int p(x\\mid \\theta) \\log \\frac{p(x\\mid \\theta)}{p(x\\mid \\theta + d\\theta)} dx\\\\ \\end{aligned} \\end{gather} $$对于分布 $\\log p(x\\mid \\theta + d\\theta)$，我们不妨在 $\\theta$ 处做泰勒展开：\n$$ \\log p(x; \\theta + d\\theta) \\approx \\log p(x; \\theta) + d\\theta^T \\nabla_{\\theta} \\log p(x; \\theta) + \\frac{1}{2} d\\theta^T H_{\\theta}(\\log p(x; \\theta)) d\\theta $$将上式带入 KL Divergence 中，有：\n$$ \\begin{gather} \\begin{aligned} D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta)) \u0026= \\int p(x\\mid \\theta) \\bigg[ -d\\theta^T \\nabla_{\\theta} \\log p(x; \\theta) - \\frac{1}{2} d\\theta^T H_{\\theta}(\\log p(x; \\theta)) d\\theta\\bigg] \\end{aligned} \\end{gather} $$不妨重新分配各项，有：\n$$ \\begin{gather} \\begin{aligned} \u0026D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta))\\\\ \u0026= -d\\theta^T \\int p(x\\mid \\theta) \\bigg[\\nabla_{\\theta} \\log p(x; \\theta)\\bigg]d\\theta - \\frac{1}{2} d\\theta^T \\int p(x\\mid \\theta) \\bigg[H_{\\theta}(\\log p(x; \\theta))\\bigg] d\\theta \\end{aligned} \\end{gather} $$对于第一项 $\\int p(x\\mid \\theta) \\bigg[\\nabla_{\\theta} \\log p(x; \\theta)\\bigg]d\\theta$，不难注意到，$\\nabla_{\\theta} \\log p(x; \\theta)$ 是 Score Function，而 Score Function 的期望为零（见 Lemma 1.）。因此第一项为零。\n对于第二项，有 Fisher Information Matrix $I(\\theta) = \\int p(x\\mid \\theta) \\bigg[H_{\\theta}(\\log p(x; \\theta))\\bigg] d\\theta$。\n我们可以总结出一个有趣的结论。对于 Statistical Manifold 上的一个基于 KL 散度的度量无穷小量 $ds^2 = \\lim_{d\\theta \\to 0} D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta))$，有：\n$$ \\begin{gather} ds^2 = \\frac{1}{2}d\\theta^\\top I(\\theta) d\\theta \\end{gather} $$这个结论告诉我们，统计流形是一个非平直的空间，上式告诉我们，在统计流形这样一个非平直空间上上走一小步 $ds$，那么在平直空间 $d\\theta$ 上，我们走了 $I(\\theta)$\n换一个更形式化的语言，对于两个无限接近的概率分布，它们之间的KL散度（信息论上的差异度量）在二阶近似下，等于由费雪信息矩阵定义的二次型的一半。换句话会所，它是在统计流形上，由KL散度自然导出的一个度量张量（Metric Tensor）。\n这恰恰和我们之前讨论的，参数空间的 “陡峭程度” 和训练的稳定性不谋而合。\n4. Fisher Divergence Fisher Divergence 衡量的是两个概率分布 $p(x)$ 和 $q(x)$ 的信息场之间的梯度差。更简单的说，是两个分布的 Score Funciton 之间的均方误差：\n$$ \\begin{gather} D_F(p \\mid q) \u0026= \\int p(x) \\bigg\\| \\nabla_\\theta \\log p(x) - \\nabla_\\theta \\log q(x, \\theta) \\bigg\\|^2 dx\\\\ \u0026= \\mathbb{E}_{x\\sim p(x)}\\bigg[\\big\\| \\nabla_\\theta \\log p(x) - \\nabla_\\theta \\log q(x, \\theta) \\big\\|^2\\bigg] \\end{gather} $$但是对 Fisher Divergence 的理解停留在形式上是不够的。不妨先回顾一下，我们可能更常用的 KL Divergence。对于两个分布 $p(x)$ 和 $q(x)$，有 KL Divergence：\n$$ \\begin{gather} \\begin{aligned} D_{KL}(p \\mid q) \u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[ \\log \\frac{p(x)}{q(x)} \\bigg]\\\\ \u0026= \\mathbb{E}_{x\\sim p(x)} \\big[ \\log p(x) - \\log q(x) \\big] \\end{aligned} \\end{gather} $$reference Using Fisher Information to bound KL divergence\n","wordCount":"843","inLanguage":"en","image":"https://wangjv0812.cn/","datePublished":"2025-09-13T17:13:56+08:00","dateModified":"2025-09-13T17:13:56+08:00","author":{"@type":"Person","name":"WangJV"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/"},"publisher":{"@type":"Organization","name":"WangJV Blog","logo":{"@type":"ImageObject","url":"https://wangjv0812.cn/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangjv0812.cn/ accesskey=h title="WangJV Blog (Alt + H)">WangJV Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangjv0812.cn/ title=Home><span>Home</span></a></li><li><a href=https://wangjv0812.cn/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://wangjv0812.cn/resources/ title=Resources><span>Resources</span></a></li><li><a href=https://wangjv0812.cn/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://wangjv0812.cn/search/ title="🔍 Search (Alt + /)" accesskey=/><span>🔍 Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://wangjv0812.cn/>Home</a>&nbsp;»&nbsp;<a href=https://wangjv0812.cn/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Fisher Information and Fisher Divergence</h1><div class=post-meta><span title='2025-09-13 17:13:56 +0800 +0800'>September 13, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;843 words&nbsp;·&nbsp;WangJV&nbsp;|&nbsp;<a href=https://github.com/WangJV0812/WangJV-Blog-Source/tree/master/content/posts/Fisher%20Information%20and%20Fisher%20Divergence/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-statistical-manifold>1. Statistical Manifold</a></li><li><a href=#2-score-function>2. Score Function</a></li><li><a href=#3-fisher-information-matrix>3. Fisher Information Matrix</a><ul><li><a href=#31-信息几何角度理解>3.1. 信息几何角度理解</a></li></ul></li><li><a href=#4-fisher-divergence>4. Fisher Divergence</a></li><li><a href=#reference>reference</a></li></ul></nav></div></details></div><div class=post-content><p>在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。</p><ul><li>Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。</li><li>Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。</li></ul><p>下面则给出一些不那么直观的，数学形式上的解释。</p><h2 id=1-statistical-manifold>1. Statistical Manifold<a hidden class=anchor aria-hidden=true href=#1-statistical-manifold>#</a></h2><p>和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。</p><p>那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。</p><h2 id=2-score-function>2. Score Function<a hidden class=anchor aria-hidden=true href=#2-score-function>#</a></h2><p>对于一个受到参数 $\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \theta)$，我们可以定义其 Score Function：</p>$$
\begin{gather}
s_\theta(x, \theta) = \nabla_\theta \log q(x; \theta)\\
s_x(x, \theta) = \nabla_x \log q(x; \theta)\\
\end{gather}
$$<p>对于 score function，我们可以从两个 level 理解它。</p><p>首先，直观的、几何的讲，对于 score function $s_x(x, \theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\theta(x, \theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\theta$ 的概率密度对数增长最快的方向。</p><p>但是这个直观的几何理解并没有解释为什么其中包含一个 $\log$ 的形式，为什么一定是 $\nabla \log$ 而非 $\nabla$。如果你的数学直觉比较强的化可能已经意识到了，这关联到了对于一个概率分布 <strong>信息</strong> 的衡量。但是如果想要深刻的理解这个问题，涉及到 信息几何 和 微分几何方面的知识。我会尽量在工科数学的范围内给出一些直观的解释。我们不妨先思考一下，直接使用 $\nabla_xq(x)$ 为什么不够好？不妨思考一个简单的 高斯分布。我们分别在高概率位置 $x_1$ 和 低概率位置 $x_2$ 采样，计算 $\nabla_\theta q(x_1)$ 和 $\nabla_\theta q(x_2)$。那么自然的：</p><ul><li>对于 $x_1，q(x_1)$ 很大，它的梯度 $\nabla_\theta q(x_1, \theta)$ 可能也比较大。</li><li>对于 $x_2，q(x_2)$ 几乎为零，它的梯度 $\nabla_\theta q(x_2, \theta)$ 也几乎为零。</li></ul><p>这导致尾部的、意外的、低概率的数据点 $x_2$ 对模型参数 $\theta$ 几乎没有贡献。但是这是完全违背了统计直觉。从信息论角度分析，意外的、小概率的事件包含更多的信息，它们应该对参数调整产生更大的影响才对。信息几何告诉我们，由参数 $\theta$ 定义的概率分布族（例如，所有可能的高斯分布）构成了一个空间，但这不是一个平坦的欧几里得空间，而是一个弯曲的统计流形。此时如果直接使用欧式几何的距离定义并不能争取的反应在参数流形上的距离关系。我们不加证明的给出结论，通过 &ldquo;信息化转化算子&rdquo; $\log$，抵消了概率变化导致的对参数空间的 “扭曲” 影响。</p><p>简而言之，从信息几何的角度出发，$\log$ 的加入将原本非平直的参数空间 “压平” 了。这样让在参数空间上直接通过类似于欧氏距离的方式度量分布成为了可能。$\log$ 是我们对抗参数空间内在扭曲性、将其“拉平”以便于我们理解和优化的第一步，也是最关键的一步。</p><h2 id=3-fisher-information-matrix>3. Fisher Information Matrix<a hidden class=anchor aria-hidden=true href=#3-fisher-information-matrix>#</a></h2><p>我们已经理解了 Score Function，Fisher Information Matrix 则是定义在参数 Score Function $s_\theta(x, \theta)$ 的协方差矩阵：</p>$$
\begin{gather}
I(\theta) = \mathbb{E}_{x\sim q(x, \theta)}\big[
s_\theta(x, \theta) s_\theta(x, \theta)^\top
\big]
\end{gather}
$$<p>但是这个形式不够直观，我们不妨将其变为一个更加常见的形式。但是展具体的证明前，需要先给出一个引理的证明：</p><p>Lemma 1. $\mathbb E_{x\sim q(x, \theta)}\big[s_\theta(x, \theta)\big] = 0$</p><p>有:</p>$$
\begin{aligned}
&\nabla_\theta \log q(x, \theta) = \frac{\nabla_\theta q(x, \theta)}{q(x, \theta)}\\
\longrightarrow & q(x, \theta) \nabla_\theta \log q(x, \theta) = \nabla_\theta q(x, \theta)\\
\end{aligned}
$$<p>可以推出：</p>$$
\begin{gather}
\begin{aligned}
\mathbb E_{x\sim q(x, \theta)}\big[s_\theta(x, \theta)]
&= \int q(x, \theta) \nabla_\theta \log q(x, \theta) dx\\
&= \int \nabla_\theta q(x, \theta) dx\\
\end{aligned}
\end{gather}
$$<p>在温和的条件下，可以交换 $\nabla$ 和 $\int$，有：</p>$$
\begin{gather}
\begin{aligned}
\mathbb E_{x\sim q(x, \theta)}\big[s_\theta(x, \theta)]
&= \nabla_\theta \int q(x, \theta) dx\\
&= \nabla_\theta 1\\
&= 0
\end{aligned}
\end{gather}
$$<p>那么从 Lemma 1. 出发，等式两边增加梯度算子 $\nabla_\theta$：</p>$$
\begin{gather}
\int q(x, \theta) \nabla_\theta^\top \log q(x, \theta) dx = 0\\
\nabla_\theta \int q(x, \theta) \nabla_\theta^\top \log q(x, \theta) dx = 0\\
\int \nabla_\theta \bigg(q(x, \theta) \nabla_\theta^\top \log q(x, \theta) \bigg)dx = 0\\
\int \nabla_\theta q(x, \theta) \nabla_\theta^\top \log q(x, \theta) + q(x, \theta) \nabla_\theta \nabla_\theta^\top \log q(x, \theta) dx = 0\\
\int \nabla_\theta q(x, \theta)\nabla_\theta \log q(x, \theta) \nabla_\theta^\top \log q(x, \theta) + q(x, \theta) \nabla_\theta \nabla_\theta^\top \log q(x, \theta) dx = 0\\
\end{gather}
$$<p>分配积分符号，有：</p>$$
\begin{gather}
\int \nabla_\theta q(x, \theta)\nabla_\theta \log q(x, \theta) \nabla_\theta^\top \log q(x, \theta) dx + \int q(x, \theta) \nabla_\theta \nabla_\theta^\top \log q(x, \theta) dx = 0\\
\end{gather}
$$<p>其中有：</p>$$
\begin{gather}
\int \nabla_\theta q(x, \theta)\nabla_\theta \log q(x, \theta) \nabla_\theta^\top \log q(x, \theta) dx = \mathbb{E}_{x\sim q(x, \theta)}(s(x, \theta)s(x, \theta)^\top)\\
\int q(x, \theta) \nabla_\theta \nabla_\theta^\top \log q(x, \theta) dx = \mathbb{E}_{x\sim q(x, \theta)}\big[\nabla^2_\theta \log q(x, \theta) \big]
\end{gather}
$$<p>证毕。有结论：</p>$$
\begin{gather}\boxed{
\begin{aligned}
I(\theta)
&= \mathbb{E}_{x\sim q(x, \theta)}\bigg[s_\theta(x, \theta)s_\theta(x, \theta)^\top\bigg]\\
&= \mathbb{E}_{x\sim q(x, \theta)}\bigg[\nabla^2_\theta \log q(x, \theta) \bigg]
\end{aligned}
}\end{gather}
$$<p>我们知道，Fisher Information Matrix 本质上是概率分布 $q(x, \theta)$ 对参数 $\theta$ 的 Hessian 矩阵。对于有一定的场论基础的同学肯定已经意识到了，Hessian 矩阵衡量了一个标量场在 $\theta$ 处的曲率。我们不妨分类讨论：</p><ul><li>对于对角线上的元素：</li></ul>$$
I(\theta)_{ii} = \mathbb{E}_{x\sim q(x, \theta)} \bigg[
\frac{\partial^2 }{\partial \theta_{ii}} \log q(x, \theta)
\bigg]
$$<p>这代表了对数似然函数在 $\theta_i$ 这个参数轴方向上的平均曲率。如果 $I(\theta)_{ii}$ 很大，说明对数函数（信息函数）$\log q(x, \theta)$ 在 $\theta_{i}$ 方向上是很陡峭的。这意味着只要 $\theta_i$ 发生一丁点变化，$\log q(x, \theta)$ 都会发生很大的变化。这说明数据对 $\theta_i$ 的约束是很强的，我们可以很精确的估计出 $\theta_i$。该参数的估计方差会很小</p><ul><li>非对角线的元素：</li></ul>$$
I(\theta)_{ij} = \mathbb{E}_{x\sim q(x, \theta)} \bigg[
\frac{\partial^2 }{\partial \theta_i \partial \theta_j} \log q(x, \theta)
\bigg]
$$<p>这代表了 $\theta_i 和 \theta_j$ 之间发生 “混淆的可能性”。几何的讲，$I(\theta)_{ij}$ 代表了在局部，当 $\theta_j$ 方向移动时，对数似然函数在 $\theta_i$ 方向上的斜率是如何变化的。</p><ol><li>如果 $I(\theta)_{ij} = 0$，这是很理想的情况，此时 $\theta_i$ 和 $\theta_j$ 是相互独立的，改变 $\theta_j$ 的值，不会影响对 $\theta_i$ 的最优估计</li><li>如果 $I(\theta)_{ij} \neq 0$，这意味着 $\theta_i$ 和 $\theta_j$ 之间是存在混淆的。这代表如果我们修正了 $\theta_i$ 的值，$\theta_j$ 如果不变化的话，此时已经不是最优了。如果 $I(\theta)_{ij} > 0$，说明 $\theta_i$ 和 $\theta_j$ 是负相关的，如果增加了 $\theta_i$，需要适当的减小 $\theta_j$ 来达到最优。相反，如果 $I(\theta)_{ij} < 0$，说明 $\theta_i$ 和 $\theta_j$ 是正相关的，如果增加了 $\theta_i$，需要适当的增加 $\theta_j$ 来达到最优。</li></ol><p><strong>总而言之，$I(\theta)$ 是定义在参数流形上的一个度量张量。它是一个局部量，在特定参数点 $\hat{I(\theta)}$ 上，衡量了整个概率分布 $p(x, \theta)$ 对于参数 $\theta$ 无穷小变化的敏感程度。</strong></p><h3 id=31-信息几何角度理解>3.1. 信息几何角度理解<a hidden class=anchor aria-hidden=true href=#31-信息几何角度理解>#</a></h3><p>下面我们给出一些更高视角下关于 Fisher Information Matrix 的思考。你会惊讶的发现，和上面的几何的观点不谋而合。</p><p>对于一个统计流形（Statistical Manifold），对于其上两个十分接近的分布 $p(x\mid \theta)$ 和 $p(x\mid \theta + d\theta)$，不妨用 KL 散度评估两个分布之间的 “距离”。那么有：</p>$$
\begin{gather}
\begin{aligned}
D_{KL}(p(x\mid \theta) \mid p(x\mid \theta + d\theta))
&= \int p(x\mid \theta) \log \frac{p(x\mid \theta)}{p(x\mid \theta + d\theta)} dx\\
\end{aligned}
\end{gather}
$$<p>对于分布 $\log p(x\mid \theta + d\theta)$，我们不妨在 $\theta$ 处做泰勒展开：</p>$$
\log p(x; \theta + d\theta) \approx \log p(x; \theta) + d\theta^T \nabla_{\theta} \log p(x; \theta) + \frac{1}{2} d\theta^T H_{\theta}(\log p(x; \theta)) d\theta
$$<p>将上式带入 KL Divergence 中，有：</p>$$
\begin{gather}
\begin{aligned}
D_{KL}(p(x\mid \theta) \mid p(x\mid \theta + d\theta))
&= \int p(x\mid \theta) \bigg[ -d\theta^T \nabla_{\theta} \log p(x; \theta) - \frac{1}{2} d\theta^T H_{\theta}(\log p(x; \theta)) d\theta\bigg]
\end{aligned}
\end{gather}
$$<p>不妨重新分配各项，有：</p>$$
\begin{gather}
\begin{aligned}
&D_{KL}(p(x\mid \theta) \mid p(x\mid \theta + d\theta))\\
&= -d\theta^T \int p(x\mid \theta) \bigg[\nabla_{\theta} \log p(x; \theta)\bigg]d\theta - \frac{1}{2} d\theta^T \int p(x\mid \theta) \bigg[H_{\theta}(\log p(x; \theta))\bigg] d\theta
\end{aligned}
\end{gather}
$$<p>对于第一项 $\int p(x\mid \theta) \bigg[\nabla_{\theta} \log p(x; \theta)\bigg]d\theta$，不难注意到，$\nabla_{\theta} \log p(x; \theta)$ 是 Score Function，而 Score Function 的期望为零（见 Lemma 1.）。因此第一项为零。</p><p>对于第二项，有 Fisher Information Matrix $I(\theta) = \int p(x\mid \theta) \bigg[H_{\theta}(\log p(x; \theta))\bigg] d\theta$。</p><p>我们可以总结出一个有趣的结论。对于 Statistical Manifold 上的一个基于 KL 散度的度量无穷小量 $ds^2 = \lim_{d\theta \to 0} D_{KL}(p(x\mid \theta) \mid p(x\mid \theta + d\theta))$，有：</p>$$
\begin{gather}
ds^2 = \frac{1}{2}d\theta^\top I(\theta) d\theta
\end{gather}
$$<p>这个结论告诉我们，统计流形是一个<strong>非平直的空间</strong>，上式告诉我们，在统计流形这样一个非平直空间上上走一小步 $ds$，那么在平直空间 $d\theta$ 上，我们走了 $I(\theta)$</p><p>换一个更形式化的语言，对于两个无限接近的概率分布，它们之间的KL散度（信息论上的差异度量）在二阶近似下，等于由费雪信息矩阵定义的二次型的一半。换句话会所，它是在统计流形上，由KL散度自然导出的一个度量张量（Metric Tensor）。</p><p>这恰恰和我们之前讨论的，参数空间的 “陡峭程度” 和训练的稳定性不谋而合。</p><h2 id=4-fisher-divergence>4. Fisher Divergence<a hidden class=anchor aria-hidden=true href=#4-fisher-divergence>#</a></h2><p>Fisher Divergence 衡量的是两个概率分布 $p(x)$ 和 $q(x)$ 的信息场之间的梯度差。更简单的说，是两个分布的 Score Funciton 之间的均方误差：</p>$$
\begin{gather}
D_F(p \mid q)
&= \int p(x) \bigg\| \nabla_\theta \log p(x) - \nabla_\theta \log q(x, \theta) \bigg\|^2 dx\\
&= \mathbb{E}_{x\sim p(x)}\bigg[\big\| \nabla_\theta \log p(x) - \nabla_\theta \log q(x, \theta) \big\|^2\bigg]
\end{gather}
$$<p>但是对 Fisher Divergence 的理解停留在形式上是不够的。不妨先回顾一下，我们可能更常用的 KL Divergence。对于两个分布 $p(x)$ 和 $q(x)$，有 KL Divergence：</p>$$
\begin{gather}
\begin{aligned}
D_{KL}(p \mid q)
&= \mathbb{E}_{x\sim p(x)} \bigg[ \log \frac{p(x)}{q(x)} \bigg]\\
&= \mathbb{E}_{x\sim p(x)} \big[ \log p(x) - \log q(x) \big]
\end{aligned}
\end{gather}
$$<h2 id=reference>reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p><a href=https://mathoverflow.net/questions/36130/using-fisher-information-to-bound-kl-divergence>Using Fisher Information to bound KL divergence</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://wangjv0812.cn/tags/%E6%95%B0%E5%AD%A6/>数学</a></li><li><a href=https://wangjv0812.cn/tags/%E6%A6%82%E7%8E%87/>概率</a></li></ul><nav class=paginav><a class=next href=https://wangjv0812.cn/2025/08/noise-contrastive-estimation/><span class=title>Next »</span><br><span>Noise Contrastive Estimation</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://wangjv0812.cn/>WangJV Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>