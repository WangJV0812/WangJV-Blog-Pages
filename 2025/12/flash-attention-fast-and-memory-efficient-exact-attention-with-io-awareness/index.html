<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness | WangJV Blog</title><meta name=keywords content="Flash Attention,Transformer,Attention,GPU,Deep Learning"><meta name=description content="1. Transformer å¤æ‚åº¦åˆ†æ
1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ
Transformer æ¨¡å‹äº‹å®ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•çš„å †å ã€‚è®©æˆ‘ä»¬å…ˆä»åŸºç¡€çš„å‘é‡ä¹˜æ³•çš„å¤æ‚åº¦åˆ†æå¼€å§‹ï¼Œä¸€æ­¥æ­¥æ‰©å±•åˆ°å¯¹å¼ é‡è¿ç®—çš„å¤æ‚åº¦æœ‰æ¸…æ™°çš„è®¤è¯†ã€‚

å¯¹äºå‘é‡ $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ï¼Œé‚£ä¹ˆå‘é‡ä¹‹é—´çš„ç‚¹ç§¯éœ€è¦è¿›è¡Œ $n$ æ¬¡ä¹˜æ³•å’Œ $n$ æ¬¡åŠ æ³•ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2n)$ã€‚
å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m \times n}$ å’Œ $\mathbf{x} \in \mathbb{R}^{n}$ï¼ŒçŸ©é˜µå‘é‡ä¹˜æ³• $\mathbf{Ax}$ éœ€è¦è¿›è¡Œ $m$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mn)$ã€‚
å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$ï¼ŒçŸ©é˜µä¹˜æ³• $\mathbf{AB}$ éœ€è¦è¿›è¡Œ $m \times p$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mnp)$ã€‚

ä¸Šé¢è®¨è®ºäº†åŸºç¡€çŸ©é˜µè¿ç®—çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸‹é¢ä¸å¦¨ä¸Šä¸Šå¼ºåº¦ï¼Œçœ‹çœ‹å¼ é‡è¿ç®—çš„å¤æ‚åº¦ã€‚å¯¹äºå¼ é‡ $\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$ï¼Œå…¶ä¸­ç»´åº¦ $\color{red}GH$ æ˜¯ batch ç»´åº¦ï¼Œ$\color{blue} KL$ æ˜¯è¢«å¸æ”¶ (Contracting) çš„ç»´åº¦ã€‚é‚£ä¹ˆå¯ä»¥å®šä¹‰ einsum æ“ä½œå¦‚ä¸‹ï¼š
C = torch.einsum('ghijkl,ghmnkl->ghijmn', A, B)
ä¸Šé¢çš„ einsum æ“ä½œä¸­ï¼Œå¼ é‡ $\mathbf{A}$ å’Œ $\mathbf{B}$ åœ¨ç»´åº¦ $\color{red}GH$ ä¸Šæ˜¯å¯¹é½çš„ (Aligned)ï¼Œåœ¨ç»´åº¦ $\color{blue}KL$ ä¸Šæ˜¯è¢«å¸æ”¶çš„ (Contracting)ã€‚é‚£ä¹ˆè¿™ä¸ª einsum æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š"><meta name=author content="WangJV"><link rel=canonical href=https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/><link crossorigin=anonymous href=https://wangjv0812.cn/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://wangjv0812.cn/icon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://wangjv0812.cn/icon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangjv0812.cn/icon/favicon-32x32.png><link rel=apple-touch-icon href=https://wangjv0812.cn/icon/apple-touch-icon.png><link rel=mask-icon href=https://wangjv0812.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/png sizes=32x32 href=https://wangjv0812.cn/icon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://wangjv0812.cn/icon/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=https://wangjv0812.cn/icon/apple-touch-icon.png><link rel=manifest href=https://wangjv0812.cn/icon/site.webmanifest><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},chtml:{scale:1,minScale:.5,matchFontHeight:!1,displayAlign:"center",displayIndent:"0",mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},svg:{scale:1,minScale:.5,mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},options:{enableMenu:!0,menuOptions:{settings:{zoom:"Click"}}},loader:{load:["ui/safe","a11y/assistive-mml"]},startup:{ready(){MathJax.startup.defaultReady();const e=new ResizeObserver(e=>{MathJax.typesetPromise()});e.observe(document.body)}}},window.innerWidth<=768&&(MathJax.chtml=MathJax.chtml||{},MathJax.chtml.scale=.9)</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><style>.MathJax{outline:0}@media(max-width:768px){.MathJax{font-size:90%!important}.MathJax_Display{overflow-x:auto;overflow-y:hidden;padding:0!important;margin:1em 0!important}.MathJax_CHTML{line-height:1.2!important}}mjx-container[jax=CHTML][display=true]{overflow-x:auto;overflow-y:hidden;padding:1px 0}</style><meta property="og:url" content="https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/"><meta property="og:site_name" content="WangJV Blog"><meta property="og:title" content="Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness"><meta property="og:description" content="1. Transformer å¤æ‚åº¦åˆ†æ 1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ Transformer æ¨¡å‹äº‹å®ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•çš„å †å ã€‚è®©æˆ‘ä»¬å…ˆä»åŸºç¡€çš„å‘é‡ä¹˜æ³•çš„å¤æ‚åº¦åˆ†æå¼€å§‹ï¼Œä¸€æ­¥æ­¥æ‰©å±•åˆ°å¯¹å¼ é‡è¿ç®—çš„å¤æ‚åº¦æœ‰æ¸…æ™°çš„è®¤è¯†ã€‚
å¯¹äºå‘é‡ $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ï¼Œé‚£ä¹ˆå‘é‡ä¹‹é—´çš„ç‚¹ç§¯éœ€è¦è¿›è¡Œ $n$ æ¬¡ä¹˜æ³•å’Œ $n$ æ¬¡åŠ æ³•ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2n)$ã€‚ å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m \times n}$ å’Œ $\mathbf{x} \in \mathbb{R}^{n}$ï¼ŒçŸ©é˜µå‘é‡ä¹˜æ³• $\mathbf{Ax}$ éœ€è¦è¿›è¡Œ $m$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mn)$ã€‚ å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$ï¼ŒçŸ©é˜µä¹˜æ³• $\mathbf{AB}$ éœ€è¦è¿›è¡Œ $m \times p$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mnp)$ã€‚ ä¸Šé¢è®¨è®ºäº†åŸºç¡€çŸ©é˜µè¿ç®—çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸‹é¢ä¸å¦¨ä¸Šä¸Šå¼ºåº¦ï¼Œçœ‹çœ‹å¼ é‡è¿ç®—çš„å¤æ‚åº¦ã€‚å¯¹äºå¼ é‡ $\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$ï¼Œå…¶ä¸­ç»´åº¦ $\color{red}GH$ æ˜¯ batch ç»´åº¦ï¼Œ$\color{blue} KL$ æ˜¯è¢«å¸æ”¶ (Contracting) çš„ç»´åº¦ã€‚é‚£ä¹ˆå¯ä»¥å®šä¹‰ einsum æ“ä½œå¦‚ä¸‹ï¼š
C = torch.einsum('ghijkl,ghmnkl->ghijmn', A, B) ä¸Šé¢çš„ einsum æ“ä½œä¸­ï¼Œå¼ é‡ $\mathbf{A}$ å’Œ $\mathbf{B}$ åœ¨ç»´åº¦ $\color{red}GH$ ä¸Šæ˜¯å¯¹é½çš„ (Aligned)ï¼Œåœ¨ç»´åº¦ $\color{blue}KL$ ä¸Šæ˜¯è¢«å¸æ”¶çš„ (Contracting)ã€‚é‚£ä¹ˆè¿™ä¸ª einsum æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-16T10:11:25+08:00"><meta property="article:modified_time" content="2025-12-16T10:11:25+08:00"><meta property="article:tag" content="Flash Attention"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Attention"><meta property="article:tag" content="GPU"><meta property="article:tag" content="Deep Learning"><meta property="og:image" content="https://wangjv0812.cn/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wangjv0812.cn/"><meta name=twitter:title content="Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness"><meta name=twitter:description content="1. Transformer å¤æ‚åº¦åˆ†æ
1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ
Transformer æ¨¡å‹äº‹å®ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•çš„å †å ã€‚è®©æˆ‘ä»¬å…ˆä»åŸºç¡€çš„å‘é‡ä¹˜æ³•çš„å¤æ‚åº¦åˆ†æå¼€å§‹ï¼Œä¸€æ­¥æ­¥æ‰©å±•åˆ°å¯¹å¼ é‡è¿ç®—çš„å¤æ‚åº¦æœ‰æ¸…æ™°çš„è®¤è¯†ã€‚

å¯¹äºå‘é‡ $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ï¼Œé‚£ä¹ˆå‘é‡ä¹‹é—´çš„ç‚¹ç§¯éœ€è¦è¿›è¡Œ $n$ æ¬¡ä¹˜æ³•å’Œ $n$ æ¬¡åŠ æ³•ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2n)$ã€‚
å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m \times n}$ å’Œ $\mathbf{x} \in \mathbb{R}^{n}$ï¼ŒçŸ©é˜µå‘é‡ä¹˜æ³• $\mathbf{Ax}$ éœ€è¦è¿›è¡Œ $m$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mn)$ã€‚
å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$ï¼ŒçŸ©é˜µä¹˜æ³• $\mathbf{AB}$ éœ€è¦è¿›è¡Œ $m \times p$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mnp)$ã€‚

ä¸Šé¢è®¨è®ºäº†åŸºç¡€çŸ©é˜µè¿ç®—çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸‹é¢ä¸å¦¨ä¸Šä¸Šå¼ºåº¦ï¼Œçœ‹çœ‹å¼ é‡è¿ç®—çš„å¤æ‚åº¦ã€‚å¯¹äºå¼ é‡ $\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$ï¼Œå…¶ä¸­ç»´åº¦ $\color{red}GH$ æ˜¯ batch ç»´åº¦ï¼Œ$\color{blue} KL$ æ˜¯è¢«å¸æ”¶ (Contracting) çš„ç»´åº¦ã€‚é‚£ä¹ˆå¯ä»¥å®šä¹‰ einsum æ“ä½œå¦‚ä¸‹ï¼š
C = torch.einsum('ghijkl,ghmnkl->ghijmn', A, B)
ä¸Šé¢çš„ einsum æ“ä½œä¸­ï¼Œå¼ é‡ $\mathbf{A}$ å’Œ $\mathbf{B}$ åœ¨ç»´åº¦ $\color{red}GH$ ä¸Šæ˜¯å¯¹é½çš„ (Aligned)ï¼Œåœ¨ç»´åº¦ $\color{blue}KL$ ä¸Šæ˜¯è¢«å¸æ”¶çš„ (Contracting)ã€‚é‚£ä¹ˆè¿™ä¸ª einsum æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wangjv0812.cn/posts/"},{"@type":"ListItem","position":2,"name":"Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness","item":"https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness","name":"Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness","description":"1. Transformer å¤æ‚åº¦åˆ†æ 1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ Transformer æ¨¡å‹äº‹å®ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•çš„å †å ã€‚è®©æˆ‘ä»¬å…ˆä»åŸºç¡€çš„å‘é‡ä¹˜æ³•çš„å¤æ‚åº¦åˆ†æå¼€å§‹ï¼Œä¸€æ­¥æ­¥æ‰©å±•åˆ°å¯¹å¼ é‡è¿ç®—çš„å¤æ‚åº¦æœ‰æ¸…æ™°çš„è®¤è¯†ã€‚\nå¯¹äºå‘é‡ $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ï¼Œé‚£ä¹ˆå‘é‡ä¹‹é—´çš„ç‚¹ç§¯éœ€è¦è¿›è¡Œ $n$ æ¬¡ä¹˜æ³•å’Œ $n$ æ¬¡åŠ æ³•ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2n)$ã€‚ å¯¹äºçŸ©é˜µ $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ å’Œ $\\mathbf{x} \\in \\mathbb{R}^{n}$ï¼ŒçŸ©é˜µå‘é‡ä¹˜æ³• $\\mathbf{Ax}$ éœ€è¦è¿›è¡Œ $m$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mn)$ã€‚ å¯¹äºçŸ©é˜µ $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n\\times p}$ï¼ŒçŸ©é˜µä¹˜æ³• $\\mathbf{AB}$ éœ€è¦è¿›è¡Œ $m \\times p$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mnp)$ã€‚ ä¸Šé¢è®¨è®ºäº†åŸºç¡€çŸ©é˜µè¿ç®—çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸‹é¢ä¸å¦¨ä¸Šä¸Šå¼ºåº¦ï¼Œçœ‹çœ‹å¼ é‡è¿ç®—çš„å¤æ‚åº¦ã€‚å¯¹äºå¼ é‡ $\\mathbf{A} \\in \\mathbb{R}^{{\\color{red}{GH}} IJ \\color{blue}{KL}}, \\mathbf{B} \\in \\mathbb{R}^{{\\color{red}{GH}} MN \\color{blue}{KL}}$ï¼Œå…¶ä¸­ç»´åº¦ $\\color{red}GH$ æ˜¯ batch ç»´åº¦ï¼Œ$\\color{blue} KL$ æ˜¯è¢«å¸æ”¶ (Contracting) çš„ç»´åº¦ã€‚é‚£ä¹ˆå¯ä»¥å®šä¹‰ einsum æ“ä½œå¦‚ä¸‹ï¼š\nC = torch.einsum(\u0026#39;ghijkl,ghmnkl-\u0026gt;ghijmn\u0026#39;, A, B) ä¸Šé¢çš„ einsum æ“ä½œä¸­ï¼Œå¼ é‡ $\\mathbf{A}$ å’Œ $\\mathbf{B}$ åœ¨ç»´åº¦ $\\color{red}GH$ ä¸Šæ˜¯å¯¹é½çš„ (Aligned)ï¼Œåœ¨ç»´åº¦ $\\color{blue}KL$ ä¸Šæ˜¯è¢«å¸æ”¶çš„ (Contracting)ã€‚é‚£ä¹ˆè¿™ä¸ª einsum æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š\n","keywords":["Flash Attention","Transformer","Attention","GPU","Deep Learning"],"articleBody":"1. Transformer å¤æ‚åº¦åˆ†æ 1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ Transformer æ¨¡å‹äº‹å®ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•çš„å †å ã€‚è®©æˆ‘ä»¬å…ˆä»åŸºç¡€çš„å‘é‡ä¹˜æ³•çš„å¤æ‚åº¦åˆ†æå¼€å§‹ï¼Œä¸€æ­¥æ­¥æ‰©å±•åˆ°å¯¹å¼ é‡è¿ç®—çš„å¤æ‚åº¦æœ‰æ¸…æ™°çš„è®¤è¯†ã€‚\nå¯¹äºå‘é‡ $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ï¼Œé‚£ä¹ˆå‘é‡ä¹‹é—´çš„ç‚¹ç§¯éœ€è¦è¿›è¡Œ $n$ æ¬¡ä¹˜æ³•å’Œ $n$ æ¬¡åŠ æ³•ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2n)$ã€‚ å¯¹äºçŸ©é˜µ $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ å’Œ $\\mathbf{x} \\in \\mathbb{R}^{n}$ï¼ŒçŸ©é˜µå‘é‡ä¹˜æ³• $\\mathbf{Ax}$ éœ€è¦è¿›è¡Œ $m$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mn)$ã€‚ å¯¹äºçŸ©é˜µ $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n\\times p}$ï¼ŒçŸ©é˜µä¹˜æ³• $\\mathbf{AB}$ éœ€è¦è¿›è¡Œ $m \\times p$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mnp)$ã€‚ ä¸Šé¢è®¨è®ºäº†åŸºç¡€çŸ©é˜µè¿ç®—çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸‹é¢ä¸å¦¨ä¸Šä¸Šå¼ºåº¦ï¼Œçœ‹çœ‹å¼ é‡è¿ç®—çš„å¤æ‚åº¦ã€‚å¯¹äºå¼ é‡ $\\mathbf{A} \\in \\mathbb{R}^{{\\color{red}{GH}} IJ \\color{blue}{KL}}, \\mathbf{B} \\in \\mathbb{R}^{{\\color{red}{GH}} MN \\color{blue}{KL}}$ï¼Œå…¶ä¸­ç»´åº¦ $\\color{red}GH$ æ˜¯ batch ç»´åº¦ï¼Œ$\\color{blue} KL$ æ˜¯è¢«å¸æ”¶ (Contracting) çš„ç»´åº¦ã€‚é‚£ä¹ˆå¯ä»¥å®šä¹‰ einsum æ“ä½œå¦‚ä¸‹ï¼š\nC = torch.einsum('ghijkl,ghmnkl-\u003eghijmn', A, B) ä¸Šé¢çš„ einsum æ“ä½œä¸­ï¼Œå¼ é‡ $\\mathbf{A}$ å’Œ $\\mathbf{B}$ åœ¨ç»´åº¦ $\\color{red}GH$ ä¸Šæ˜¯å¯¹é½çš„ (Aligned)ï¼Œåœ¨ç»´åº¦ $\\color{blue}KL$ ä¸Šæ˜¯è¢«å¸æ”¶çš„ (Contracting)ã€‚é‚£ä¹ˆè¿™ä¸ª einsum æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š\n$$ O\\left(2 \\times G \\times H \\times I \\times J \\times M \\times N \\times K \\times L\\right) $$å³å¯¹äº Batching å’Œ Contracting éƒ½åªä¾¿åˆ©ä¸€éï¼Œè€Œä¸­é—´çš„ç»´åº¦åˆ™éœ€è¦åˆ†åˆ«éå†ã€‚einsum å±•å¼€æ˜¯è¿™æ ·çš„ï¼š\ndef einsum('ghijkl,ghmnkl-\u003eghijmn', A, B): G, H, I, J, K, L = A.shape _, _, M, N, _, _ = B.shape C = torch.zeros((G, H, I, J, M, N)) for g, h, i, j, m, n in product(range(G), range(H), range(I), range(J), range(M), range(N)): acc = 0 for k, l in product(range(K), range(L)): acc += A[g, h, i, j, k, l] * B[g, h, m, n, k, l] C[g, h, i, j, m, n] = acc return C ä¸å¦¨æ€»ç»“ä¸€ä¸‹ï¼Œä¸Šé¢è¿™äº›æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š\næ•°æ®ç»´åº¦ å½¢å¼ FLOPs $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ $\\mathbf{xy}$ $O(2n)$ $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}, \\mathbf{x} \\in \\mathbb{R}^{n}$ $\\mathbf{Ax}$ $O(2mn)$ $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}, \\mathbf{B} \\in \\mathbb{R}^{n\\times p}$ $\\mathbf{AB}$ $O(2mnp)$ $\\mathbf{A} \\in \\mathbb{R}^{{\\color{red}{GH}} IJ \\color{blue}{KL}}, \\mathbf{B} \\in \\mathbb{R}^{{\\color{red}{GH}} MN \\color{blue}{KL}}$ einsum(â€˜ghijkl,ghmnkl-\u003eghijmnâ€™, A, B) $O\\left(2 G H I J M N K L\\right)$ 1.2. åå‘ä¼ æ’­å¤æ‚åº¦åˆ†æ æ¨ç†æ—¶ï¼Œè‡³éœ€è¦è®¡ç®—çŸ©é˜µçš„æ­£å‘è¿ç®—å°±è¡Œï¼Œä½†æ˜¯è®­ç»ƒæ—¶è¦è€ƒè™‘çš„å°±å¤šäº†ï¼Œè¿˜éœ€è¦è®¡ç®— Loss å¯¹å‚æ•°çš„æ¢¯åº¦ã€‚å› æ­¤æœ¬èŠ‚æˆ‘ä»¬è¦åˆ†ææœ€åŸºæœ¬çš„çŸ©é˜µå¾®åˆ†ã€‚\nå¯¹äºä»»æ„å¯ä¼˜åŒ–å˜é‡ $x$ï¼Œå‡è®¾æœ‰ä¸€ä¸ªæ ‡é‡ Loss $L$ï¼Œæƒ³ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¼˜åŒ–ï¼Œéœ€è¦æ±‚å¾— $\\frac{\\partial L}{\\partial x}$ã€‚é‚£ä¹ˆå¯¹äº Loss $L$ï¼Œå¯ä»¥å®šä¹‰å…¨å¾®åˆ†ï¼š\n$$ dL = \\sum_{\\forall i \\text{ need optimized}} \\frac{\\partial L}{\\partial x_i} dx_i $$è¿™ä¸ªå½¢å¼ä¸å¤Ÿå¥½å¤„ç†ï¼Œä½†æ˜¯ä¸éš¾å‘ç°è¿™æ°å¥½æ˜¯æ¢¯åº¦çŸ©é˜µ $\\frac{\\partial L}{\\partial x}$ ä¸ $dx$ çš„ Frobenius å†…ç§¯ (Frobenius inner product)ï¼š\n$$ dL = \\text{Tr}\\left(\\left(\\frac{\\partial L}{\\partial x}\\right)^T dx\\right) $$é‚£ä¹ˆå‡è®¾æ­£å‘ä¼ æ’­ä¸­çš„æŸä¸€æ­¥æœ‰ $\\mathbf{C} = \\mathbf{A B}$ï¼Œå¯¹äº $L$ çš„å…¨å¾®åˆ†ï¼Œæœ‰ï¼š\n$$ \\begin{aligned} dL \u0026= \\text{Tr} \\left(\\left( \\frac{\\partial L}{\\partial C}\\right)^T dC\\right)\\\\ \u0026= \\text{Tr} \\left(\\left( \\frac{\\partial L}{\\partial C}\\right)^T A dB\\right)\\\\ \u0026= \\text{Tr} \\left(\\left( \\frac{\\partial L}{\\partial B}\\right)^T dB\\right)\\\\ \\end{aligned} $$é‚£ä¹ˆæœ‰ï¼š\n$$ \\begin{aligned} \\left( \\frac{\\partial L}{\\partial B}\\right)^T \u0026= \\left( \\frac{\\partial L}{\\partial C}\\right)^T A\\\\ \\frac{\\partial L}{\\partial B} \u0026= A^T \\frac{\\partial L}{\\partial C} \\end{aligned} $$æ ¹æ®ä¹‹å‰çš„è®¨è®ºï¼Œè®¡ç®— $\\frac{\\partial L}{\\partial B}$ çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2 m n p)$ã€‚åŒç†å¯å¾—ï¼š\n$$ \\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial C} B^T $$å°†è¿™äº›åŠ èµ·æ¥ï¼Œå³å¯¹äºçŸ©é˜µä¹˜æ³• $\\mathbf{C} = \\mathbf{A B}$ï¼Œåå‘ä¼ æ’­éœ€è¦è®¡ç®—ä¸¤æ¬¡çŸ©é˜µä¹˜æ³•ã€‚å› æ­¤ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€æ¬¡æ­£å‘å’Œä¸¤æ¬¡åå‘ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦çº¦ä¸º $O(6mnp)$ã€‚ç®€å•çš„è®²ï¼Œå°±æ˜¯æ­£å‘ä¼ æ’­çš„ä¸‰å€ã€‚\nå¦‚æœæˆ‘ä»¬è®¤ä¸º $\\mathbf{A}$ æ˜¯æ•°æ®æµï¼Œ$\\mathbf{B}$ æ˜¯å‚æ•°ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ° openai åœ¨ Scaling Laws for Neural Language Models ä¸­æåˆ°çš„ Trasformer è®­ç»ƒæ—¶éœ€è¦çš„è®¡ç®—é‡ä¸º $6 \\times \\text{å‚æ•°é‡} \\times \\text{token æ•°}$ã€‚\n1.3. Attention å¤æ‚åº¦åˆ†æ Coolï¼ŒåŸºäºä¸Šé¢çš„è®¨è®ºï¼Œæˆ‘ä»¬å¯ä»¥å…·ä½“çš„åˆ†æ Transformer ä¸­æ¯ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦äº†ã€‚\n1.3.1. Attention æ­£å‘ä¼ æ’­å¤æ‚åº¦åˆ†æ å¯¹äºä¸€ä¸ªè¾“å…¥åºåˆ— $\\mathbf{x} \\in \\mathbb{R}^{B\\times S\\times C}$ï¼Œå’Œå‚æ•° $W_q \\in \\mathbb{R}^{C\\times C_q}, W_k \\in \\mathbb{R}^{C\\times C_q}, W_v \\in \\mathbb{R}^{C\\times C_v}$ï¼Œéœ€è¦å…ˆè®¡ç®—å‡ºæŸ¥è¯¢ (Query)ã€é”® (Key)ã€å€¼ (Value)ï¼š\nQuery = torch.einsum('bsc, cq -\u003e bsq', x, W_q) # B, S, C_q Key = torch.einsum('bsc, cq -\u003e bsq', x, W_k) # B, S, C_q Value = torch.einsum('bsc, cv -\u003e bsv', x, W_v) # B, S, C_v æ ¹æ®ä¹‹å‰çš„è®¨è®ºï¼Œå¯ä»¥å†™å‡ºè¿™ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦ï¼š\næ“ä½œ Flops Query $O(2 B S C C_q)$ Key $O(2 B S C C_q)$ Value $O(2 B S C C_v)$ ä¹‹åéœ€è¦å°†è®¡ç®—å‡ºçš„ queryã€keyã€value åˆ’åˆ†å‡º headã€‚å‡è®¾æˆ‘ä»¬é€‰çš„ head æ•°ä¸º $H$ï¼Œä¸” $C_q$ å’Œ $C_v$ éƒ½å¯ä»¥è¢« $H$ æ•´é™¤ï¼Œé‚£ä¹ˆæœ‰ï¼š\nq = q.view(B, S, H, C_q // H).transpose(1, 2).contiguous() # B, H, S, C_q // H k = k.view(B, S, H, C_q // H).transpose(1, 2).contiguous() # B, H, S, C_q // H v = v.view(B, S, H, C_v // H).transpose(1, 2).contiguous() # B, H, S, C_v // H è¿™æ­¥æ²¡æœ‰å…·ä½“çš„è®¡ç®—ï¼Œåªæœ‰æ•°æ®æŒªåŠ¨ã€‚ä¸‹ä¸€æ­¥åˆ™è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Attention Scores)ï¼š\nscores = torch.einsum('bhsc, bhtc -\u003e bhst', q, k) / (c_q // H) ** 0.5 # B, H, S, S scores = torch.softmax(scores, dim=-1) è¿™ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š\næ“ä½œ Flops scores (einsum) $O(2 B H S S (C_q // H))$ scores (softmax) $O(3 B H S S)$ ä¹‹åå°†æ³¨æ„åŠ›åˆ†æ•°åº”ç”¨åˆ° value ä¸Šï¼Œè®¡ç®—åŠ æƒå’Œå¹¶å°†å¤šå¤´è¿˜åŸï¼š\ncontext = torch.einsum('bhst, bhtv -\u003e bhsv', scores, v) # B, H, S, C_v // H context = context.transpose(1, 2).contiguous() context = context.view(B, S, C_v) # B, S, C_v ä¹‹åï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„åˆ°è¾“å‡ºç©ºé—´å¹¶æ®‹å·®è¿æ¥ï¼š\noutput = torch.einusm('bsc, cd -\u003e bsd', contrext, W_o) # B, S, C output = output + x # Residual connection è¿™å‡ æ­¥çš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š\næ“ä½œ Flops output (einsum) $O(2 B S C_v C)$ residual connection $O(B S C)$ è¿™æ ·å°±å®Œæˆäº† Attention block çš„æ­£å‘ä¼ æ’­ã€‚åˆ†æä¸­ä¸éš¾å‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥å°† Transformer çš„è®¡ç®—åˆ’åˆ†ä¸ºä¸¤ç±»ï¼š\nå‚æ•°å¯†é›†å‹ (Parameter-bound)ï¼šä¾‹å¦‚çº¿æ€§æŠ•å½± (Linear Projection)ã€FFN ç­‰ã€‚ å¤æ‚åº¦è¿‘ä¼¼ä¸ºï¼š$2 \\times \\text{æ€»å‚æ•°é‡} \\times \\text{token æ•°}$ã€‚ åºåˆ—å¯†é›†å‹ (Sequence-bound)ï¼šä¾‹å¦‚ Attention Score è®¡ç®—ã€Softmax ç­‰ã€‚ å¤æ‚åº¦è¿‘ä¼¼ä¸ºï¼š$2 \\times \\text{å±‚æ•°} \\times O(B \\times C \\times \\text{token æ•°}^2)$ã€‚ 1.3.2. ä» GPT-3 åˆ° ç°ä»£å¤§æ¨¡å‹ç®—åŠ›ç“¶é¢ˆçš„è½¬ç§» åœ¨ OpenAI å‘è¡¨ Scaling Laws çš„ GPT-3 æ—¶ä»£ï¼ŒTransformer çš„è®¡ç®—ç“¶é¢ˆä¸»è¦åœ¨äºå‚æ•°é‡ã€‚\nä»¥ GPT-3 175B ä¸ºä¾‹ï¼Œæ¨¡å‹å‚æ•°é‡ $P \\approx 1750$ äº¿ï¼Œå±‚æ•° $L=96$ï¼Œéšè—ç»´æ•° $C=12288$ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ $S=2048$ã€‚\nå‚æ•°å¯†é›†å‹è®¡ç®— (Linear)ï¼š $$ 2 \\times 1.75 \\times 10^{11} \\times 2048 \\approx 7 \\times 10^{14} \\text{ FLOPs} $$ åºåˆ—å¯†é›†å‹è®¡ç®— (Attention)ï¼š $$ L \\times 4 \\times B \\times C \\times S^2 = 96 \\times 4 \\times 1 \\times 12288 \\times 2048^2 \\approx 2 \\times 10^{13} \\text{ FLOPs} $$åœ¨ GPT-3 æ—¶ä»£ï¼ŒAttention çš„è®¡ç®—é‡ä»…ä¸ºçº¿æ€§å±‚çš„ $\\frac{1}{35}\\approx 2.8\\%$ï¼Œç¡®å®å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚\nä½†æ˜¯ä»Šæ—¶ä¸åŒå¾€æ—¥ï¼Œç°ä»£æ¨¡å‹ï¼ˆå¦‚ Gemini 2.5 Proï¼‰å°† Context Length æå‡åˆ°äº† 1M çº§åˆ«ã€‚æ­¤æ—¶å‡è®¾æˆ‘ä»¬ç”¨ä¸€ä¸ªåŒæ ·è§„æ¨¡çš„æ¨¡å‹å¤„ç† 1M é•¿åº¦çš„è¾“å…¥ï¼š\nå‚æ•°å¯†é›†å‹è®¡ç®— (Linear)ï¼šç”±äºä¸ $S$ å‘ˆçº¿æ€§å…³ç³»ï¼Œè®¡ç®—é‡å¢é•¿ 500 å€ï¼š $$ 7 \\times 10^{14} \\times 500 \\approx 3.5 \\times 10^{17} \\text{ FLOPs} $$ åºåˆ—å¯†é›†å‹è®¡ç®— (Attention)ï¼šç”±äºä¸ $S$ å‘ˆå¹³æ–¹å…³ç³»ï¼Œè®¡ç®—é‡å¢é•¿ $500^2 = 250,000$ å€ï¼š $$ 2 \\times 10^{13} \\times 250,000 \\approx 5 \\times 10^{18} \\text{ FLOPs} $$å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ 1M ä¸Šä¸‹æ–‡ä¸­ï¼ŒAttention çš„è®¡ç®—é‡åè¶…çº¿æ€§å±‚ 10 å€ä»¥ä¸Šï¼ è®¡ç®—ç“¶é¢ˆä»å‚æ•°è½¬ç§»åˆ°äº†åºåˆ—é•¿åº¦ã€‚è¿™ä¹Ÿè‡ªç„¶çš„å¼•å‡ºäº†æˆ‘ä»¬ä¼˜åŒ– Attention è®¡ç®—çš„åŠ¨æœºã€‚\n2. ç°ä»£ GPU ç»“æ„åˆ†æ 2.1. GPU è®¡ç®—æ¶æ„åˆ†æ æˆ‘ä»¬ä¸å¦¨ä»ç°åœ¨æœ€ä¸»æµçš„è®¡ç®—å¡ A100 ä½ä¾‹ï¼Œåˆ†æç°ä»£ GPU çš„è®¡ç®—æ¶æ„ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œåœ¨ç¡¬ä»¶ä¸Š GPU å¯ä»¥åˆ’åˆ†ä¸º GPU -\u003e GPC (Graphics Processing Cluster) -\u003e SM (Streaming Multiprocessor) -\u003e Sub-SM è¿™å‡ ä¸ªå±‚çº§ã€‚ä¸‹å›¾æ˜¯ Nvidia A100 æ¶æ„å›¾ï¼Œæ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒ A100 ç™½çš®ä¹¦ã€‚\nå¯¹äºå®Œæ•´çš„ GA100 æ ¸å¿ƒï¼Œæœ‰ 8 ç»„ GPCï¼Œ128 ä¸ª SMã€‚å‡ºäºè‰¯å“ç‡ï¼ˆæˆæœ¬/åˆ€æ³•ï¼‰è€ƒè™‘ï¼ŒA100 åªä¼šå¯ç”¨ 7 ç»„ GPCï¼Œ108 ä¸ª SMã€‚æ¯ä¸ª GPC éƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„è®¡ç®—å•å…ƒï¼ˆä¸åŒ…æ‹¬ L2-Cache å’Œ æ˜¾å­˜æ§åˆ¶å•å…ƒï¼‰ã€‚è€é»„æƒ³ç”Ÿäº§ç¬¬ä¸€çº§åˆ«çš„ GPU å°±ä¸éœ€è¦é‡æ–°è®¾è®¡ï¼Œè‡³éœ€è¦è£åˆ‡ GPC æ•°é‡å³å¯ ï¼ˆåˆ€æ³•.jpgï¼‰ã€‚\næ¯ä¸ª GPC ä¸­åŒ…å« 16 ä¸ª SMã€‚æ¯ä¸ª SM åŒ…å« 4 ä¸ª Sub-SM å’Œ ä¸€ä¸ªæä¸ºé‡è¦çš„ 192KB ç»Ÿä¸€å†…å­˜å— (Unified Shared Memory / L1 Cache)ã€‚è¿™ç‚¹è¯·åŠ¡å¿…è®°ä½ï¼Œå®ƒæ˜¯ FlashAttention å‘æŒ¥é­”æ³•çš„ä¸»èˆå°ã€‚\nSub-SM æ˜¯ GPU ç¼–ç¨‹ä¸­çš„æœ€åŸºç¡€å•ä½ã€‚ä¸€ä¸ª Sub-SM åŒ…å«ï¼š\n16 ä¸ª INT32 CUDA Core 16 ä¸ª FP32 CUDA Core 8 ä¸ª FP64 CUDA Core (åŒç²¾åº¦) 1 ä¸ª ç¬¬ä¸‰ä»£ Tensor Core ä¸ CPU ä¸åŒï¼Œä¸€ä¸ª Sub-SM åªåŒ…å«ä¸€ä¸ªæŒ‡ä»¤è°ƒåº¦å•å…ƒ (Scheduler)ï¼Œåªæœ‰ä¸€ä¸ª PC å¯„å­˜å™¨ï¼Œå³æ‰€æœ‰çš„ Cuda Core ä»»æ„æ—¶åˆ»éƒ½åªèƒ½å¯¹ä¸åŒæ•°æ®æ‰§è¡ŒåŒä¸€æ¡æŒ‡ä»¤ï¼Œè¿™å°±æ˜¯ SIMT æ¶æ„ã€‚\nä¸€ä¸ª Cuda Core åœ¨ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸå¯ä»¥å®Œæˆä¸€æ¬¡æ ‡é‡ $a\\times b + c$ è¿ç®—ã€‚è€Œ Tensor Core åˆ™å¯ä»¥åŒæ—¶ï¼ˆä¸ä¸€å®šæ˜¯ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸï¼‰å®Œæˆä¸€æ¬¡ $A \\times B + C$ çš„çŸ©é˜µä¹˜æ³•ï¼Œ$A$ ä¸ $B$ çš„è§„æ¨¡å—åˆ° Tensor Core çš„ç‰ˆæœ¬çš„é€‰æ‹©çš„ç²¾åº¦å½±å“ã€‚å¯¹äº FP16 ç²¾åº¦ï¼ŒTensor Core å¯ä»¥è¿›è¡Œ $4 \\times 4$ çš„çŸ©é˜µä¹˜æ³•ï¼Œå³ $4 \\times 4 \\times 4 = 64$ æ¬¡æ ‡é‡è¿ç®—ã€‚å¯¹äºçŸ©é˜µä¹˜æ³•ï¼ŒTensor Core æ˜¾è‘—å¿«äº Cuda Coreã€‚æˆ‘ä»¬è¯´è¿‡äº†ï¼Œä¸€ä¸ª Sub-SM æ‰€æœ‰çº¿ç¨‹å…±äº«ä¸€ä¸ª PC æŒ‡é’ˆï¼Œå› æ­¤ä¸€ä¸ª Sub-SM å†…çš„ Cuda Core å’Œ Tensor Core ä¸èƒ½å¹¶è¡Œå·¥ä½œï¼Œè€Œæ˜¯éœ€è¦äº¤æ›¿å·¥ä½œã€‚\nå¦‚æœä½ å­¦ä¹ è¿‡ä¸€äº› CUDA ç¼–ç¨‹ï¼Œä¸€å®šä¼šå¯¹ Warp è¿™ä¸ªæ¦‚å¿µä¸é™Œç”Ÿã€‚Warp æ˜¯ CUDA ç¼–ç¨‹ä¸­æœ€å°çš„è°ƒåº¦å•ä½ï¼ˆæ³¨æ„ï¼Œæ˜¯ç¼–ç¨‹ä¸­çš„æœ€å°è°ƒåº¦å•ä½ï¼Œå¹¶ä¸ä¸ Sub-SM ç›´æ¥å¯¹åº”ï¼‰ï¼Œä¸€ä¸ª Warp åŒ…å« 32 ä¸ªçº¿ç¨‹ï¼Œåˆšå¥½äº¤ç»™ä¸€ä¸ª Sub-SM ä¸­çš„ Cuda Core è®¡ç®—ã€‚ç¼–ç¨‹ä¸­ä¸€ä¸ª SM æœ€å¤šå¯ä»¥åˆ†é… 64 ä¸ª Warpã€‚Warp æ•°æ˜¾è‘—å¤šäºä¸€ä¸ª SM ä¸­ Sub-SM çš„æ•°é‡ï¼ˆ4ä¸ªï¼‰ã€‚è¿™å°±æ˜¯ GPU ç¼–ç¨‹ä¸ CPU ç¼–ç¨‹æœ€å¤§çš„ä¸åŒäº†ï¼Œä¸ CPU é€šè¿‡æå‡å•çº¿ç¨‹æ€§èƒ½å’Œä¹±åºæ‰§è¡Œæ¥æ©ç›–å»¶è¿Ÿä¸åŒï¼ŒGPU é€šè¿‡å¤§é‡çº¿ç¨‹æ¥æ©ç›–å»¶è¿Ÿã€‚\nä¸Šå›¾ä¸­ä½ å¯èƒ½å·²ç»å‘ç°äº†ï¼Œæ¯ä¸ª Sub-SM éƒ½æœ‰ä¸€ä¸ªå¤§çš„å“äººçš„ Register-Files ($16384 \\times 32$ bit)ã€‚æŒ‡ä»¤å‘å‡ºåï¼Œæ•°æ®ä¼šä»æ˜¾å­˜ï¼ˆHBM æˆ–è€… GDDR6ï¼‰é€æ­¥æ¬è¿åˆ° L2 Cache -\u003e L1 Cacheã€‚ä¹‹åæ•°æ®ä¼šè¢«æ¬è¿åˆ°ç»™ Warp åˆ†é…çš„å¯¹åº”åŒºåŸŸçš„ Register Files ä¸­ã€‚å®Œæˆæ•°æ®å‡†å¤‡åï¼Œå¯¹åº”çš„ Warp ä¼šè¢« Launch è¿› Cuda Core æˆ–è€… Tensor Core è¿›è¡Œè®¡ç®—ã€‚è®¡ç®—å®Œæˆåï¼Œç»“æœä¼šè¢«å†™å› Register Filesï¼Œä¹‹åå†é€æ­¥å†™å› L1 Cache -\u003e L2 Cache -\u003e æ˜¾å­˜ã€‚\nå¯¹äº Pytorch ä¸­çš„ä¸€ä¸ªå¼ é‡æ“ä½œï¼Œä¾‹å¦‚ï¼š\nA = torch.randn(1024, 1024).cuda() B = torch.randn(1024, 1024).cuda() C = torch.einsum('ij, jk -\u003e ik', A, B) pytorch ä¸­çš„æ‰€æœ‰å˜é‡éƒ½ä¼šè¢«ä¿å­˜åœ¨æ˜¾å­˜ä¸­ï¼Œå½“è¿›è¡Œè®¡ç®—æ—¶ï¼Œæ•°æ®ä¼šè¢«æ¬è¿åˆ° L2 Cache ä¸­ã€‚ä¹‹åæ ¹æ®è‡ªåŠ¨åˆ†é…çš„ SMï¼Œå†æ¬è¿åˆ°å¯¹åº” SM çš„ L1 Cache ä¸­ã€‚ä¸€èˆ¬è€Œè¨€çŸ©é˜µè¿ç®—éƒ½ä¼šä½¿ç”¨ TensorCore æ¥å¤„ç†ã€‚å®Œæˆè®¡ç®—åï¼Œç»“æœä¼šè¢«å†™å› L1 Cache -\u003e L2 Cache -\u003e æ˜¾å­˜ã€‚\nè¿™é‡ŒåŸ‹ä¸‹äº†ä¸€ä¸ªéšæ‚£ï¼šå¦‚æœåœ¨ HBM ä¸­é¢‘ç¹è¯»å†™å·¨å¤§çš„ä¸­é—´çŸ©é˜µï¼ˆæ¯”å¦‚ Attention Scoreï¼‰ï¼Œå³ä½¿ Tensor Core ç®—å¾—å†å¿«ï¼Œä¹Ÿä¼šè¢«æ‹¥å µçš„æ˜¾å­˜å¸¦å®½æ‹–æ­»ã€‚è¿™å°†åœ¨ä¸‹ä¸€èŠ‚è¯¦ç»†åˆ†æã€‚ æ€»ç»“ï¼š\n2.2. è®¡ç®—ä¸ IO çš„å¹³è¡¡ åœ¨ç™½çš®ä¹¦ä¸­ä¸éš¾æ‰¾åˆ°ï¼š\nOperation Throughput FP16/BF16 Tensor Core 312 TFlops HBM Memory Bandwidth 1555 GB/s L1 Cache Bandwidth per SM 19 TB/s ä¸éš¾å‘ç°ï¼Œå¦‚æœå¸Œæœ›è®¡ç®—å’Œ IO å¹³è¡¡ï¼Œé‚£ä¹ˆæ¯ä¸ªä»æ˜¾å­˜ä¸­æ¬è¿çš„æ•°æ®éƒ½è‡³å°‘åº”è¯¥åšï¼š\n$$ \\frac{312 \\times 10^{12}}{1.555 \\times 10^{12}} = 200 \\text{ FLOPs/Byte} $$æˆ‘ä»¬å¯ä»¥å¾ˆæ¸…æ™°çš„å°†è®¡ç®—åˆ’åˆ†ä¸ºä¸¤ç±»ï¼š\nè®¡ç®—ç“¶é¢ˆå‹ (Compute-bound)ï¼šæ¯ä¸ªä»æ˜¾å­˜ä¸­æ¬è¿çš„æ•°æ®å¯ä»¥åšè¶…è¿‡ 200 FLOPs çš„è®¡ç®—ã€‚ä¾‹å¦‚å¤§å‹çš„çŸ©é˜µä¹˜æ³•ã€å¤§ Channel çš„å·ç§¯ç­‰ã€‚ IO ç“¶é¢ˆå‹ (IO-bound)ï¼šæ¯ä¸ªä»æ˜¾å­˜ä¸­æ¬è¿çš„æ•°æ®åªèƒ½åšå°‘äº 200 FLOPs çš„è®¡ç®—ã€‚ä¾‹å¦‚ softmaxã€sumã€batch normã€layer normç­‰ã€‚ å¯¹äºä¸åŒçš„å­˜å‚¨è®¾å¤‡ï¼Œè¿™ä¸ªå¹³è¡¡ç‚¹ä¹Ÿæ˜¯ä¸åŒçš„ã€‚ä¾‹å¦‚å¯¹äº L1 Cacheï¼š\n$$ \\frac{312 \\times 10^{12}}{19 \\times 10^{12}} \\approx 16 \\text{ FLOPs/Byte} $$å› æ­¤å¦‚æœæ•°æ®å¯ä»¥è¢«æ”¾å…¥ L1 Cache ä¸­ï¼Œé‚£ä¹ˆæ¯ä¸ªä» L1 Cache ä¸­æ¬è¿çš„æ•°æ®åªéœ€è¦åšè¶…è¿‡ 16 FLOPs çš„è®¡ç®—å³å¯ã€‚\n2.3. å°† IO å¼€é”€å¼•å…¥å¤æ‚åº¦åˆ†æ å½“æ„è¯†åˆ°äº† IO ä¸æ˜¯æ²¡æœ‰ä»£ä»·çš„ä¹‹åï¼Œæˆ‘ä»¬é‡æ–°å›åˆ° Transformer æ¡†æ¶ä¸­ï¼Œçœ‹çœ‹å¦‚æœå°† IO å¼€é”€çº³å…¥å¤æ‚åº¦åˆ†æï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚æˆ‘ä»¬ç€é‡åˆ†æå…¶ä¸­å¼€é”€æœ€å¤§çš„ï¼Œä¹Ÿæ˜¯ $O(n^2)$ çš„ Attention Score è®¡ç®—ã€‚å…¶ç®—æ³•æµç¨‹å¯ä»¥ç®€å•å†™ä¸ºï¼š\nçŸ©é˜µ $\\mathbf{Q, K, V} \\in \\mathbb{R}^{N\\times d}$ å­˜å‚¨åœ¨ HBM ä¸­ã€‚ ä» HBM è¯»å– $\\mathbf{Q, K}$ï¼Œè®¡ç®— $\\mathbf{S} = \\mathbf{QK}^T$, å¹¶å°† $\\mathbf{S}$ å†™å› HBMã€‚ ä» HBM è¯»å– $\\mathbf{S}$ï¼Œè®¡ç®— $\\mathbf{P} = \\text{softmax}(\\mathbf{S})$ï¼Œå¹¶å°† $\\mathbf{P}$ å†™å› HBMã€‚ ä» HBM è¯»å– $\\mathbf{P, V}$ï¼Œè®¡ç®— $\\mathbf{O} = \\mathbf{PV}$ï¼Œå¹¶å°† $\\mathbf{O}$ å†™å› HBMã€‚ è¿”å› $\\mathbf{O}$ã€‚ ä¸éš¾å‘ç°ï¼Œå¯¹äºå¯¹äº $\\mathbf{QK}^T$ï¼Œå¹¶å‡è®¾ä½¿ç”¨ F32 å­˜å‚¨ï¼ˆ4byteï¼‰ï¼Œæœ‰ï¼š\nIO å¼€é”€ï¼šè¯»å– $\\mathbf{Q, K}$ï¼Œå†™å› $\\mathbf{S}$ï¼Œæ€»å¤æ‚åº¦ä¸º $O(4 \\times (2Nd + N^2))$ è®¡ç®—å¼€é”€ï¼Œå¤æ‚åº¦ä¸º $O(2Nd^2)$ é‚£ä¹ˆç®—æ•°å¼ºåº¦æ¯”ä¸ºï¼š\n$$ \\frac{2Nd^2}{2 \\times (2Nd + N^2)} \\approx \\frac{d}{2} $$å› ä¸º $d=64$ï¼Œé‚£ä¹ˆæ¯ä¸€ byte ä»…è¿›è¡Œ 32 FLOPsï¼Œè¿œä½äº 200 FLOPs/Byte çš„å¹³è¡¡ç‚¹ï¼Œå±äºå…¸å‹çš„ IO ç“¶é¢ˆå‹æ“ä½œã€‚å¦‚æœè§‰å¾—è¿™ä¸ªå†…å­˜ç“¶é¢ˆè¿˜å¯ä»¥æ¥å—ï¼Œå¯¹äº softmaxï¼Œæœ‰ï¼š\nIO å¼€é”€ï¼šè¯»å– $\\mathbf{S}$ï¼Œå†™å› $\\mathbf{P}$ï¼Œæ€»å¤æ‚åº¦ä¸º $O(8N^2)$ è®¡ç®—å¼€é”€ï¼Œå¤æ‚åº¦ä¸º $O(5N^2)$ é‚£ä¹ˆç®—æ•°å¼ºåº¦æ¯”ä¸ºï¼š\n$$ \\frac{5N^2}{8N^2} = 0.625 $$è¿™ æ˜¯ä¸€ä¸ªä½çš„ä»¤äººç»æœ›çš„ç®—æ•°å¼ºåº¦æ¯”ï¼Œå±äºéå¸¸å…¸å‹çš„ IO ç“¶é¢ˆå‹æ“ä½œã€‚äº‹å®ä¸Šå¦‚æœä½¿ç”¨äº† LayerNomrm æˆ–è€… Block Dropoutï¼Œè¿™ä¸ªé—®é¢˜ä¼šæ›´ä¸¥é‡ã€‚è¿™å¯¼è‡´å¦‚æœç›´æ¥è®¡ç®— Attention æ—¶ï¼Œç»å¤§éƒ¨åˆ†æ—¶é—´ SM éƒ½åœ¨ç©ºè½¬ï¼Œç­‰å¾…æ•°æ®ä» HBM ä¸­æ¬è¿è¿‡æ¥ï¼ˆæˆ–è€…æ¬è¿å›å»ï¼‰ã€‚è¿™å¯¼è‡´äº‹å®ä¸Š IO æˆä¸ºäº† Attention è®¡ç®—çš„ä¸»è¦ç“¶é¢ˆã€‚\n3. Flash Attention æ ¹æ®ä¸Šé¢çš„è®¨è®ºï¼Œä¸€ä¸ªæ¸…æ™°çš„ç»“è®ºæ˜¯ï¼ŒAttention é¢ä¸´ç€ä¸¥é‡çš„ IO ç“¶é¢ˆï¼Œå¯¼è‡´å¤§éƒ¨åˆ†æ—¶é—´ï¼ŒGPU éƒ½åœ¨ç©ºè½¬ï¼Œç­‰å¾…æ•°æ®æ¬è¿ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼ŒFlash Attention ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯å°† $\\mathbf{Q, K, V}$ åˆ’åˆ†ä¸ºå¾ˆå¤šå— (Tiles)ï¼Œæ¯ä¸ªå—æ”¾åœ¨ä¸€ä¸ª SM ä¸­çš„ L1 Cache ä¸­ï¼Œåœ¨ L1 Cache å®Œæˆ Attention è®¡ç®—åï¼Œå†å°†ç»“æœå†™å› HBMã€‚ è¿™æ ·å°±å¤§å¤§å‡å°‘äº†å¯¹ HBM çš„è®¿é—®æ¬¡æ•°ï¼Œä»è€Œæå‡äº† Attention è®¡ç®—çš„æ•ˆç‡ã€‚\n3.1. åˆ†å— softmax è®¡ç®— å‡è®¾å­˜åœ¨å‘é‡ $\\mathbf{x} \\in \\mathbb{R}^{B}$ï¼Œsoftmax çš„è®¡ç®—è¿‡ç¨‹ä¸ºï¼š\nä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œè®¡ç®— $m(\\mathbf{x}) = \\max_i x_i$ $f(x) = \\begin{bmatrix} e^{x_1 - m(\\mathbf{x})}\u0026 e^{x_2 - m(\\mathbf{x})}\u0026 \\cdots\u0026 e^{x_B - m(\\mathbf{x})} \\end{bmatrix}$ $l(\\mathbf{x}) = \\sum_i f(x)_i$ $\\text{softmax}(\\mathbf{x}) = \\frac{f(\\mathbf{x})}{l(\\mathbf{x})}$ è¿™æ˜¯ä¸€ä¸ªå¾ˆç›´è§‰ç®€å•çš„è¿‡ç¨‹ï¼Œé‚£ä¹ˆå¯¹äºä¸¤ä¸ªå‘é‡ $\\mathbf{x}^1, \\mathbf{x}^2 \\in \\mathbb{R}^{B}$ï¼Œå°†å…¶æ‹¼æ¥æˆä¸€ä¸ªå‘é‡ $\\mathbf{x} = \\begin{bmatrix} \\mathbf{x}^1 \u0026 \\mathbf{x}^2 \\end{bmatrix}$ï¼Œè”åˆå‘é‡ $\\mathbf{x}$ çš„ softmax è®¡ç®—è¿‡ç¨‹ä¸ºï¼š\nè®¡ç®— $m(\\mathbf{x}) = \\max(m(\\mathbf{x}^1), m(\\mathbf{x}^2))$ $f(\\mathbf{x}) = \\begin{bmatrix} e^{m(\\mathbf{x^1}) - m(\\mathbf{x})}f(\\mathbf x^1)\u0026 e^{m(\\mathbf{x^2}) - m(\\mathbf{x})}f(\\mathbf x^2) \\end{bmatrix}$ $l(\\mathbf{x}) = e^{m(\\mathbf{x^1}) - m(\\mathbf{x})}l(\\mathbf x^1) + e^{m(\\mathbf{x^2}) - m(\\mathbf{x})}l(\\mathbf x^2)$ $\\text{softmax}(\\mathbf{x}) = \\frac{f(\\mathbf{x})}{l(\\mathbf{x})}$ 3.2. Flash Attention ç®—æ³•æµç¨‹ è¿™å°±æ˜¯ FlashAttention æœ€æ ¸å¿ƒçš„ç®—æ³•ã€‚è¿™ä¸ªç®—æ³•å®ç°äº†å°†ä¸€ä¸ªå¤§å‘é‡çš„ softmax è®¡ç®—ï¼Œåˆ’åˆ†ä¸ºå¤šä¸ªå°å‘é‡çš„ softmax è®¡ç®—ï¼Œå¹¶ä¸”åªéœ€è¦ä¿å­˜æ¯ä¸ªå°å‘é‡çš„æœ€å¤§å€¼å’Œç´¯åŠ å’Œå³å¯ã€‚è¿™æ ·å°±å¤§å¤§å‡å°‘äº†å¯¹æ˜¾å­˜çš„è®¿é—®æ¬¡æ•°ï¼Œä»è€Œæå‡äº†è®¡ç®—æ•ˆç‡ã€‚åŸºäºè¿™ä¸ªç®—æ³•ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°† Attention åˆ’åˆ†ä¸ºå¤šä¸ªå— (Tiles)ï¼Œæ¯ä¸ªå—éƒ½æ”¾åœ¨ SM çš„ L1 Cache ä¸­è®¡ç®—ã€‚é‚£ä¹ˆï¼ŒAttention çš„è®¡ç®—æµç¨‹å¯ä»¥å†™ä½œï¼š\nçŸ©é˜µ $\\mathbf{Q, K, V} \\in \\mathbb{R}^{N \\times d}$ï¼Œå­˜å‚¨åœ¨ HBM ä¸­ï¼Œå‡è®¾ L1 Cache çš„å¤§å°ä¸º $M$ bytes.\nå¯¹äº Decoder Only Attentionï¼Œæ¯ä¸ª Query äº‹å®ä¸Šåªéœ€è¦ä½¿ç”¨ä¸€æ¬¡ï¼Œå’Œä¹‹å‰åºåˆ—æ‰€æœ‰ Key å’Œ Value è®¡ç®—ã€‚å› æ­¤æˆ‘ä»¬å…ˆå°† Query åœ¨åºåˆ—æ–¹å‘åˆ†å—ï¼ŒåŠ è½½å…¥ L1 Cache ä¸­ã€‚ä¸å¦¨å¦ Query çš„å—å¤§å°ä¸º $B_q$ã€‚éœ€è¦æ³¨æ„ï¼Œåé¢çš„æ“ä½œä¸­ï¼ŒQuery æ˜¯ä¸ä¼šè¢«å†™å› HBM çš„ï¼Œæ˜¯ä¸€å—å›ºå®šå†…å­˜ã€‚æœ‰ $\\mathbf{Q}_i \\in \\mathbb{R}^{B_q \\times d}$\nå»ºç«‹è¿‡ç¨‹å˜é‡\n$\\mathbf{O}\\in \\mathbb{R}^{B_q \\times d}$ï¼Œç”¨äºç»´æŠ¤åŠ æƒè¾“å‡ºã€‚åˆå§‹åŒ–ä¸º 0ï¼Œå­˜å‚¨åœ¨ L1 Cache ä¸­ã€‚ $\\mathbf{m_i} \\in \\mathbb{R}^{B_q}$ï¼Œç”¨äºç»´æŠ¤ Query çš„æœ€å¤§å€¼ã€‚åˆå§‹åŒ–ä¸º $-\\infty$ï¼Œå­˜å‚¨åœ¨ L1 Cache ä¸­ã€‚ $\\mathbf{l_i} \\in \\mathbb{R}^{B_q}$ï¼Œç”¨äºç»´æŠ¤ Query çš„ç´¯åŠ å’Œã€‚åˆå§‹åŒ–ä¸º 0ï¼Œå­˜å‚¨åœ¨ L1 Cache ä¸­ã€‚ ä» HBM ä¸­æµå¼åŠ è½½ $\\mathbf{K_j, V_j}$ï¼Œä¸ SRAM ä¸­çš„ Query è®¡ç®— Attentionã€‚\nè®¡ç®—å±€éƒ¨ç›¸ä¼¼åº¦ $\\mathbf S_{ij} = \\mathbf{Q}_i \\mathbf{K}_j^T$ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ä¸€æ­¥ä¸€èˆ¬åªå­˜å‚¨åœ¨ Register Files ä¸­ï¼Œä¸å†™å› L1 Cacheã€‚ è®¡ç®—å±€éƒ¨æœ€å¤§å€¼ $\\mathbf{m_{ij}} = \\max(\\mathbf{m_i}, \\max \\text{ of } \\mathbf{S_{ij}})$ è®¡ç®—å±€éƒ¨ softmax $\\mathbf{p_{ij}} = \\text{softmax}(\\mathbf{S_{ij}} - \\mathbf{m_{ij}})$ æ­¤æ—¶æˆ‘ä»¬æ‰‹ä¸Šæœ‰ä¸¤å¥—æ•°æ®ï¼Œåˆ†åˆ«æ˜¯ï¼š æ—§æ•°æ®ï¼š$\\mathbf{m_i}, \\mathbf{l_i}, \\mathbf{O}$ æ–°æ•°æ®ï¼š$\\mathbf{m_{ij}}, \\mathbf{p_{ij}}$ æ­¤æ—¶åªè¦åŸºäºæˆ‘ä»¬å‰é¢è®¨è®ºçš„ åˆ†å— softmax è®¡ç®—ï¼Œå°±å¯ä»¥æ›´æ–°è¿‡ç¨‹å˜é‡ï¼š æ›´æ–°æœ€å¤§å€¼ï¼š$\\mathbf{m_i} = \\mathbf{m_{ij}}$ æ›´æ–°ç¼©æ”¾å› å­ï¼š$e^{\\mathbf{m_i} - \\mathbf{m_{ij}}}$ æ›´æ–°ç´¯åŠ å’Œï¼š$\\mathbf{l_i} = e^{\\mathbf{m_i} - \\mathbf{m_{ij}}} \\mathbf{l_i} + \\sum \\mathbf{p_{ij}}$ æ›´æ–°è¾“å‡ºï¼š$\\mathbf{O} = e^{\\mathbf{m_i} - \\mathbf{m_{ij}}} \\mathbf{O} + \\mathbf{p_{ij}} \\mathbf{V_j}$ å®Œæˆè®¡ç®—åï¼Œé‡Šæ”¾ $\\mathbf{K_j, V_j}$ å ç”¨çš„ L1 Cache ç©ºé—´ï¼Œç»§ç»­åŠ è½½ä¸‹ä¸€å— $\\mathbf{K_{j+1}, V_{j+1}}$ï¼Œé‡å¤ä¸Šè¿°è®¡ç®—ï¼Œç›´åˆ°æ‰€æœ‰ Key å’Œ Value å‡è¢«å¤„ç†å®Œã€‚ å½“æ‰€æœ‰ Key å’Œ Value å‡è¢«å¤„ç†å®Œåï¼Œé™¤ä»¥ç¼©æ”¾å› å­ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡º $\\mathbf{O}$\nå°†è¾“å‡º $\\mathbf{O}$ å†™å› HBMã€‚\nå¯ä»¥çœ‹åˆ°ï¼ŒFlash Attention åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œéœ€è¦ä¿å­˜å¦‚ä¸‹æ•°æ®ï¼š\nQuery å— $\\mathbf{Q_i} \\in \\mathbb{R}^{B_q \\times d}$ Key å’Œ Value å— $\\mathbf{K_j, V_j} \\in \\mathbb{R}^{B_k \\times d}$ è¿‡ç¨‹å˜é‡ $\\mathbf{O} \\in \\mathbb{R}^{B_q \\times d}, \\mathbf{m_i} \\in \\mathbb{R}^{B_q}, \\mathbf{l_i} \\in \\mathbb{R}^{B_q}$ å¦‚æœä½¿ç”¨ F32 å­˜å‚¨ï¼ˆ4 byteï¼‰ï¼Œé‚£ä¹ˆæ€»çš„å†…å­˜å¼€é”€çº¦æŸä¸ºï¼š\n$$ \\begin{aligned} \u00264 \\times (2 B_q \\times d + 2 \\times B_k \\times d + 2 \\times B_q) \\text{ bytes}\\\\ \u0026\\approx 8 \\times (B_q \\times d + B_k \\times d) \\text{ bytes} ","wordCount":"1683","inLanguage":"en","image":"https://wangjv0812.cn/","datePublished":"2025-12-16T10:11:25+08:00","dateModified":"2025-12-16T10:11:25+08:00","author":{"@type":"Person","name":"WangJV"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/"},"publisher":{"@type":"Organization","name":"WangJV Blog","logo":{"@type":"ImageObject","url":"https://wangjv0812.cn/icon/favicon-32x32.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangjv0812.cn/ accesskey=h title="WangJV Blog (Alt + H)"><img src=https://wangjv0812.cn/icon/translucent-icon.png alt aria-label=logo height=35>WangJV Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangjv0812.cn/ title=Home><span>Home</span></a></li><li><a href=https://wangjv0812.cn/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://wangjv0812.cn/resources/ title=Resources><span>Resources</span></a></li><li><a href=https://wangjv0812.cn/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://wangjv0812.cn/search/ title="ğŸ” Search (Alt + /)" accesskey=/><span>ğŸ” Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://wangjv0812.cn/>Home</a>&nbsp;Â»&nbsp;<a href=https://wangjv0812.cn/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h1><div class=post-meta><span title='2025-12-16 10:11:25 +0800 +0800'>December 16, 2025</span>&nbsp;Â·&nbsp;WangJV&nbsp;|&nbsp;<a href=https://github.com/WangJV0812/WangJV-Blog-Source/tree/master/content/posts/FlashAttention/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-transformer-å¤æ‚åº¦åˆ†æ>1. Transformer å¤æ‚åº¦åˆ†æ</a><ul><li><a href=#11-çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ>1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ</a></li><li><a href=#12-åå‘ä¼ æ’­å¤æ‚åº¦åˆ†æ>1.2. åå‘ä¼ æ’­å¤æ‚åº¦åˆ†æ</a></li><li><a href=#13-attention-å¤æ‚åº¦åˆ†æ>1.3. Attention å¤æ‚åº¦åˆ†æ</a></li><li><a href=#132-ä»-gpt-3-åˆ°-ç°ä»£å¤§æ¨¡å‹ç®—åŠ›ç“¶é¢ˆçš„è½¬ç§»>1.3.2. ä» GPT-3 åˆ° ç°ä»£å¤§æ¨¡å‹ç®—åŠ›ç“¶é¢ˆçš„è½¬ç§»</a></li></ul></li><li><a href=#2-ç°ä»£-gpu-ç»“æ„åˆ†æ>2. ç°ä»£ GPU ç»“æ„åˆ†æ</a><ul><li><a href=#21-gpu-è®¡ç®—æ¶æ„åˆ†æ>2.1. GPU è®¡ç®—æ¶æ„åˆ†æ</a></li><li><a href=#22-è®¡ç®—ä¸-io-çš„å¹³è¡¡>2.2. è®¡ç®—ä¸ IO çš„å¹³è¡¡</a></li><li><a href=#23-å°†-io-å¼€é”€å¼•å…¥å¤æ‚åº¦åˆ†æ>2.3. å°† IO å¼€é”€å¼•å…¥å¤æ‚åº¦åˆ†æ</a></li></ul></li><li><a href=#3-flash-attention>3. Flash Attention</a><ul><li><a href=#31-åˆ†å—-softmax-è®¡ç®—>3.1. åˆ†å— softmax è®¡ç®—</a></li><li><a href=#32-flash-attention-ç®—æ³•æµç¨‹>3.2. Flash Attention ç®—æ³•æµç¨‹</a></li><li><a href=#33-flash-attention-å¤æ‚åº¦åˆ†æ>3.3. Flash Attention å¤æ‚åº¦åˆ†æ</a></li><li><a href=#34-åå‘ä¼ æ’­ä¼˜åŒ–>3.4. åå‘ä¼ æ’­ä¼˜åŒ–</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h2 id=1-transformer-å¤æ‚åº¦åˆ†æ>1. Transformer å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#1-transformer-å¤æ‚åº¦åˆ†æ>#</a></h2><h3 id=11-çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ>1.1. çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#11-çŸ©é˜µè¿ç®—å¤æ‚åº¦åˆ†æ>#</a></h3><p>Transformer æ¨¡å‹äº‹å®ä¸Šæ˜¯çŸ©é˜µä¹˜æ³•çš„å †å ã€‚è®©æˆ‘ä»¬å…ˆä»åŸºç¡€çš„å‘é‡ä¹˜æ³•çš„å¤æ‚åº¦åˆ†æå¼€å§‹ï¼Œä¸€æ­¥æ­¥æ‰©å±•åˆ°å¯¹å¼ é‡è¿ç®—çš„å¤æ‚åº¦æœ‰æ¸…æ™°çš„è®¤è¯†ã€‚</p><ul><li>å¯¹äºå‘é‡ $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ï¼Œé‚£ä¹ˆå‘é‡ä¹‹é—´çš„ç‚¹ç§¯éœ€è¦è¿›è¡Œ $n$ æ¬¡ä¹˜æ³•å’Œ $n$ æ¬¡åŠ æ³•ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2n)$ã€‚</li><li>å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m \times n}$ å’Œ $\mathbf{x} \in \mathbb{R}^{n}$ï¼ŒçŸ©é˜µå‘é‡ä¹˜æ³• $\mathbf{Ax}$ éœ€è¦è¿›è¡Œ $m$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mn)$ã€‚</li><li>å¯¹äºçŸ©é˜µ $\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$ï¼ŒçŸ©é˜µä¹˜æ³• $\mathbf{AB}$ éœ€è¦è¿›è¡Œ $m \times p$ æ¬¡å‘é‡ç‚¹ç§¯ï¼Œæ¯æ¬¡ç‚¹ç§¯çš„å¤æ‚åº¦ä¸º $O(2n)$ï¼Œå› æ­¤æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2mnp)$ã€‚</li></ul><p>ä¸Šé¢è®¨è®ºäº†åŸºç¡€çŸ©é˜µè¿ç®—çš„æ—¶é—´å¤æ‚åº¦ï¼Œä¸‹é¢ä¸å¦¨ä¸Šä¸Šå¼ºåº¦ï¼Œçœ‹çœ‹å¼ é‡è¿ç®—çš„å¤æ‚åº¦ã€‚å¯¹äºå¼ é‡ $\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$ï¼Œå…¶ä¸­ç»´åº¦ $\color{red}GH$ æ˜¯ batch ç»´åº¦ï¼Œ$\color{blue} KL$ æ˜¯è¢«å¸æ”¶ (Contracting) çš„ç»´åº¦ã€‚é‚£ä¹ˆå¯ä»¥å®šä¹‰ <a href=https://docs.pytorch.org/docs/stable/generated/torch.einsum.html>einsum</a> æ“ä½œå¦‚ä¸‹ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;ghijkl,ghmnkl-&gt;ghijmn&#39;</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>)</span>
</span></span></code></pre></div><p>ä¸Šé¢çš„ einsum æ“ä½œä¸­ï¼Œå¼ é‡ $\mathbf{A}$ å’Œ $\mathbf{B}$ åœ¨ç»´åº¦ $\color{red}GH$ ä¸Šæ˜¯å¯¹é½çš„ (Aligned)ï¼Œåœ¨ç»´åº¦ $\color{blue}KL$ ä¸Šæ˜¯è¢«å¸æ”¶çš„ (Contracting)ã€‚é‚£ä¹ˆè¿™ä¸ª einsum æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š</p>$$
O\left(2 \times G \times H \times I \times J \times M \times N \times K \times L\right)
$$<p>å³å¯¹äº Batching å’Œ Contracting éƒ½åªä¾¿åˆ©ä¸€éï¼Œè€Œä¸­é—´çš„ç»´åº¦åˆ™éœ€è¦åˆ†åˆ«éå†ã€‚einsum å±•å¼€æ˜¯è¿™æ ·çš„ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>einsum</span><span class=p>(</span><span class=s1>&#39;ghijkl,ghmnkl-&gt;ghijmn&#39;</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>G</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>I</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>L</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>M</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>B</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>G</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>I</span><span class=p>,</span> <span class=n>J</span><span class=p>,</span> <span class=n>M</span><span class=p>,</span> <span class=n>N</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>g</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>,</span> <span class=n>m</span><span class=p>,</span> <span class=n>n</span> <span class=ow>in</span> <span class=n>product</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>G</span><span class=p>),</span> <span class=nb>range</span><span class=p>(</span><span class=n>H</span><span class=p>),</span> <span class=nb>range</span><span class=p>(</span><span class=n>I</span><span class=p>),</span> <span class=nb>range</span><span class=p>(</span><span class=n>J</span><span class=p>),</span> <span class=nb>range</span><span class=p>(</span><span class=n>M</span><span class=p>),</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>acc</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>l</span> <span class=ow>in</span> <span class=n>product</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>K</span><span class=p>),</span> <span class=nb>range</span><span class=p>(</span><span class=n>L</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>            <span class=n>acc</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>g</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>l</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>g</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>m</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>l</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>g</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>,</span> <span class=n>m</span><span class=p>,</span> <span class=n>n</span><span class=p>]</span> <span class=o>=</span> <span class=n>acc</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>C</span>
</span></span></code></pre></div><p>ä¸å¦¨æ€»ç»“ä¸€ä¸‹ï¼Œä¸Šé¢è¿™äº›æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š</p><table><thead><tr><th>æ•°æ®ç»´åº¦</th><th>å½¢å¼</th><th>FLOPs</th></tr></thead><tbody><tr><td>$\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$</td><td>$\mathbf{xy}$</td><td>$O(2n)$</td></tr><tr><td>$\mathbf{A} \in \mathbb{R}^{m \times n}, \mathbf{x} \in \mathbb{R}^{n}$</td><td>$\mathbf{Ax}$</td><td>$O(2mn)$</td></tr><tr><td>$\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$</td><td>$\mathbf{AB}$</td><td>$O(2mnp)$</td></tr><tr><td>$\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$</td><td>einsum(&lsquo;ghijkl,ghmnkl->ghijmn&rsquo;, A, B)</td><td>$O\left(2 G H I J M N K L\right)$</td></tr></tbody></table><h3 id=12-åå‘ä¼ æ’­å¤æ‚åº¦åˆ†æ>1.2. åå‘ä¼ æ’­å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#12-åå‘ä¼ æ’­å¤æ‚åº¦åˆ†æ>#</a></h3><p>æ¨ç†æ—¶ï¼Œè‡³éœ€è¦è®¡ç®—çŸ©é˜µçš„æ­£å‘è¿ç®—å°±è¡Œï¼Œä½†æ˜¯è®­ç»ƒæ—¶è¦è€ƒè™‘çš„å°±å¤šäº†ï¼Œè¿˜éœ€è¦è®¡ç®— Loss å¯¹å‚æ•°çš„æ¢¯åº¦ã€‚å› æ­¤æœ¬èŠ‚æˆ‘ä»¬è¦åˆ†ææœ€åŸºæœ¬çš„çŸ©é˜µå¾®åˆ†ã€‚</p><p>å¯¹äºä»»æ„å¯ä¼˜åŒ–å˜é‡ $x$ï¼Œå‡è®¾æœ‰ä¸€ä¸ªæ ‡é‡ Loss $L$ï¼Œæƒ³ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¼˜åŒ–ï¼Œéœ€è¦æ±‚å¾— $\frac{\partial L}{\partial x}$ã€‚é‚£ä¹ˆå¯¹äº Loss $L$ï¼Œå¯ä»¥å®šä¹‰å…¨å¾®åˆ†ï¼š</p>$$
dL = \sum_{\forall i \text{ need optimized}} \frac{\partial L}{\partial x_i} dx_i
$$<p>è¿™ä¸ªå½¢å¼ä¸å¤Ÿå¥½å¤„ç†ï¼Œä½†æ˜¯ä¸éš¾å‘ç°è¿™æ°å¥½æ˜¯æ¢¯åº¦çŸ©é˜µ $\frac{\partial L}{\partial x}$ ä¸ $dx$ çš„ Frobenius å†…ç§¯ (Frobenius inner product)ï¼š</p>$$
dL = \text{Tr}\left(\left(\frac{\partial L}{\partial x}\right)^T dx\right)
$$<p>é‚£ä¹ˆå‡è®¾æ­£å‘ä¼ æ’­ä¸­çš„æŸä¸€æ­¥æœ‰ $\mathbf{C} = \mathbf{A B}$ï¼Œå¯¹äº $L$ çš„å…¨å¾®åˆ†ï¼Œæœ‰ï¼š</p>$$
\begin{aligned}
dL
&= \text{Tr} \left(\left( \frac{\partial L}{\partial C}\right)^T dC\right)\\
&= \text{Tr} \left(\left( \frac{\partial L}{\partial C}\right)^T A dB\right)\\
&= \text{Tr} \left(\left( \frac{\partial L}{\partial B}\right)^T dB\right)\\
\end{aligned}
$$<p>é‚£ä¹ˆæœ‰ï¼š</p>$$
\begin{aligned}
\left( \frac{\partial L}{\partial B}\right)^T &= \left( \frac{\partial L}{\partial C}\right)^T A\\
\frac{\partial L}{\partial B} &= A^T \frac{\partial L}{\partial C}
\end{aligned}
$$<p>æ ¹æ®ä¹‹å‰çš„è®¨è®ºï¼Œè®¡ç®— $\frac{\partial L}{\partial B}$ çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(2 m n p)$ã€‚åŒç†å¯å¾—ï¼š</p>$$
\frac{\partial L}{\partial A} = \frac{\partial L}{\partial C} B^T
$$<p>å°†è¿™äº›åŠ èµ·æ¥ï¼Œå³å¯¹äºçŸ©é˜µä¹˜æ³• $\mathbf{C} = \mathbf{A B}$ï¼Œåå‘ä¼ æ’­éœ€è¦è®¡ç®—ä¸¤æ¬¡çŸ©é˜µä¹˜æ³•ã€‚å› æ­¤ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€æ¬¡æ­£å‘å’Œä¸¤æ¬¡åå‘ï¼Œæ€»çš„æ—¶é—´å¤æ‚åº¦çº¦ä¸º $O(6mnp)$ã€‚ç®€å•çš„è®²ï¼Œå°±æ˜¯æ­£å‘ä¼ æ’­çš„ä¸‰å€ã€‚</p><p>å¦‚æœæˆ‘ä»¬è®¤ä¸º $\mathbf{A}$ æ˜¯æ•°æ®æµï¼Œ$\mathbf{B}$ æ˜¯å‚æ•°ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ° openai åœ¨ <a href=https://arxiv.org/abs/2001.08361>Scaling Laws for Neural Language Models</a> ä¸­æåˆ°çš„ Trasformer è®­ç»ƒæ—¶éœ€è¦çš„è®¡ç®—é‡ä¸º $6 \times \text{å‚æ•°é‡} \times \text{token æ•°}$ã€‚</p><h3 id=13-attention-å¤æ‚åº¦åˆ†æ>1.3. Attention å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#13-attention-å¤æ‚åº¦åˆ†æ>#</a></h3><p>Coolï¼ŒåŸºäºä¸Šé¢çš„è®¨è®ºï¼Œæˆ‘ä»¬å¯ä»¥å…·ä½“çš„åˆ†æ Transformer ä¸­æ¯ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦äº†ã€‚</p><p><img alt="Attention Pipline" loading=lazy src=https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/Images/Attention-qkv.png></p><h4 id=131-attention-æ­£å‘ä¼ æ’­å¤æ‚åº¦åˆ†æ>1.3.1. Attention æ­£å‘ä¼ æ’­å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#131-attention-æ­£å‘ä¼ æ’­å¤æ‚åº¦åˆ†æ>#</a></h4><p>å¯¹äºä¸€ä¸ªè¾“å…¥åºåˆ— $\mathbf{x} \in \mathbb{R}^{B\times S\times C}$ï¼Œå’Œå‚æ•° $W_q \in \mathbb{R}^{C\times C_q}, W_k \in \mathbb{R}^{C\times C_q}, W_v \in \mathbb{R}^{C\times C_v}$ï¼Œéœ€è¦å…ˆè®¡ç®—å‡ºæŸ¥è¯¢ (Query)ã€é”® (Key)ã€å€¼ (Value)ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>Query</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;bsc, cq -&gt; bsq&#39;</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>W_q</span><span class=p>)</span> <span class=c1># B, S, C_q</span>
</span></span><span class=line><span class=cl><span class=n>Key</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;bsc, cq -&gt; bsq&#39;</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>W_k</span><span class=p>)</span> <span class=c1># B, S, C_q</span>
</span></span><span class=line><span class=cl><span class=n>Value</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;bsc, cv -&gt; bsv&#39;</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>W_v</span><span class=p>)</span> <span class=c1># B, S, C_v</span>
</span></span></code></pre></div><p>æ ¹æ®ä¹‹å‰çš„è®¨è®ºï¼Œå¯ä»¥å†™å‡ºè¿™ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦ï¼š</p><table><thead><tr><th>æ“ä½œ</th><th>Flops</th></tr></thead><tbody><tr><td>Query</td><td>$O(2 B S C C_q)$</td></tr><tr><td>Key</td><td>$O(2 B S C C_q)$</td></tr><tr><td>Value</td><td>$O(2 B S C C_v)$</td></tr></tbody></table><p>ä¹‹åéœ€è¦å°†è®¡ç®—å‡ºçš„ queryã€keyã€value åˆ’åˆ†å‡º headã€‚å‡è®¾æˆ‘ä»¬é€‰çš„ head æ•°ä¸º $H$ï¼Œä¸” $C_q$ å’Œ $C_v$ éƒ½å¯ä»¥è¢« $H$ æ•´é™¤ï¼Œé‚£ä¹ˆæœ‰ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>q</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>C_q</span> <span class=o>//</span> <span class=n>H</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span> <span class=c1># B, H, S, C_q // H</span>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=n>k</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>C_q</span> <span class=o>//</span> <span class=n>H</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span> <span class=c1># B, H, S, C_q // H</span>
</span></span><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>v</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>C_v</span> <span class=o>//</span> <span class=n>H</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span> <span class=c1># B, H, S, C_v // H</span>
</span></span></code></pre></div><p>è¿™æ­¥æ²¡æœ‰å…·ä½“çš„è®¡ç®—ï¼Œåªæœ‰æ•°æ®æŒªåŠ¨ã€‚ä¸‹ä¸€æ­¥åˆ™è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Attention Scores)ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;bhsc, bhtc -&gt; bhst&#39;</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>c_q</span> <span class=o>//</span> <span class=n>H</span><span class=p>)</span> <span class=o>**</span> <span class=mf>0.5</span>  <span class=c1># B, H, S, S</span>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><p>è¿™ä¸€æ­¥çš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š</p><table><thead><tr><th>æ“ä½œ</th><th>Flops</th></tr></thead><tbody><tr><td>scores (einsum)</td><td>$O(2 B H S S (C_q // H))$</td></tr><tr><td>scores (softmax)</td><td>$O(3 B H S S)$</td></tr></tbody></table><p>ä¹‹åå°†æ³¨æ„åŠ›åˆ†æ•°åº”ç”¨åˆ° value ä¸Šï¼Œè®¡ç®—åŠ æƒå’Œå¹¶å°†å¤šå¤´è¿˜åŸï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;bhst, bhtv -&gt; bhsv&#39;</span><span class=p>,</span> <span class=n>scores</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span> <span class=c1># B, H, S, C_v // H</span>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>context</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>context</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>S</span><span class=p>,</span> <span class=n>C_v</span><span class=p>)</span> <span class=c1># B, S, C_v </span>
</span></span></code></pre></div><p>ä¹‹åï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚æ˜ å°„åˆ°è¾“å‡ºç©ºé—´å¹¶æ®‹å·®è¿æ¥ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einusm</span><span class=p>(</span><span class=s1>&#39;bsc, cd -&gt; bsd&#39;</span><span class=p>,</span> <span class=n>contrext</span><span class=p>,</span> <span class=n>W_o</span><span class=p>)</span> <span class=c1># B, S, C</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>output</span> <span class=o>+</span> <span class=n>x</span> <span class=c1># Residual connection</span>
</span></span></code></pre></div><p>è¿™å‡ æ­¥çš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š</p><table><thead><tr><th>æ“ä½œ</th><th>Flops</th></tr></thead><tbody><tr><td>output (einsum)</td><td>$O(2 B S C_v C)$</td></tr><tr><td>residual connection</td><td>$O(B S C)$</td></tr></tbody></table><p>è¿™æ ·å°±å®Œæˆäº† Attention block çš„æ­£å‘ä¼ æ’­ã€‚åˆ†æä¸­ä¸éš¾å‘ç°ï¼Œæˆ‘ä»¬å¯ä»¥å°† Transformer çš„è®¡ç®—åˆ’åˆ†ä¸ºä¸¤ç±»ï¼š</p><ol><li><strong>å‚æ•°å¯†é›†å‹ (Parameter-bound)</strong>ï¼šä¾‹å¦‚çº¿æ€§æŠ•å½± (Linear Projection)ã€FFN ç­‰ã€‚
å¤æ‚åº¦è¿‘ä¼¼ä¸ºï¼š$2 \times \text{æ€»å‚æ•°é‡} \times \text{token æ•°}$ã€‚</li><li><strong>åºåˆ—å¯†é›†å‹ (Sequence-bound)</strong>ï¼šä¾‹å¦‚ Attention Score è®¡ç®—ã€Softmax ç­‰ã€‚
å¤æ‚åº¦è¿‘ä¼¼ä¸ºï¼š$2 \times \text{å±‚æ•°} \times O(B \times C \times \text{token æ•°}^2)$ã€‚</li></ol><h3 id=132-ä»-gpt-3-åˆ°-ç°ä»£å¤§æ¨¡å‹ç®—åŠ›ç“¶é¢ˆçš„è½¬ç§»>1.3.2. ä» GPT-3 åˆ° ç°ä»£å¤§æ¨¡å‹ç®—åŠ›ç“¶é¢ˆçš„è½¬ç§»<a hidden class=anchor aria-hidden=true href=#132-ä»-gpt-3-åˆ°-ç°ä»£å¤§æ¨¡å‹ç®—åŠ›ç“¶é¢ˆçš„è½¬ç§»>#</a></h3><p>åœ¨ OpenAI å‘è¡¨ <a href=https://arxiv.org/abs/2001.08361>Scaling Laws</a> çš„ GPT-3 æ—¶ä»£ï¼ŒTransformer çš„è®¡ç®—ç“¶é¢ˆä¸»è¦åœ¨äºå‚æ•°é‡ã€‚</p><p>ä»¥ <strong>GPT-3 175B</strong> ä¸ºä¾‹ï¼Œæ¨¡å‹å‚æ•°é‡ $P \approx 1750$ äº¿ï¼Œå±‚æ•° $L=96$ï¼Œéšè—ç»´æ•° $C=12288$ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ $S=2048$ã€‚</p><ul><li>å‚æ•°å¯†é›†å‹è®¡ç®— (Linear)ï¼š</li></ul>$$
2 \times 1.75 \times 10^{11} \times 2048 \approx 7 \times 10^{14} \text{ FLOPs}
$$<ul><li>åºåˆ—å¯†é›†å‹è®¡ç®— (Attention)ï¼š</li></ul>$$
L \times 4 \times B \times C \times S^2 = 96 \times 4 \times 1 \times 12288 \times 2048^2 \approx 2 \times 10^{13} \text{ FLOPs}
$$<p>åœ¨ GPT-3 æ—¶ä»£ï¼ŒAttention çš„è®¡ç®—é‡ä»…ä¸ºçº¿æ€§å±‚çš„ $\frac{1}{35}\approx 2.8\%$ï¼Œç¡®å®å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚</p><p>ä½†æ˜¯ä»Šæ—¶ä¸åŒå¾€æ—¥ï¼Œç°ä»£æ¨¡å‹ï¼ˆå¦‚ Gemini 2.5 Proï¼‰å°† Context Length æå‡åˆ°äº† <a href=https://help.magai.co/en/articles/9148741-what-is-an-ai-context-limit-aka-context-window>1M çº§åˆ«</a>ã€‚æ­¤æ—¶å‡è®¾æˆ‘ä»¬ç”¨ä¸€ä¸ªåŒæ ·è§„æ¨¡çš„æ¨¡å‹å¤„ç† 1M é•¿åº¦çš„è¾“å…¥ï¼š</p><ul><li>å‚æ•°å¯†é›†å‹è®¡ç®— (Linear)ï¼šç”±äºä¸ $S$ å‘ˆçº¿æ€§å…³ç³»ï¼Œè®¡ç®—é‡å¢é•¿ 500 å€ï¼š</li></ul>$$
7 \times 10^{14} \times 500 \approx 3.5 \times 10^{17} \text{ FLOPs}
$$<ul><li>åºåˆ—å¯†é›†å‹è®¡ç®— (Attention)ï¼šç”±äºä¸ $S$ å‘ˆå¹³æ–¹å…³ç³»ï¼Œè®¡ç®—é‡å¢é•¿ $500^2 = 250,000$ å€ï¼š</li></ul>$$
2 \times 10^{13} \times 250,000 \approx 5 \times 10^{18} \text{ FLOPs}
$$<p>å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ 1M ä¸Šä¸‹æ–‡ä¸­ï¼ŒAttention çš„è®¡ç®—é‡åè¶…çº¿æ€§å±‚ 10 å€ä»¥ä¸Šï¼ è®¡ç®—ç“¶é¢ˆä»å‚æ•°è½¬ç§»åˆ°äº†åºåˆ—é•¿åº¦ã€‚è¿™ä¹Ÿè‡ªç„¶çš„å¼•å‡ºäº†æˆ‘ä»¬ä¼˜åŒ– Attention è®¡ç®—çš„åŠ¨æœºã€‚</p><h2 id=2-ç°ä»£-gpu-ç»“æ„åˆ†æ>2. ç°ä»£ GPU ç»“æ„åˆ†æ<a hidden class=anchor aria-hidden=true href=#2-ç°ä»£-gpu-ç»“æ„åˆ†æ>#</a></h2><h3 id=21-gpu-è®¡ç®—æ¶æ„åˆ†æ>2.1. GPU è®¡ç®—æ¶æ„åˆ†æ<a hidden class=anchor aria-hidden=true href=#21-gpu-è®¡ç®—æ¶æ„åˆ†æ>#</a></h3><p>æˆ‘ä»¬ä¸å¦¨ä»ç°åœ¨æœ€ä¸»æµçš„è®¡ç®—å¡ A100 ä½ä¾‹ï¼Œåˆ†æç°ä»£ GPU çš„è®¡ç®—æ¶æ„ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œåœ¨ç¡¬ä»¶ä¸Š GPU å¯ä»¥åˆ’åˆ†ä¸º GPU -> GPC (Graphics Processing Cluster) -> SM (Streaming Multiprocessor) -> Sub-SM è¿™å‡ ä¸ªå±‚çº§ã€‚ä¸‹å›¾æ˜¯ Nvidia A100 æ¶æ„å›¾ï¼Œæ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒ <a href=https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf>A100 ç™½çš®ä¹¦</a>ã€‚</p><p><img alt="Ampere global Structure" loading=lazy src=https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/Images/Full%20GA100%20Amper%20Architecture%20.png></p><p>å¯¹äºå®Œæ•´çš„ GA100 æ ¸å¿ƒï¼Œæœ‰ 8 ç»„ GPCï¼Œ128 ä¸ª SMã€‚å‡ºäºè‰¯å“ç‡ï¼ˆæˆæœ¬/åˆ€æ³•ï¼‰è€ƒè™‘ï¼ŒA100 åªä¼šå¯ç”¨ 7 ç»„ GPCï¼Œ108 ä¸ª SMã€‚æ¯ä¸ª GPC éƒ½æ˜¯ä¸€ä¸ªå®Œæ•´çš„è®¡ç®—å•å…ƒï¼ˆä¸åŒ…æ‹¬ L2-Cache å’Œ æ˜¾å­˜æ§åˆ¶å•å…ƒï¼‰ã€‚è€é»„æƒ³ç”Ÿäº§ç¬¬ä¸€çº§åˆ«çš„ GPU å°±ä¸éœ€è¦é‡æ–°è®¾è®¡ï¼Œè‡³éœ€è¦è£åˆ‡ GPC æ•°é‡å³å¯ ï¼ˆåˆ€æ³•.jpgï¼‰ã€‚</p><p><img alt="Ampere SM Architecture" loading=lazy src=https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/Images/SM%20Architecture.png></p><p>æ¯ä¸ª GPC ä¸­åŒ…å« 16 ä¸ª SMã€‚æ¯ä¸ª SM åŒ…å« 4 ä¸ª Sub-SM å’Œ ä¸€ä¸ªæä¸ºé‡è¦çš„ <strong>192KB ç»Ÿä¸€å†…å­˜å— (Unified Shared Memory / L1 Cache)</strong>ã€‚è¿™ç‚¹è¯·åŠ¡å¿…è®°ä½ï¼Œå®ƒæ˜¯ FlashAttention å‘æŒ¥é­”æ³•çš„ä¸»èˆå°ã€‚</p><p>Sub-SM æ˜¯ GPU ç¼–ç¨‹ä¸­çš„æœ€åŸºç¡€å•ä½ã€‚ä¸€ä¸ª Sub-SM åŒ…å«ï¼š</p><ul><li>16 ä¸ª INT32 CUDA Core</li><li>16 ä¸ª FP32 CUDA Core</li><li>8 ä¸ª FP64 CUDA Core (åŒç²¾åº¦)</li><li>1 ä¸ª ç¬¬ä¸‰ä»£ Tensor Core</li></ul><p>ä¸ CPU ä¸åŒï¼Œä¸€ä¸ª Sub-SM åªåŒ…å«ä¸€ä¸ªæŒ‡ä»¤è°ƒåº¦å•å…ƒ (Scheduler)ï¼Œåªæœ‰ä¸€ä¸ª PC å¯„å­˜å™¨ï¼Œå³æ‰€æœ‰çš„ Cuda Core ä»»æ„æ—¶åˆ»éƒ½åªèƒ½<strong>å¯¹ä¸åŒæ•°æ®æ‰§è¡ŒåŒä¸€æ¡æŒ‡ä»¤</strong>ï¼Œè¿™å°±æ˜¯ SIMT æ¶æ„ã€‚</p><p>ä¸€ä¸ª Cuda Core åœ¨ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸå¯ä»¥å®Œæˆä¸€æ¬¡æ ‡é‡ $a\times b + c$ è¿ç®—ã€‚è€Œ Tensor Core åˆ™å¯ä»¥åŒæ—¶ï¼ˆä¸ä¸€å®šæ˜¯ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸï¼‰å®Œæˆä¸€æ¬¡ $A \times B + C$ çš„çŸ©é˜µä¹˜æ³•ï¼Œ$A$ ä¸ $B$ çš„è§„æ¨¡å—åˆ° Tensor Core çš„ç‰ˆæœ¬çš„é€‰æ‹©çš„ç²¾åº¦å½±å“ã€‚å¯¹äº FP16 ç²¾åº¦ï¼ŒTensor Core å¯ä»¥è¿›è¡Œ $4 \times 4$ çš„çŸ©é˜µä¹˜æ³•ï¼Œå³ $4 \times 4 \times 4 = 64$ æ¬¡æ ‡é‡è¿ç®—ã€‚å¯¹äºçŸ©é˜µä¹˜æ³•ï¼ŒTensor Core æ˜¾è‘—å¿«äº Cuda Coreã€‚æˆ‘ä»¬è¯´è¿‡äº†ï¼Œä¸€ä¸ª Sub-SM æ‰€æœ‰çº¿ç¨‹å…±äº«ä¸€ä¸ª PC æŒ‡é’ˆï¼Œå› æ­¤ä¸€ä¸ª Sub-SM å†…çš„ Cuda Core å’Œ Tensor Core ä¸èƒ½å¹¶è¡Œå·¥ä½œï¼Œè€Œæ˜¯éœ€è¦äº¤æ›¿å·¥ä½œã€‚</p><p>å¦‚æœä½ å­¦ä¹ è¿‡ä¸€äº› CUDA ç¼–ç¨‹ï¼Œä¸€å®šä¼šå¯¹ Warp è¿™ä¸ªæ¦‚å¿µä¸é™Œç”Ÿã€‚Warp æ˜¯ CUDA ç¼–ç¨‹ä¸­æœ€å°çš„è°ƒåº¦å•ä½ï¼ˆæ³¨æ„ï¼Œæ˜¯ç¼–ç¨‹ä¸­çš„æœ€å°è°ƒåº¦å•ä½ï¼Œå¹¶ä¸ä¸ Sub-SM ç›´æ¥å¯¹åº”ï¼‰ï¼Œä¸€ä¸ª Warp åŒ…å« 32 ä¸ªçº¿ç¨‹ï¼Œåˆšå¥½äº¤ç»™ä¸€ä¸ª Sub-SM ä¸­çš„ Cuda Core è®¡ç®—ã€‚ç¼–ç¨‹ä¸­ä¸€ä¸ª SM æœ€å¤šå¯ä»¥åˆ†é… 64 ä¸ª Warpã€‚Warp æ•°æ˜¾è‘—å¤šäºä¸€ä¸ª SM ä¸­ Sub-SM çš„æ•°é‡ï¼ˆ4ä¸ªï¼‰ã€‚è¿™å°±æ˜¯ GPU ç¼–ç¨‹ä¸ CPU ç¼–ç¨‹æœ€å¤§çš„ä¸åŒäº†ï¼Œä¸ CPU é€šè¿‡æå‡å•çº¿ç¨‹æ€§èƒ½å’Œä¹±åºæ‰§è¡Œæ¥æ©ç›–å»¶è¿Ÿä¸åŒï¼ŒGPU é€šè¿‡å¤§é‡çº¿ç¨‹æ¥æ©ç›–å»¶è¿Ÿã€‚</p><p>ä¸Šå›¾ä¸­ä½ å¯èƒ½å·²ç»å‘ç°äº†ï¼Œæ¯ä¸ª Sub-SM éƒ½æœ‰ä¸€ä¸ªå¤§çš„å“äººçš„ Register-Files ($16384 \times 32$ bit)ã€‚æŒ‡ä»¤å‘å‡ºåï¼Œæ•°æ®ä¼šä»æ˜¾å­˜ï¼ˆHBM æˆ–è€… GDDR6ï¼‰é€æ­¥æ¬è¿åˆ° L2 Cache -> L1 Cacheã€‚ä¹‹åæ•°æ®ä¼šè¢«æ¬è¿åˆ°ç»™ Warp åˆ†é…çš„å¯¹åº”åŒºåŸŸçš„ Register Files ä¸­ã€‚å®Œæˆæ•°æ®å‡†å¤‡åï¼Œå¯¹åº”çš„ Warp ä¼šè¢« Launch è¿› Cuda Core æˆ–è€… Tensor Core è¿›è¡Œè®¡ç®—ã€‚è®¡ç®—å®Œæˆåï¼Œç»“æœä¼šè¢«å†™å› Register Filesï¼Œä¹‹åå†é€æ­¥å†™å› L1 Cache -> L2 Cache -> æ˜¾å­˜ã€‚</p><p>å¯¹äº Pytorch ä¸­çš„ä¸€ä¸ªå¼ é‡æ“ä½œï¼Œä¾‹å¦‚ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=n>A</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=mi>1024</span><span class=p>)</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>B</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=mi>1024</span><span class=p>)</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>einsum</span><span class=p>(</span><span class=s1>&#39;ij, jk -&gt; ik&#39;</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>B</span><span class=p>)</span>
</span></span></code></pre></div><p>pytorch ä¸­çš„æ‰€æœ‰å˜é‡éƒ½ä¼šè¢«ä¿å­˜åœ¨æ˜¾å­˜ä¸­ï¼Œå½“è¿›è¡Œè®¡ç®—æ—¶ï¼Œæ•°æ®ä¼šè¢«æ¬è¿åˆ° L2 Cache ä¸­ã€‚ä¹‹åæ ¹æ®è‡ªåŠ¨åˆ†é…çš„ SMï¼Œå†æ¬è¿åˆ°å¯¹åº” SM çš„ L1 Cache ä¸­ã€‚ä¸€èˆ¬è€Œè¨€çŸ©é˜µè¿ç®—éƒ½ä¼šä½¿ç”¨ TensorCore æ¥å¤„ç†ã€‚å®Œæˆè®¡ç®—åï¼Œç»“æœä¼šè¢«å†™å› L1 Cache -> L2 Cache -> æ˜¾å­˜ã€‚</p><p><strong>è¿™é‡ŒåŸ‹ä¸‹äº†ä¸€ä¸ªéšæ‚£</strong>ï¼šå¦‚æœåœ¨ HBM ä¸­é¢‘ç¹è¯»å†™å·¨å¤§çš„ä¸­é—´çŸ©é˜µï¼ˆæ¯”å¦‚ Attention Scoreï¼‰ï¼Œå³ä½¿ Tensor Core ç®—å¾—å†å¿«ï¼Œä¹Ÿä¼šè¢«æ‹¥å µçš„æ˜¾å­˜å¸¦å®½æ‹–æ­»ã€‚è¿™å°†åœ¨ä¸‹ä¸€èŠ‚è¯¦ç»†åˆ†æã€‚
æ€»ç»“ï¼š</p><h3 id=22-è®¡ç®—ä¸-io-çš„å¹³è¡¡>2.2. è®¡ç®—ä¸ IO çš„å¹³è¡¡<a hidden class=anchor aria-hidden=true href=#22-è®¡ç®—ä¸-io-çš„å¹³è¡¡>#</a></h3><p>åœ¨ç™½çš®ä¹¦ä¸­ä¸éš¾æ‰¾åˆ°ï¼š</p><table><thead><tr><th>Operation</th><th>Throughput</th></tr></thead><tbody><tr><td>FP16/BF16 Tensor Core</td><td>312 TFlops</td></tr><tr><td>HBM Memory Bandwidth</td><td>1555 GB/s</td></tr><tr><td>L1 Cache Bandwidth per SM</td><td>19 TB/s</td></tr></tbody></table><p>ä¸éš¾å‘ç°ï¼Œå¦‚æœå¸Œæœ›è®¡ç®—å’Œ IO å¹³è¡¡ï¼Œé‚£ä¹ˆæ¯ä¸ªä»æ˜¾å­˜ä¸­æ¬è¿çš„æ•°æ®éƒ½è‡³å°‘åº”è¯¥åšï¼š</p>$$
\frac{312 \times 10^{12}}{1.555 \times 10^{12}} = 200 \text{ FLOPs/Byte}
$$<p>æˆ‘ä»¬å¯ä»¥å¾ˆæ¸…æ™°çš„å°†è®¡ç®—åˆ’åˆ†ä¸ºä¸¤ç±»ï¼š</p><ul><li>è®¡ç®—ç“¶é¢ˆå‹ (Compute-bound)ï¼šæ¯ä¸ªä»æ˜¾å­˜ä¸­æ¬è¿çš„æ•°æ®å¯ä»¥åšè¶…è¿‡ 200 FLOPs çš„è®¡ç®—ã€‚ä¾‹å¦‚å¤§å‹çš„çŸ©é˜µä¹˜æ³•ã€å¤§ Channel çš„å·ç§¯ç­‰ã€‚</li><li>IO ç“¶é¢ˆå‹ (IO-bound)ï¼šæ¯ä¸ªä»æ˜¾å­˜ä¸­æ¬è¿çš„æ•°æ®åªèƒ½åšå°‘äº 200 FLOPs çš„è®¡ç®—ã€‚ä¾‹å¦‚ softmaxã€sumã€batch normã€layer normç­‰ã€‚</li></ul><p>å¯¹äºä¸åŒçš„å­˜å‚¨è®¾å¤‡ï¼Œè¿™ä¸ªå¹³è¡¡ç‚¹ä¹Ÿæ˜¯ä¸åŒçš„ã€‚ä¾‹å¦‚å¯¹äº L1 Cacheï¼š</p>$$
\frac{312 \times 10^{12}}{19 \times 10^{12}} \approx 16 \text{ FLOPs/Byte}
$$<p>å› æ­¤å¦‚æœæ•°æ®å¯ä»¥è¢«æ”¾å…¥ L1 Cache ä¸­ï¼Œé‚£ä¹ˆæ¯ä¸ªä» L1 Cache ä¸­æ¬è¿çš„æ•°æ®åªéœ€è¦åšè¶…è¿‡ 16 FLOPs çš„è®¡ç®—å³å¯ã€‚</p><h3 id=23-å°†-io-å¼€é”€å¼•å…¥å¤æ‚åº¦åˆ†æ>2.3. å°† IO å¼€é”€å¼•å…¥å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#23-å°†-io-å¼€é”€å¼•å…¥å¤æ‚åº¦åˆ†æ>#</a></h3><p>å½“æ„è¯†åˆ°äº† IO ä¸æ˜¯æ²¡æœ‰ä»£ä»·çš„ä¹‹åï¼Œæˆ‘ä»¬é‡æ–°å›åˆ° Transformer æ¡†æ¶ä¸­ï¼Œçœ‹çœ‹å¦‚æœå°† IO å¼€é”€çº³å…¥å¤æ‚åº¦åˆ†æï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚æˆ‘ä»¬ç€é‡åˆ†æå…¶ä¸­å¼€é”€æœ€å¤§çš„ï¼Œä¹Ÿæ˜¯ $O(n^2)$ çš„ Attention Score è®¡ç®—ã€‚å…¶ç®—æ³•æµç¨‹å¯ä»¥ç®€å•å†™ä¸ºï¼š</p><ul><li>çŸ©é˜µ $\mathbf{Q, K, V} \in \mathbb{R}^{N\times d}$ å­˜å‚¨åœ¨ HBM ä¸­ã€‚<ul><li>ä» HBM è¯»å– $\mathbf{Q, K}$ï¼Œè®¡ç®— $\mathbf{S} = \mathbf{QK}^T$, å¹¶å°† $\mathbf{S}$ å†™å› HBMã€‚</li><li>ä» HBM è¯»å– $\mathbf{S}$ï¼Œè®¡ç®— $\mathbf{P} = \text{softmax}(\mathbf{S})$ï¼Œå¹¶å°† $\mathbf{P}$ å†™å› HBMã€‚</li><li>ä» HBM è¯»å– $\mathbf{P, V}$ï¼Œè®¡ç®— $\mathbf{O} = \mathbf{PV}$ï¼Œå¹¶å°† $\mathbf{O}$ å†™å› HBMã€‚</li><li>è¿”å› $\mathbf{O}$ã€‚</li></ul></li></ul><p>ä¸éš¾å‘ç°ï¼Œå¯¹äºå¯¹äº $\mathbf{QK}^T$ï¼Œå¹¶å‡è®¾ä½¿ç”¨ F32 å­˜å‚¨ï¼ˆ4byteï¼‰ï¼Œæœ‰ï¼š</p><ul><li>IO å¼€é”€ï¼šè¯»å– $\mathbf{Q, K}$ï¼Œå†™å› $\mathbf{S}$ï¼Œæ€»å¤æ‚åº¦ä¸º $O(4 \times (2Nd + N^2))$</li><li>è®¡ç®—å¼€é”€ï¼Œå¤æ‚åº¦ä¸º $O(2Nd^2)$</li></ul><p>é‚£ä¹ˆç®—æ•°å¼ºåº¦æ¯”ä¸ºï¼š</p>$$
\frac{2Nd^2}{2 \times (2Nd + N^2)} \approx \frac{d}{2}
$$<p>å› ä¸º $d=64$ï¼Œé‚£ä¹ˆæ¯ä¸€ byte ä»…è¿›è¡Œ 32 FLOPsï¼Œè¿œä½äº 200 FLOPs/Byte çš„å¹³è¡¡ç‚¹ï¼Œå±äºå…¸å‹çš„ IO ç“¶é¢ˆå‹æ“ä½œã€‚å¦‚æœè§‰å¾—è¿™ä¸ªå†…å­˜ç“¶é¢ˆè¿˜å¯ä»¥æ¥å—ï¼Œå¯¹äº softmaxï¼Œæœ‰ï¼š</p><ul><li>IO å¼€é”€ï¼šè¯»å– $\mathbf{S}$ï¼Œå†™å› $\mathbf{P}$ï¼Œæ€»å¤æ‚åº¦ä¸º $O(8N^2)$</li><li>è®¡ç®—å¼€é”€ï¼Œå¤æ‚åº¦ä¸º $O(5N^2)$</li></ul><p>é‚£ä¹ˆç®—æ•°å¼ºåº¦æ¯”ä¸ºï¼š</p>$$
\frac{5N^2}{8N^2} = 0.625
$$<p>è¿™ æ˜¯ä¸€ä¸ªä½çš„ä»¤äººç»æœ›çš„ç®—æ•°å¼ºåº¦æ¯”ï¼Œå±äºéå¸¸å…¸å‹çš„ IO ç“¶é¢ˆå‹æ“ä½œã€‚äº‹å®ä¸Šå¦‚æœä½¿ç”¨äº† LayerNomrm æˆ–è€… Block Dropoutï¼Œè¿™ä¸ªé—®é¢˜ä¼šæ›´ä¸¥é‡ã€‚è¿™å¯¼è‡´å¦‚æœç›´æ¥è®¡ç®— Attention æ—¶ï¼Œç»å¤§éƒ¨åˆ†æ—¶é—´ SM éƒ½åœ¨ç©ºè½¬ï¼Œç­‰å¾…æ•°æ®ä» HBM ä¸­æ¬è¿è¿‡æ¥ï¼ˆæˆ–è€…æ¬è¿å›å»ï¼‰ã€‚è¿™å¯¼è‡´äº‹å®ä¸Š IO æˆä¸ºäº† Attention è®¡ç®—çš„ä¸»è¦ç“¶é¢ˆã€‚</p><h2 id=3-flash-attention>3. Flash Attention<a hidden class=anchor aria-hidden=true href=#3-flash-attention>#</a></h2><p>æ ¹æ®ä¸Šé¢çš„è®¨è®ºï¼Œä¸€ä¸ªæ¸…æ™°çš„ç»“è®ºæ˜¯ï¼ŒAttention é¢ä¸´ç€ä¸¥é‡çš„ IO ç“¶é¢ˆï¼Œå¯¼è‡´å¤§éƒ¨åˆ†æ—¶é—´ï¼ŒGPU éƒ½åœ¨ç©ºè½¬ï¼Œç­‰å¾…æ•°æ®æ¬è¿ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œ<a href=https://arxiv.org/pdf/2205.14135>Flash Attention</a> ç»™å‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯å°† $\mathbf{Q, K, V}$ åˆ’åˆ†ä¸ºå¾ˆå¤šå— (Tiles)ï¼Œæ¯ä¸ªå—æ”¾åœ¨ä¸€ä¸ª SM ä¸­çš„ L1 Cache ä¸­ï¼Œåœ¨ L1 Cache å®Œæˆ Attention è®¡ç®—åï¼Œå†å°†ç»“æœå†™å› HBMã€‚ è¿™æ ·å°±å¤§å¤§å‡å°‘äº†å¯¹ HBM çš„è®¿é—®æ¬¡æ•°ï¼Œä»è€Œæå‡äº† Attention è®¡ç®—çš„æ•ˆç‡ã€‚</p><h3 id=31-åˆ†å—-softmax-è®¡ç®—>3.1. åˆ†å— softmax è®¡ç®—<a hidden class=anchor aria-hidden=true href=#31-åˆ†å—-softmax-è®¡ç®—>#</a></h3><p>å‡è®¾å­˜åœ¨å‘é‡ $\mathbf{x} \in \mathbb{R}^{B}$ï¼Œsoftmax çš„è®¡ç®—è¿‡ç¨‹ä¸ºï¼š</p><ol><li>ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œè®¡ç®— $m(\mathbf{x}) = \max_i x_i$</li><li>$f(x) = \begin{bmatrix} e^{x_1 - m(\mathbf{x})}& e^{x_2 - m(\mathbf{x})}& \cdots& e^{x_B - m(\mathbf{x})} \end{bmatrix}$</li><li>$l(\mathbf{x}) = \sum_i f(x)_i$</li><li>$\text{softmax}(\mathbf{x}) = \frac{f(\mathbf{x})}{l(\mathbf{x})}$</li></ol><p>è¿™æ˜¯ä¸€ä¸ªå¾ˆç›´è§‰ç®€å•çš„è¿‡ç¨‹ï¼Œé‚£ä¹ˆå¯¹äºä¸¤ä¸ªå‘é‡ $\mathbf{x}^1, \mathbf{x}^2 \in \mathbb{R}^{B}$ï¼Œå°†å…¶æ‹¼æ¥æˆä¸€ä¸ªå‘é‡ $\mathbf{x} = \begin{bmatrix} \mathbf{x}^1 & \mathbf{x}^2 \end{bmatrix}$ï¼Œè”åˆå‘é‡ $\mathbf{x}$ çš„ softmax è®¡ç®—è¿‡ç¨‹ä¸ºï¼š</p><ol><li>è®¡ç®— $m(\mathbf{x}) = \max(m(\mathbf{x}^1), m(\mathbf{x}^2))$</li><li>$f(\mathbf{x}) = \begin{bmatrix} e^{m(\mathbf{x^1}) - m(\mathbf{x})}f(\mathbf x^1)& e^{m(\mathbf{x^2}) - m(\mathbf{x})}f(\mathbf x^2) \end{bmatrix}$</li><li>$l(\mathbf{x}) = e^{m(\mathbf{x^1}) - m(\mathbf{x})}l(\mathbf x^1) + e^{m(\mathbf{x^2}) - m(\mathbf{x})}l(\mathbf x^2)$</li><li>$\text{softmax}(\mathbf{x}) = \frac{f(\mathbf{x})}{l(\mathbf{x})}$</li></ol><h3 id=32-flash-attention-ç®—æ³•æµç¨‹>3.2. Flash Attention ç®—æ³•æµç¨‹<a hidden class=anchor aria-hidden=true href=#32-flash-attention-ç®—æ³•æµç¨‹>#</a></h3><p>è¿™å°±æ˜¯ FlashAttention æœ€æ ¸å¿ƒçš„ç®—æ³•ã€‚è¿™ä¸ªç®—æ³•å®ç°äº†å°†ä¸€ä¸ªå¤§å‘é‡çš„ softmax è®¡ç®—ï¼Œåˆ’åˆ†ä¸ºå¤šä¸ªå°å‘é‡çš„ softmax è®¡ç®—ï¼Œå¹¶ä¸”åªéœ€è¦ä¿å­˜æ¯ä¸ªå°å‘é‡çš„æœ€å¤§å€¼å’Œç´¯åŠ å’Œå³å¯ã€‚è¿™æ ·å°±å¤§å¤§å‡å°‘äº†å¯¹æ˜¾å­˜çš„è®¿é—®æ¬¡æ•°ï¼Œä»è€Œæå‡äº†è®¡ç®—æ•ˆç‡ã€‚åŸºäºè¿™ä¸ªç®—æ³•ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°† Attention åˆ’åˆ†ä¸ºå¤šä¸ªå— (Tiles)ï¼Œæ¯ä¸ªå—éƒ½æ”¾åœ¨ SM çš„ L1 Cache ä¸­è®¡ç®—ã€‚é‚£ä¹ˆï¼ŒAttention çš„è®¡ç®—æµç¨‹å¯ä»¥å†™ä½œï¼š</p><ol><li><p>çŸ©é˜µ $\mathbf{Q, K, V} \in \mathbb{R}^{N \times d}$ï¼Œå­˜å‚¨åœ¨ HBM ä¸­ï¼Œå‡è®¾ L1 Cache çš„å¤§å°ä¸º $M$ bytes.</p></li><li><p>å¯¹äº Decoder Only Attentionï¼Œæ¯ä¸ª Query äº‹å®ä¸Šåªéœ€è¦ä½¿ç”¨ä¸€æ¬¡ï¼Œå’Œä¹‹å‰åºåˆ—æ‰€æœ‰ Key å’Œ Value è®¡ç®—ã€‚å› æ­¤æˆ‘ä»¬å…ˆå°† Query åœ¨åºåˆ—æ–¹å‘åˆ†å—ï¼ŒåŠ è½½å…¥ L1 Cache ä¸­ã€‚ä¸å¦¨å¦ Query çš„å—å¤§å°ä¸º $B_q$ã€‚éœ€è¦æ³¨æ„ï¼Œåé¢çš„æ“ä½œä¸­ï¼ŒQuery æ˜¯ä¸ä¼šè¢«å†™å› HBM çš„ï¼Œæ˜¯ä¸€å—å›ºå®šå†…å­˜ã€‚æœ‰ $\mathbf{Q}_i \in \mathbb{R}^{B_q \times d}$</p></li><li><p>å»ºç«‹è¿‡ç¨‹å˜é‡</p><ul><li>$\mathbf{O}\in \mathbb{R}^{B_q \times d}$ï¼Œç”¨äºç»´æŠ¤åŠ æƒè¾“å‡ºã€‚åˆå§‹åŒ–ä¸º 0ï¼Œå­˜å‚¨åœ¨ L1 Cache ä¸­ã€‚</li><li>$\mathbf{m_i} \in \mathbb{R}^{B_q}$ï¼Œç”¨äºç»´æŠ¤ Query çš„æœ€å¤§å€¼ã€‚åˆå§‹åŒ–ä¸º $-\infty$ï¼Œå­˜å‚¨åœ¨ L1 Cache ä¸­ã€‚</li><li>$\mathbf{l_i} \in \mathbb{R}^{B_q}$ï¼Œç”¨äºç»´æŠ¤ Query çš„ç´¯åŠ å’Œã€‚åˆå§‹åŒ–ä¸º 0ï¼Œå­˜å‚¨åœ¨ L1 Cache ä¸­ã€‚</li></ul></li><li><p>ä» HBM ä¸­æµå¼åŠ è½½ $\mathbf{K_j, V_j}$ï¼Œä¸ SRAM ä¸­çš„ Query è®¡ç®— Attentionã€‚</p><ul><li>è®¡ç®—å±€éƒ¨ç›¸ä¼¼åº¦ $\mathbf S_{ij} = \mathbf{Q}_i \mathbf{K}_j^T$ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ä¸€æ­¥ä¸€èˆ¬åªå­˜å‚¨åœ¨ Register Files ä¸­ï¼Œä¸å†™å› L1 Cacheã€‚</li><li>è®¡ç®—å±€éƒ¨æœ€å¤§å€¼ $\mathbf{m_{ij}} = \max(\mathbf{m_i}, \max \text{ of } \mathbf{S_{ij}})$</li><li>è®¡ç®—å±€éƒ¨ softmax $\mathbf{p_{ij}} = \text{softmax}(\mathbf{S_{ij}} - \mathbf{m_{ij}})$</li><li>æ­¤æ—¶æˆ‘ä»¬æ‰‹ä¸Šæœ‰ä¸¤å¥—æ•°æ®ï¼Œåˆ†åˆ«æ˜¯ï¼š<ul><li>æ—§æ•°æ®ï¼š$\mathbf{m_i}, \mathbf{l_i}, \mathbf{O}$</li><li>æ–°æ•°æ®ï¼š$\mathbf{m_{ij}}, \mathbf{p_{ij}}$</li></ul></li><li>æ­¤æ—¶åªè¦åŸºäºæˆ‘ä»¬å‰é¢è®¨è®ºçš„ åˆ†å— softmax è®¡ç®—ï¼Œå°±å¯ä»¥æ›´æ–°è¿‡ç¨‹å˜é‡ï¼š<ul><li>æ›´æ–°æœ€å¤§å€¼ï¼š$\mathbf{m_i} = \mathbf{m_{ij}}$</li><li>æ›´æ–°ç¼©æ”¾å› å­ï¼š$e^{\mathbf{m_i} - \mathbf{m_{ij}}}$</li><li>æ›´æ–°ç´¯åŠ å’Œï¼š$\mathbf{l_i} = e^{\mathbf{m_i} - \mathbf{m_{ij}}} \mathbf{l_i} + \sum \mathbf{p_{ij}}$</li><li>æ›´æ–°è¾“å‡ºï¼š$\mathbf{O} = e^{\mathbf{m_i} - \mathbf{m_{ij}}} \mathbf{O} + \mathbf{p_{ij}} \mathbf{V_j}$</li></ul></li><li>å®Œæˆè®¡ç®—åï¼Œé‡Šæ”¾ $\mathbf{K_j, V_j}$ å ç”¨çš„ L1 Cache ç©ºé—´ï¼Œç»§ç»­åŠ è½½ä¸‹ä¸€å— $\mathbf{K_{j+1}, V_{j+1}}$ï¼Œé‡å¤ä¸Šè¿°è®¡ç®—ï¼Œç›´åˆ°æ‰€æœ‰ Key å’Œ Value å‡è¢«å¤„ç†å®Œã€‚</li></ul></li><li><p>å½“æ‰€æœ‰ Key å’Œ Value å‡è¢«å¤„ç†å®Œåï¼Œé™¤ä»¥ç¼©æ”¾å› å­ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡º $\mathbf{O}$</p></li><li><p>å°†è¾“å‡º $\mathbf{O}$ å†™å› HBMã€‚</p></li></ol><p>å¯ä»¥çœ‹åˆ°ï¼ŒFlash Attention åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œéœ€è¦ä¿å­˜å¦‚ä¸‹æ•°æ®ï¼š</p><ul><li>Query å— $\mathbf{Q_i} \in \mathbb{R}^{B_q \times d}$</li><li>Key å’Œ Value å— $\mathbf{K_j, V_j} \in \mathbb{R}^{B_k \times d}$</li><li>è¿‡ç¨‹å˜é‡ $\mathbf{O} \in \mathbb{R}^{B_q \times d}, \mathbf{m_i} \in \mathbb{R}^{B_q}, \mathbf{l_i} \in \mathbb{R}^{B_q}$</li></ul><p>å¦‚æœä½¿ç”¨ F32 å­˜å‚¨ï¼ˆ4 byteï¼‰ï¼Œé‚£ä¹ˆæ€»çš„å†…å­˜å¼€é”€çº¦æŸä¸ºï¼š</p>$$
\begin{aligned}
&4 \times (2 B_q \times d + 2 \times B_k \times d + 2 \times B_q) \text{ bytes}\\
&\approx 8 \times (B_q \times d + B_k \times d) \text{ bytes} <m \end{aligned} $$<p>è¿™æ ·å°±å¯ä»¥ç¡®å®šä½ çš„è®¾å¤‡ä¸Šï¼ŒQuery å’Œ Key/Value çš„å—å¤§å° $B_q, B_k$ã€‚é€šå¸¸æˆ‘ä»¬ä¼šé€‰æ‹© $B_q = B_k$ã€‚å¯¹äºå…¸å‹çš„ A100 åœºæ™¯ï¼Œ$M=192KB, d=64$ï¼Œå¯ä»¥è®¡ç®—å‡º $B_q = B_k \approx 125$ã€‚ä½†æ˜¯ä¸€èˆ¬ä¸ä¼šæçš„è¿™ä¹ˆæé™ï¼Œå¯¹äº A100ï¼ŒGPT-2 æ¨¡å‹ï¼ŒFlash Attention è®ºæ–‡ä¸­é€‰æ‹© $B_q = B_k = 64$ã€‚</p><h3 id=33-flash-attention-å¤æ‚åº¦åˆ†æ>3.3. Flash Attention å¤æ‚åº¦åˆ†æ<a hidden class=anchor aria-hidden=true href=#33-flash-attention-å¤æ‚åº¦åˆ†æ>#</a></h3><p>ä¸ä¼ ç»Ÿçš„ Attention ç›¸æ¯”ï¼ŒFlash Attention çš„è®¡ç®—å¤æ‚åº¦æ²¡æœ‰å¤ªå¤šå˜åŒ–ï¼ˆäº‹å®ä¸Šå¤šäº†ä¸€ç‚¹ç‚¹ï¼Œå› ä¸ºè¦é‡æ„softmaxï¼‰ã€‚ä½†æ˜¯ä¸»è¦çš„å¼€é”€è¿˜æ˜¯åœ¨ä¸¤æ¬¡ MatMul ä¸Šã€‚å³ $O(4N^2d)$</p><p>ä½†æ˜¯å¯¹äº IOï¼Œä¸éš¾å‘ç°ï¼š</p><ul><li>Query åªéœ€è¦ä» HBM è¯»å–ä¸€æ¬¡ï¼Œå¤æ‚åº¦ä¸º $O(Nd)$</li><li>å¯¹äºæ¯ä¸€å— Queryï¼ŒKey å’Œ Value éƒ½éœ€è¦éå†ä¸€æ¬¡<ul><li>è¯»å– Key å’Œ Valueï¼Œå¤æ‚åº¦ä¸º $O(Nd)$</li><li>ä¸€å…±éœ€è¦ $O(\frac{Nd}{M})$ æ¬¡éå†ï¼ˆå…¶ä¸­ $M$ æ˜¯ L1 Cache å¤§å°ï¼‰</li><li>å› æ­¤æ€»çš„å¤æ‚åº¦ä¸º $O(\frac{N^2 d^2}{M})$</li></ul></li><li>æœ€ç»ˆå†™å›è¾“å‡º $O(Nd)$</li></ul><p>å› æ­¤ï¼ŒFlash Attention çš„ IO å¤æ‚åº¦å¯ä»¥è¿‘ä¼¼ä¸ºï¼š$\boxed{O(\frac{N^2 d^2}{M})}$</p><p>é‚£ä¹ˆï¼Œå¯ä»¥è®¡ç®—å‡ºç®—æ•°å¤æ‚åº¦ï¼š</p>$$
\text{AI}_{\text{Flash Attention}} = \frac{4 N^2 d}{\frac{N^2 d^2}{M}} = \frac{4M}{d}
$$<p>å¯¹äº A100ï¼Œ$M=192KB, d=64$ï¼Œå¯ä»¥è®¡ç®—å‡ºï¼š</p>$$
\text{AI}_{\text{Flash Attention}} = \frac{4 \times 192 \times 1024}{64} \approx 12288 \text{ FLOPs/Byte}
$$<p>è¿™ä¸ªæ•°å€¼è¿œé«˜äº 200 FLOPs/Byte çš„å¹³è¡¡ç‚¹ï¼Œè¯´æ˜ Flash Attention å¯ä»¥å¾ˆå¥½åœ°åˆ©ç”¨ GPU çš„è®¡ç®—èƒ½åŠ›ï¼Œé¿å… IO ç“¶é¢ˆã€‚</p><p>æ›´æœ‰è¶£çš„ä¸€ç‚¹æ˜¯ï¼Œå› ä¸º FlashAttention å¹¶æ²¡æœ‰ç›´æ¥å­˜å‚¨ $\mathbf{QK}^T$ï¼Œè¿™è®©ç©ºé—´å¤æ‚åº¦ä» $O(N^2d)$ ä¼˜åŒ–ä¸º $O(Nd)$ã€‚</p><h3 id=34-åå‘ä¼ æ’­ä¼˜åŒ–>3.4. åå‘ä¼ æ’­ä¼˜åŒ–<a hidden class=anchor aria-hidden=true href=#34-åå‘ä¼ æ’­ä¼˜åŒ–>#</a></h3><p>æˆ‘ä»¬å‰é¢è¯´äº†ï¼Œåœ¨æ¨ç†æ—¶æ˜¯å®Œå…¨ä¸å­˜å‚¨ $\mathbf{QK}^T$ çš„ï¼Œä½†æ˜¯åœ¨è®­ç»ƒæ—¶ï¼Œåå‘ä¼ æ’­éœ€è¦ä½¿ç”¨åˆ° $\mathbf{QK}^T$ã€‚Flash Attention è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§æœ‰æ•ˆä½†çš„æ–¹å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨åå‘ä¼ æ’­æ—¶ï¼Œç°åœºè®¡ç®— $\mathbf{QK}^T$ï¼Œç®—å®Œå°±ç”¨ï¼Œç”¨å®Œå°±æ‰”ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„ä»¥æ—¶é—´æ¢ç©ºé—´çš„æ–¹å¼ã€‚å› ä¸º $\mathbf{QK}^T$ çš„è®¡ç®—æ˜¯ IO ç“¶é¢ˆå‹çš„ï¼Œå› æ­¤é‡æ–°è®¡ç®—å¹¶ä¸ä¼šå¸¦æ¥å¤ªå¤§çš„æ—¶é—´å¼€é”€ã€‚</p><p>åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬ä»¥ $d\mathbf{O}$ ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—è¾“å‡º $d\mathbf{V}, d\mathbf{Q}, d\mathbf{K}$ã€‚è¿™å¯¼è‡´åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å­˜å‚¨çš„å˜é‡ä¸º $d\mathbf{V}, d\mathbf{Q}, d\mathbf{K}, \mathbf{Q, K, V, O}$ã€‚å› æ­¤äº‹å®ä¸Šåœ¨åå‘ä¼ æ’­æ—¶ï¼ŒFLash Attention ä¼šé‡æ–°åˆ†é… BlockSizeã€‚åå‘ä¼ æ’­äº‹å®ä¸Šä¹Ÿæ˜¯ä¸€ç§çŸ©é˜µä¹˜æ³•çš„åˆ†å—è®¡ç®—ã€‚å…·ä½“æµç¨‹æˆ‘ä»¬å°±ä¸å…·ä½“åˆ†æäº†ã€‚</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://arxiv.org/abs/2001.08361>Scaling Laws for Neural Language Models</a></li><li><a href=https://en.wikipedia.org/wiki/Attention_(machine_learning)>Attention (Machine Learning) wikipedia</a></li><li><a href=https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf>A100 ç™½çš®ä¹¦</a></li><li><a href=https://arxiv.org/pdf/2205.14135>Flash Attention</a></li><li><a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/#introduction>cuda c++ programing guide 3. Introduction</a></li><li><a href=https://developer.nvidia.com/blog/understanding-gpu-architecture/>Understanding GPU Architecture</a></li><li><a href=https://arxiv.org/pdf/2205.14135>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://wangjv0812.cn/tags/flash-attention/>Flash Attention</a></li><li><a href=https://wangjv0812.cn/tags/transformer/>Transformer</a></li><li><a href=https://wangjv0812.cn/tags/attention/>Attention</a></li><li><a href=https://wangjv0812.cn/tags/gpu/>GPU</a></li><li><a href=https://wangjv0812.cn/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=next href=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/><span class=title>Next Â»</span><br><span>Lumine: Training an Agent to play Genshin</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=WangJV0812/WangJV-Blog-Pages data-repo-id=R_kgDOPZMmQw data-category=comments data-category-id=DIC_kwDOPZMmQ84Czj63 data-mapping=url data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://wangjv0812.cn/>WangJV Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>