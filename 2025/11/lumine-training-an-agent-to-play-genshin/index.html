<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lumine: Training an Agent to play Genshin | WangJV Blog</title><meta name=keywords content="AI-Agent,大模型,LLM"><meta name=description content="讨论字节跳动最近很有趣的一个工作，他们试着教会一个 AI 玩原神"><meta name=author content="WangJV"><link rel=canonical href=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/><link crossorigin=anonymous href=https://wangjv0812.cn/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://wangjv0812.cn/icon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://wangjv0812.cn/icon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangjv0812.cn/icon/favicon-32x32.png><link rel=apple-touch-icon href=https://wangjv0812.cn/icon/apple-touch-icon.png><link rel=mask-icon href=https://wangjv0812.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=icon type=image/png sizes=32x32 href=https://wangjv0812.cn/icon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://wangjv0812.cn/icon/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=https://wangjv0812.cn/icon/apple-touch-icon.png><link rel=manifest href=https://wangjv0812.cn/icon/site.webmanifest><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},chtml:{scale:1,minScale:.5,matchFontHeight:!1,displayAlign:"center",displayIndent:"0",mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},svg:{scale:1,minScale:.5,mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},options:{enableMenu:!0,menuOptions:{settings:{zoom:"Click"}}},loader:{load:["ui/safe","a11y/assistive-mml"]},startup:{ready(){MathJax.startup.defaultReady();const e=new ResizeObserver(e=>{MathJax.typesetPromise()});e.observe(document.body)}}},window.innerWidth<=768&&(MathJax.chtml=MathJax.chtml||{},MathJax.chtml.scale=.9)</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><style>.MathJax{outline:0}@media(max-width:768px){.MathJax{font-size:90%!important}.MathJax_Display{overflow-x:auto;overflow-y:hidden;padding:0!important;margin:1em 0!important}.MathJax_CHTML{line-height:1.2!important}}mjx-container[jax=CHTML][display=true]{overflow-x:auto;overflow-y:hidden;padding:1px 0}</style><meta property="og:url" content="https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/"><meta property="og:site_name" content="WangJV Blog"><meta property="og:title" content="Lumine: Training an Agent to play Genshin"><meta property="og:description" content="讨论字节跳动最近很有趣的一个工作，他们试着教会一个 AI 玩原神"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-26T10:22:16+08:00"><meta property="article:modified_time" content="2025-11-26T10:22:16+08:00"><meta property="article:tag" content="AI-Agent"><meta property="article:tag" content="大模型"><meta property="article:tag" content="LLM"><meta property="og:image" content="https://wangjv0812.cn/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wangjv0812.cn/"><meta name=twitter:title content="Lumine: Training an Agent to play Genshin"><meta name=twitter:description content="讨论字节跳动最近很有趣的一个工作，他们试着教会一个 AI 玩原神"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wangjv0812.cn/posts/"},{"@type":"ListItem","position":2,"name":"Lumine: Training an Agent to play Genshin","item":"https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lumine: Training an Agent to play Genshin","name":"Lumine: Training an Agent to play Genshin","description":"讨论字节跳动最近很有趣的一个工作，他们试着教会一个 AI 玩原神","keywords":["AI-Agent","大模型","LLM"],"articleBody":"我的博客只是对字节的技术报告的拙略模仿。如果希望获得更准确的信息，请直接阅读论文原文Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds\n1. Introduction and Motivation 在讨论今天的内容之前，我们不妨先思考，训练一个 MOBA 游戏的 AI（例如DOTA2、LOL 或者王者荣耀）和一个 RPG 游戏的 AI，那个更难？\n这个问题不难回答。事实上，在 RTS、MOBA、围棋等游戏上，AI 已经展现出了与人类顶尖高手持平甚至超越的水平，但是在 RPG 游戏中，却鲜有能达到一般人类水平的 AI。\n对于 围棋，在这些游戏中几乎是最简单的：\n状态空间有限，约 $10^{170}$ 规则确定，状态转移中无任何噪声 对于 MOBA 游戏，在学习上比 围棋 复杂的多，但是也在目前cloning learning 和 rl 的能力范围之内：\n状态复杂但有限：地图有限、英雄有限、技能有限 时间连续但可离散化 可以用 self-play 蒸馏巨量数据 但是 RPG 游戏就复杂的多了，它几乎就是一个仿真无噪声的真实世界：\n状态空间无限且高维 每个物品、任务、装备、交互都是状态的一部分，且使用自然语言描述 NPC 会动态行动、玩家选择导致世界变化 甚至有天气、昼夜系统、AI 同伴状态、动态任务等 长时序依赖 根据我们的讨论不难看出，RPG 几乎就是一个可以简单简单仿真的、不需要开发复杂机器人物理引擎的真实世界。研究 RPG 游戏中的 Agent 的训练和推理方式，几乎是在研究如何训练一个通用的、可以在真实世界中行动的智能体。甚至可以大胆的说，在开放世界 RPG 中训练 Agent 可能会是一条通向 AGI 之路。\n但是在讨论具体是如何教会 LLM 玩 原神 之前，我们先看看什么是 Agent。\n1.1. Agentic Model 需要什么能力？ 对于一个 LLM Powered Agent 而言，其核心能力可以总结为以下几点：\nPlanning: 可以将一个复杂任务分解为多个子任务，并给每个子任务分配子目标 Tool Use: 能够有效地使用各种工具和接口来完成任务 Memory: 能够记住过去的事件和信息，以便在未来的决策中使用 下面的讨论将围绕这几个核心能力进行\n2. 环境 智能体需通过与 “多样动态环境” 交互才能习得通用能力，而电子游戏因高效交互性与丰富动态特性，成为该领域的热门开发平台。其中，商业电子游戏（尤其是 3A 级别）优势突出：\n相对而言，原神是一个还不错的实验环境。《原神》作为一款在线第三人称动作角色扮演（action role-playing）开放世界游戏，拥有数百小时的通关内容，整合了电子游戏中常见的大部分挑战，使其成为实现我们愿景的 “理想且高要求” 的测试平台，具体体现在以下方面：\n开放世界探索，融合高保真自然景观，且具有奔跑、跳跃、攀爬、滑翔、游泳、航行、潜水等多种移动方式，要求智能体具备复杂空间理解与 3D 导航能力 长时域进程，具有数百小时连续且自然语言、图像描述的，包含战斗、解密等多种玩法的主线剧情，支线任务需长期投入 养成系统：角色与队伍养成需数周 / 数月，涉及资源分配、策略规划等长期决策，区别于传统研究中 “短时目标” 的局限，适合测试智能体的 “长时域战略规划能力”。 谜题多样，融合探索、观察、环境交互与策略推理，如 “解读地形线索激活机关”“元素技能组合触发机制”“限时激活装置”“多阶段遗迹谜题” 等。需要模型具备深刻的感知（观察线索）、逻辑（因果推理）、执行（精细操作）能力，同时考验空间认知、长期记忆与自适应问题解决能力。 全面引导与结构化教程：新玩法 / 机制首次出现时，屏幕直接提供清晰指引，且教程可存档查阅，降低智能体 “学习新规则” 的门槛。且挑战难度与玩法解锁循序渐进，符合智能体 “分阶段习得技能” 的学习规律，形成自然的训练课程。 3. Lumine 模型设计 Lumine 基于 Qwen2-VL-7B-Base 微调而来，具备多模态能力，参数量为 7B。以原始像素图像序列作为输入，生成可执行的键盘与鼠标操作，同时伴随可解释的中间推理过程。通过在这一基础上增加显式的推理与动作预测机制，Lumine 能够在交互环境中实现闭环视觉决策。\n在每个时间步骤 $t$，模型会首先基于其历史视觉观测 $\\forall o \u003c t$、先前的推理轨迹 $r \u003c t$ 与动作 $a \u003c t$，判断是否进入 【思考模式】以生成推理内容 $r_t$。若模型决定不进行显式推理，则将 $r_t$ 设为 null（空值）。随后，模型会预测下一个可执行动作 $a_t$。形式上，模型 $\\pi_\\theta$ 通过以下公式刻画推理与动作的联合分布\n$$ \\pi_\\theta(a_t, r_t \\mid o_{\\leq t}, r_{","wordCount":"162","inLanguage":"en","image":"https://wangjv0812.cn/","datePublished":"2025-11-26T10:22:16+08:00","dateModified":"2025-11-26T10:22:16+08:00","author":{"@type":"Person","name":"WangJV"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/"},"publisher":{"@type":"Organization","name":"WangJV Blog","logo":{"@type":"ImageObject","url":"https://wangjv0812.cn/icon/favicon-32x32.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangjv0812.cn/ accesskey=h title="WangJV Blog (Alt + H)"><img src=https://wangjv0812.cn/icon/translucent-icon.png alt aria-label=logo height=35>WangJV Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangjv0812.cn/ title=Home><span>Home</span></a></li><li><a href=https://wangjv0812.cn/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://wangjv0812.cn/resources/ title=Resources><span>Resources</span></a></li><li><a href=https://wangjv0812.cn/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://wangjv0812.cn/search/ title="🔍 Search (Alt + /)" accesskey=/><span>🔍 Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://wangjv0812.cn/>Home</a>&nbsp;»&nbsp;<a href=https://wangjv0812.cn/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lumine: Training an Agent to play Genshin</h1><div class=post-description>讨论字节跳动最近很有趣的一个工作，他们试着教会一个 AI 玩原神</div><div class=post-meta><span title='2025-11-26 10:22:16 +0800 +0800'>November 26, 2025</span>&nbsp;·&nbsp;WangJV&nbsp;|&nbsp;<a href=https://github.com/WangJV0812/WangJV-Blog-Source/tree/master/content/posts/Lumine:%20Training%20an%20Agent%20to%20play%20Genshin/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-introduction-and-motivation>1. Introduction and Motivation</a><ul><li><a href=#11-agentic-model-需要什么能力>1.1. Agentic Model 需要什么能力？</a></li></ul></li><li><a href=#2-环境>2. 环境</a></li><li><a href=#3-lumine-模型设计>3. Lumine 模型设计</a><ul><li><a href=#31-观测空间>3.1. 观测空间</a></li><li><a href=#32-混合思考>3.2. 混合思考</a></li><li><a href=#33-键盘和鼠标建模>3.3. 键盘和鼠标建模</a></li></ul></li><li><a href=#4-数据收集和模型训练方案>4. 数据收集和模型训练方案</a><ul><li><a href=#41-数据收集>4.1. 数据收集</a></li><li><a href=#42-pre-training>4.2. Pre-training</a></li><li><a href=#43-指令跟随>4.3. 指令跟随</a></li><li><a href=#44-推理>4.4. 推理</a></li><li><a href=#45-训练细节>4.5. 训练细节</a></li></ul></li><li><a href=#5-推理>5. 推理</a><ul><li><a href=#51-上下文管理>5.1. 上下文管理</a></li><li><a href=#52-推理实时性优化>5.2. 推理实时性优化</a></li><li><a href=#53-prompting>5.3. Prompting</a></li></ul></li><li><a href=#6-experiment-result>6. Experiment Result</a><ul><li><a href=#61-scalling-result>6.1. Scalling Result</a></li><li><a href=#62-泛化能力>6.2. 泛化能力</a></li></ul></li><li><a href=#appendix>Appendix</a><ul><li><a href=#appendix-a-lumine-混合思考模式下的输入输出格式详解>Appendix A: Lumine 混合思考模式下的输入输出格式详解</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>我的博客只是对字节的技术报告的拙略模仿。如果希望获得更准确的信息，请直接阅读论文原文<a href=https://wangjv0812.cn/files/paper/Lumine-TechnicalReport.pdf>Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</a></p><h2 id=1-introduction-and-motivation>1. Introduction and Motivation<a hidden class=anchor aria-hidden=true href=#1-introduction-and-motivation>#</a></h2><p>在讨论今天的内容之前，我们不妨先思考，训练一个 MOBA 游戏的 AI（例如DOTA2、LOL 或者王者荣耀）和一个 RPG 游戏的 AI，那个更难？</p><p>这个问题不难回答。事实上，在 RTS、MOBA、围棋等游戏上，AI 已经展现出了与人类顶尖高手持平甚至超越的水平，但是在 RPG 游戏中，却鲜有能达到一般人类水平的 AI。</p><p>对于 围棋，在这些游戏中几乎是最简单的：</p><ul><li>状态空间有限，约 $10^{170}$</li><li>规则确定，状态转移中无任何噪声</li></ul><p>对于 MOBA 游戏，在学习上比 围棋 复杂的多，但是也在目前cloning learning 和 rl 的能力范围之内：</p><ul><li>状态复杂但有限：地图有限、英雄有限、技能有限</li><li>时间连续但可离散化</li><li>可以用 self-play 蒸馏巨量数据</li></ul><p>但是 RPG 游戏就复杂的多了，它几乎就是一个仿真无噪声的真实世界：</p><ul><li>状态空间无限且高维</li><li>每个物品、任务、装备、交互都是状态的一部分，且使用自然语言描述</li><li>NPC 会动态行动、玩家选择导致世界变化</li><li>甚至有天气、昼夜系统、AI 同伴状态、动态任务等</li><li>长时序依赖</li></ul><p>根据我们的讨论不难看出，RPG 几乎就是一个可以简单简单仿真的、不需要开发复杂机器人物理引擎的真实世界。研究 RPG 游戏中的 Agent 的训练和推理方式，几乎是在研究如何训练一个通用的、可以在真实世界中行动的智能体。甚至可以大胆的说，在开放世界 RPG 中训练 Agent 可能会是一条通向 AGI 之路。</p><p>但是在讨论具体是如何教会 LLM 玩 <em>原神</em> 之前，我们先看看什么是 Agent。</p><h3 id=11-agentic-model-需要什么能力>1.1. Agentic Model 需要什么能力？<a hidden class=anchor aria-hidden=true href=#11-agentic-model-需要什么能力>#</a></h3><p><img alt="Agent System Capability" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/AgentSystemOverview.png></p><p>对于一个 LLM Powered Agent 而言，其核心能力可以总结为以下几点：</p><ul><li>Planning:<ul><li>可以将一个复杂任务分解为多个子任务，并给每个子任务分配子目标</li></ul></li><li>Tool Use:<ul><li>能够有效地使用各种工具和接口来完成任务</li></ul></li><li>Memory:<ul><li>能够记住过去的事件和信息，以便在未来的决策中使用</li></ul></li></ul><p>下面的讨论将围绕这几个核心能力进行</p><h2 id=2-环境>2. 环境<a hidden class=anchor aria-hidden=true href=#2-环境>#</a></h2><p>智能体需通过与 “多样动态环境” 交互才能习得通用能力，而电子游戏因高效交互性与丰富动态特性，成为该领域的热门开发平台。其中，商业电子游戏（尤其是 3A 级别）优势突出：</p><p>相对而言，原神是一个还不错的实验环境。《原神》作为一款在线第三人称动作角色扮演（action role-playing）开放世界游戏，拥有数百小时的通关内容，整合了电子游戏中常见的大部分挑战，使其成为实现我们愿景的 “理想且高要求” 的测试平台，具体体现在以下方面：</p><p><img alt="Overview of Gameplay in Genshin Imppact" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/Overview%20of%20gameplay%20in%20genshin.png></p><ol><li><strong>开放世界探索</strong>，融合高保真自然景观，且具有奔跑、跳跃、攀爬、滑翔、游泳、航行、潜水等多种移动方式，要求智能体具备复杂空间理解与 3D 导航能力</li><li><strong>长时域进程</strong>，具有数百小时连续且自然语言、图像描述的，包含战斗、解密等多种玩法的主线剧情，支线任务需长期投入</li><li><strong>养成系统</strong>：角色与队伍养成需数周 / 数月，涉及资源分配、策略规划等长期决策，区别于传统研究中 “短时目标” 的局限，适合测试智能体的 “长时域战略规划能力”。</li><li><strong>谜题多样</strong>，融合探索、观察、环境交互与策略推理，如 “解读地形线索激活机关”“元素技能组合触发机制”“限时激活装置”“多阶段遗迹谜题” 等。需要模型具备深刻的感知（观察线索）、逻辑（因果推理）、执行（精细操作）能力，同时考验空间认知、长期记忆与自适应问题解决能力。</li><li><strong>全面引导与结构化教程</strong>：新玩法 / 机制首次出现时，屏幕直接提供清晰指引，且教程可存档查阅，降低智能体 “学习新规则” 的门槛。且挑战难度与玩法解锁循序渐进，符合智能体 “分阶段习得技能” 的学习规律，形成自然的训练课程。</li></ol><h2 id=3-lumine-模型设计>3. Lumine 模型设计<a hidden class=anchor aria-hidden=true href=#3-lumine-模型设计>#</a></h2><p><img alt="Overview of Lumine Model" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/Overview%20of%20Lumine%20Model.png></p><p>Lumine 基于 Qwen2-VL-7B-Base 微调而来，具备多模态能力，参数量为 7B。以原始像素图像序列作为输入，生成可执行的键盘与鼠标操作，同时伴随可解释的中间推理过程。通过在这一基础上增加显式的推理与动作预测机制，Lumine 能够在交互环境中实现闭环视觉决策。</p><p>在每个时间步骤 $t$，模型会首先基于其历史视觉观测 $\forall o < t$、先前的推理轨迹 $r < t$ 与动作 $a < t$，判断是否进入 【思考模式】以生成推理内容 $r_t$。若模型决定不进行显式推理，则将 $r_t$ 设为 null（空值）。随后，模型会预测下一个可执行动作 $a_t$。形式上，模型 $\pi_\theta$ 通过以下公式刻画推理与动作的联合分布</p>$$
\pi_\theta(a_t, r_t \mid o_{\leq t}, r_{<t}, a_{<t})=\pi_\theta(r_t \mid o_{\leq t}, r_{<t}, a_{<t}) \pi_\theta(a_t \mid o_{\leq t}, r_{\leq t}, a_{<t}) $$<p>模型先根据 $o_{\leq t}, r_{<t}, a_{<t}$ 历史观测、动作和推理生成该帧的推理 $r_t=\arg\max_{r_t} \pi_\theta(r_t \mid o_{\leq t}, r_{<t}, a_{<t})$，再基于 $o_{\leq t}, r_{\leq t}, a_{<t}$ 历史观测、动作和推理生成动作 $a_t=\arg\max_{a_t}\pi_\theta(a_t \mid o_{\leq t}, r_{\leq t}, a_{<t})$。</p><p>这种分解形式体现了模型的 “感知 - 推理 - 行动”（perceive-reason-action）范式：中间推理过程会提供一个显式的潜在结构，为后续的动作生成提供引导。接下来，我们将具体定义 Lumine 的观测空间与动作空间。</p><h3 id=31-观测空间>3.1. 观测空间<a hidden class=anchor aria-hidden=true href=#31-观测空间>#</a></h3><p>Lumine 会接收游戏输出的连续视觉输入。每帧画面会被调整为 1280×720 的分辨率，选择这个分辨率是为了平衡识别 GUI 中的文字和推理效率。为与人类约 200-250 毫秒的视觉反应时间保持一致，同时避免错过关键时序事件，Lumine 每 200 毫秒处理一帧观测画面（5Hz）。</p><h3 id=32-混合思考>3.2. 混合思考<a hidden class=anchor aria-hidden=true href=#32-混合思考>#</a></h3><p><img alt="Hybrid Think Mode" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/HybridThinkMode.png></p><p>Lumine 采用 “混合思考模式”（hybrid think mode），即在某些时间步骤中进行显式推理，而在其他时间步骤中直接输出动作。Lumine 会根据当前环境状态与任务需求，自主决定是否进入思考模式。在进入思考模式时，会用 &lt;thought_start> 和 &lt;thought_end> 标记推理内容的边界，生成对当前情境的分析与决策依据。在非思考模式下，模型则直接生成动作指令，无需显式推理。</p><p>这样将 Lumine 的内心独白（思考）与动作输出隔离开，方便结构化解析。推理既承担对过往行为的反思作用，也充当后续步骤的规划依据。推理通常出现在关键转折点，例如：环境突然变化导致原有计划失效、需要调整策略时，或某一任务已完成、需提出新目标时。</p><p>对于上面的推理内容的详细分析将在 Appendix 中提供。</p><h3 id=33-键盘和鼠标建模>3.3. 键盘和鼠标建模<a hidden class=anchor aria-hidden=true href=#33-键盘和鼠标建模>#</a></h3><p>之前的一些工作会使用一个单独的动作预测头或重新定义词汇表来表示动作。但是这样做事实上没有利用模型本身对 “鼠标” 和 “键盘” 的语意的理解。而大型语言模型（LLMs）已能很好地捕捉这类语义。另有部分研究采用代码格式让模型调用 API 对这类交互进行建模，这种方式不仅表述冗长，在高频交互场景下效率也较低。</p><p>Lumine 产生的动作会使用 &lt;|action_start|> and &lt;|action_end|> 包裹，在动作内部，各组件以分号分隔，依次指定鼠标移动 $\Delta X, \Delta Y, \Delta Z$ 与一系列按键操作 $K_1, K_2, \cdots, K_6$</p><h4 id=鼠标移动>鼠标移动<a hidden class=anchor aria-hidden=true href=#鼠标移动>#</a></h4><p>Lumine 将鼠标移动操作离散化，将鼠标位移规范 $\Delta X, \Delta Y$ 为 $(-1000, 1000)$ 的整数，而将滚轮位移预测 $\Delta Z$ 规范为 $[-5, 5]$ 的整数。并且在每个时间步（200ms） 中完成预测的位移。</p><h4 id=键盘操作>键盘操作<a hidden class=anchor aria-hidden=true href=#键盘操作>#</a></h4><p>为了捕捉足够高频的键盘操作，Lumine 在每个时间步内直接预测出 6 个键盘操作序列。在该时间步骤的 200ms 内，每个键盘操作块 $K_i$ 代表连续 33ms 内的按键状态，实现 30hz 的控制频率。每个块包含在该段时间中需要按下的 0-4 个按键（包含鼠标左右键和键盘按键）。未在动作块中列出的键会自动释放；连续动作块中重复出现的键会保持 “按下” 状态。</p><p>一个可能的 Action 输出大概长这样：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;action_start&gt;</span>
</span></span><span class=line><span class=cl>92 0 0 ; Shift W ; Shift W ; Shift W ; F W ; F W ; F
</span></span><span class=line><span class=cl><span class=nt>&lt;action_end&gt;</span>
</span></span></code></pre></div><p>该动作描述了智能体的操作过程：朝着右侧的宝箱冲刺（按住 Shift 键和 W 键）（同时鼠标向右转动 92 个单位），抵达宝箱位置后停下，并尝试打开宝箱。下表是 Lumine 的 “键位表”：</p><table><thead><tr><th>Key</th><th>Token</th><th>Key</th><th>Token</th><th>Key</th><th>Token</th><th>Key</th><th>Token</th><th>Key</th><th>Token</th><th>Key</th><th>Token</th></tr></thead><tbody><tr><td>LMB</td><td>LB</td><td>7</td><td>seven</td><td>H</td><td>H</td><td>R</td><td>R</td><td>F2</td><td>Two</td><td>F12</td><td>Twelve</td></tr><tr><td>RMB</td><td>RB</td><td>8</td><td>eight</td><td>I</td><td>I</td><td>S</td><td>S</td><td>F3</td><td>Three</td><td>Esc</td><td>Esc</td></tr><tr><td>MMB</td><td>MB</td><td>9</td><td>nine</td><td>J</td><td>J</td><td>T</td><td>T</td><td>F4</td><td>Four</td><td>Tab</td><td>Tab</td></tr><tr><td>0</td><td>zero</td><td>A</td><td>A</td><td>K</td><td>K</td><td>U</td><td>U</td><td>F5</td><td>Five</td><td>Caps</td><td>Caps</td></tr><tr><td>1</td><td>one</td><td>B</td><td>B</td><td>L</td><td>L</td><td>V</td><td>V</td><td>F6</td><td>Six</td><td>Shift</td><td>Shift</td></tr><tr><td>2</td><td>two</td><td>C</td><td>C</td><td>M</td><td>M</td><td>W</td><td>W</td><td>F7</td><td>Seven</td><td>Ctrl</td><td>Ctrl</td></tr><tr><td>3</td><td>three</td><td>D</td><td>D</td><td>N</td><td>N</td><td>X</td><td>X</td><td>F8</td><td>Eight</td><td>Alt</td><td>Alt</td></tr><tr><td>4</td><td>four</td><td>E</td><td>E</td><td>O</td><td>O</td><td>Y</td><td>Y</td><td>F9</td><td>Nine</td><td>Space</td><td>Space</td></tr><tr><td>5</td><td>five</td><td>F</td><td>F</td><td>P</td><td>P</td><td>Z</td><td>Z</td><td>F10</td><td>Ten</td><td></td><td></td></tr><tr><td>6</td><td>six</td><td>G</td><td>G</td><td>Q</td><td>Q</td><td>F1</td><td>One</td><td>F11</td><td>Eleven</td><td></td><td></td></tr></tbody></table><h2 id=4-数据收集和模型训练方案>4. 数据收集和模型训练方案<a hidden class=anchor aria-hidden=true href=#4-数据收集和模型训练方案>#</a></h2><p>Lumine 在 70B 参数 Qwen2-VL-7B-Base 基础上进行后训练。后训练过程包含三个阶段：</p><p><img alt="Three-Stage Training Recipe" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/Training%20recipe.png></p><ol><li><strong>预训练</strong>：使用 1731 小时的人类游戏视频数据让模型掌握基本的动作关系。模型从不同场景的原始观测数据中学习如何行动，进而形成稳健且具有响应性的控制行为。</li><li><strong>微调</strong>：使用 200 小时的指令跟随数据，让模型掌握使用自然语言控制游戏的能力。智能体的动作以自然语言为基础，使其能够遵循文本指令完成短期任务（short-horizon tasks）。</li><li><strong>强化学习</strong>：15 小时格式清晰的推理数据，让模型对齐混合推理的格式化输出。模型学习执行明确的推理，以指导后续的动作生成，从而支持复杂的长期决策（long-horizon decision making）。</li></ol><h3 id=41-数据收集>4.1. 数据收集<a hidden class=anchor aria-hidden=true href=#41-数据收集>#</a></h3><p>Lumine 收集了 2424 小时的人类游戏数据集，包括游戏的视频流和与之匹配的键鼠操作数据。收集的数据为使用全新原神账号，通关蒙德部分主线剧情，并使用系统提供的角色完成 80% 的地图探索度。对于人类玩家，完成这些探索大约需要 30 小时。</p><p><img alt="Data pipline" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/data_pipeline.png></p><h3 id=42-pre-training>4.2. Pre-training<a hidden class=anchor aria-hidden=true href=#42-pre-training>#</a></h3><p>预训练阶段的核心目标是让在大规模广泛数据上训练的 VLM 适应学习原神这一任务的视觉视觉动作关系。对于收集的 2424 小时人类数据，我们先通过 rulebased filter 去除无效的闲置动作和镜头抖动为主的数据段，得到 1731 小时的高质量 gameplay 数据用于 Lumine 的预训练。</p><p>如果只使用原神 gameplay 数据预训练，可能导致模型 “遗忘” 之前掌握的通用知识和推理能力，事实上这些能力对于后期的指令对齐和推理至关重要。因此除了原神 Gameplay 数据外，还添加了 $20\%$ 的 web 多模态数据和通用指数数据。此外，该阶段还使用了少量的指令跟随数据。剩余的指令跟随数据则用于评估 pre-training 阶段的模型性能。</p><h3 id=43-指令跟随>4.3. 指令跟随<a hidden class=anchor aria-hidden=true href=#43-指令跟随>#</a></h3><p>实验展示出，与训练的 Lumine 已经可以本能的和周围的物体、NPC 和敌人 等进行互动，但是在不同的行为模式中转换时。仅通过预训练的模型已经可以完成某一些单一场景的任务（例如采集、解密、战斗、与 NPC 交互等等）。但是无法自己驱动自己去完成一个复杂任务中不同场景下的切换。在指令跟随训练阶段。我们希望做到通过自然语言指令来引导模型执行不同的任务。</p><p>因此在数据集构建阶上，我们需要给出与 Gameplay 对应的自然语言描述的指令。让模型对齐自然语言指令与对应的动作。</p><p><img alt="puzzle instructor" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/puzzle%20instructor.png></p><p>指令跟随阶段的数据集构建起始于人工数据标注。我们设计了 38 中，三个级别的分类体系，同时按感玩法的宏观和微观层面。</p><p><img alt="Classifier dataset distribution" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/classifier_dataset_distribution.png></p><p>为了确保标注的准确性和一致性，我们为每个标注员提供了明确的定义和示例。标注员需要对每个 20 s 的视频片段做出完整的分割，并标注出每个片段精确的起始和结束时间。通过人工标注，我们得到了 165 小时高质量、细颗粒度的游戏分类数据。但是 165 小时事实上并不够多。为了进一步扩展自动标注，我们微调了一个 Qwen2-VL-2B 的多模态 LLM 分类器。该分类器的核心功能是通过分析视频帧序列，自动对游戏内容进行分类。该模型接收固定长度的 5 帧连续帧作为输入，并输出一个描述游戏内活动的单一类别标签。</p><p>我们首先将具有相同标签的连续帧合并为变长视频片段；从每个片段中均匀采样 5 帧，构成一个训练样本（帧数少于 5 帧的片段通过重复最后一帧进行补全）。</p><p>随后，我们使用该分类器对所有原始游戏数据进行标注，并识别出 “被分类器分配了不同标签的相邻游戏片段” 之间的过渡点。这些过渡点往往意味着游戏模式或者任务场景的转变。在过渡点附近，我们提取 20 帧（4s）的片段，并通过 GPT-4.1 基于分类器的标签生成基于标注类别的，多样的上下文相关指令。这个过程除了丰富了数据的多样性，还可以基于 GPT4.1 强大的能力对分类器的标签做了验证甚至修正。在进行与预训练相同的过滤后，获得了 200 小时的高质量指令跟随数据。</p><h3 id=44-推理>4.4. 推理<a hidden class=anchor aria-hidden=true href=#44-推理>#</a></h3><p>基于前两个阶段训练的模型，Lumine 已经具备了遵循文本指导生成动作的能力。进一步需要增强其推理能力，以支持自主探索和长时程决策。为了实现这一目标，我们基于 <em>原神</em> 第一章 <em>捕风的异乡人</em> 主线剧情，从人类玩家执行该任务的原始数据中选取了 27 段游戏视频。为标注员提供 10s 连续视频，让他们在帧序列上标注出关键的决策点，并以第一人称撰写思考内容，简洁准确的蝉鸣行动背后的逻辑。</p><p>这一过程完成了 15 小时的游戏内容标注，其中包含 15k 条推理轨迹。连续思考内容之间的平均间隔为 3.2 秒。每个推理序列平均包含 37.4±11.7 个词元（token）。</p><p>为更好地贴合真实推理场景，我们未采用任何动作过滤，使 Lumine 能够学习在关键决策点进行适当等待。随后，我们在该数据集上对 Lumine 模型进行微调，最终得到 Lumine-Thinking 模型 —— 这是一个无需人工干预即可完成时长数小时任务的自主模型。</p><h3 id=45-训练细节>4.5. 训练细节<a hidden class=anchor aria-hidden=true href=#45-训练细节>#</a></h3><p>在训练中，Lumine 尝试了 无历史信息 和 有历史信息 两种训练方式。在无历史信息的设置中，模型仅接收当前时间步的视觉观测作为输入；而在有历史信息的设置中，模型会接收过去 $k$ 个时间步的视觉观测、动作和推理作为上下文。从实验结果来看，在预训练阶段：有历史信息设置下，多轮训练（多训练轮次）仍能持续带来性能提升；而无历史信息设置下，模型在训练至第二轮后容易出现过拟合现象。</p><p><img alt="Training Information" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/Training%20information.png></p><h2 id=5-推理>5. 推理<a hidden class=anchor aria-hidden=true href=#5-推理>#</a></h2><h3 id=51-上下文管理>5.1. 上下文管理<a hidden class=anchor aria-hidden=true href=#51-上下文管理>#</a></h3><p>为了保证 Lumine 可以完成数小时的推理，在上下文上，采用了滑动窗口机制。我们将上下文分为：</p><ol><li>系统提示词：永远保留</li><li>reasoning token：思考产生的指令，可以设计性保留，视频展示的是保留最近的一条推理</li><li>视觉观测 和 Action：FIFO，只保留最近的 20 条。</li></ol><p><img alt="Lumine Inference" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/inference.png></p><h3 id=52-推理实时性优化>5.2. 推理实时性优化<a hidden class=anchor aria-hidden=true href=#52-推理实时性优化>#</a></h3><p>在推理时，使用了两台设备，一台运行游戏客户端，另一台推理服务器则运行 Lumine 模型。推理过程可以简单的这样描述：</p><ol><li>捕捉游戏画面，发送给推理服务器</li><li>推理服务器进行视觉编码和文本解码，生成动作指令</li><li>将动作指令发送回游戏客户端，执行相应的键鼠操作</li></ol><p>但是希望一个 7B 模型可以在 200ms 内完成一次推理是比较困难的。因此我们使用了一些优化手段来提升推理速度。</p><ol><li>流式输出： Lumine 将 autoregressive 生成的完整动作序列拆分为 6 个连续 “动作块”（每个块对应 33ms 的键盘 / 鼠标操作），一旦生成单个完整块（以分号为结束标志），无需等待全序列完成即可立即执行。这一设计将 “200ms 内生成全序列” 的严格约束，放松为 “每个 33ms 内生成单个块”，大幅降低时序压力。</li><li>prefill：使用 KV Cache，避免重复计算上下文</li><li>StreamingLLM：即前面提到的上下文管理策略。</li><li>张量并行：将模型权重与 KV Head 分配到 4 张 H20 上，提高吞吐量</li><li>量化：W8A8，减少计算量和通讯带宽</li></ol><p><img alt="Inference Optimization" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/Inference%20Optimization.png></p><h3 id=53-prompting>5.3. Prompting<a hidden class=anchor aria-hidden=true href=#53-prompting>#</a></h3><p>中文本本指令跟随阶段的提示词设计如下：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>你是一名经验丰富的原神PC端玩家，精通键盘鼠标操作。请根据当前游戏画面，规划接下来 200ms 的操作，由 6 步组成，每步间隔 33ms。每步动作在执行时刻起持续 33ms，直至下一步开始。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**输出格式**
</span></span><span class=line><span class=cl>  &lt;|action_start|&gt;X Y Z ; k1 k2 k3 ; k4 k5 ; k6 ; k7 ; k8 ; k9 k10&lt;|action_end|&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**说明**
</span></span><span class=line><span class=cl>1. **鼠标位移**：首先给出相对位移X,Y（X&gt;0 右移，Y&gt;0 下移）以及滚轮量Z（Z&gt;0 上滚）。
</span></span><span class=line><span class=cl>2. **按键序列**：随后列出 6 组按键；同组内用空格分隔，不同组用分号分隔。
</span></span><span class=line><span class=cl>  - 每组最多 4 个按键。
</span></span><span class=line><span class=cl>  - 若某组无按键，留空但保留 ’;’。
</span></span><span class=line><span class=cl>3. 只输出符合上述格式的纯字符串，不换行、不加引号。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**按键命名规范**
</span></span><span class=line><span class=cl>  - 数字键 ’1–9’：使用小写英文，如 ’one’ 表示键盘上的 ’1’。
</span></span><span class=line><span class=cl>  - 功能键 ’F1–F12’：使用首字母大写英文单词，如 ’One’ 表示 ’F1’，’Two’ 表示 ’F2’，依此类推。
</span></span><span class=line><span class=cl>  - 其他按键（如字母、Shift、Tab、Space 等）：统一使用首字母大写的真实键盘名称，如 ’A’、’D’、’Shift’、’Space’ 等。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>你当前要完成的任务是：&lt;instruction&gt;
</span></span></code></pre></div><p>中文指令下自动思考的提示词是：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>你是一名资深原神PC玩家，熟练使用键盘和鼠标进行高水平操作。你熟知游戏机制与战斗节奏，能够从实时画面中迅速提取关键信息，在关键时刻进行思考并做出精准决策。请根据当前画面，规划接下来 200ms 的操作，由 6 步组成，每步间隔 33ms。每步动作在执行时刻起持续 33ms，直至下一步开始。若当前情境延续前一分析策略，可直接输出动作；仅当局势发生明显变化、先前分析失效或出现新目标时，需进行必要的思考并输出思考内容。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**输出格式**
</span></span><span class=line><span class=cl>- **仅动作（常用）**
</span></span><span class=line><span class=cl>  &lt;|action_start|&gt;X Y Z ; k1 k2 k3 ; k4 k5 ; k6 ; k7 ; k8 ; k9 k10&lt;|action_end|&gt;
</span></span><span class=line><span class=cl>- **思考 + 动作（必要时）**
</span></span><span class=line><span class=cl>  &lt;|thought_start|&gt;思考内容&lt;|thought_end|&gt;&lt;|action_start|&gt;X Y Z ; k1 k2 k3 ; k4 k5 ; k6 ; k7 ; k8 ;k9 k10&lt;|action_end|&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**说明**
</span></span><span class=line><span class=cl>1. **鼠标位移**：首先给出相对位移X,Y（X&gt;0 右移，Y&gt;0 下移）以及滚轮量Z（Z&gt;0 上滚）。
</span></span><span class=line><span class=cl>2. **按键序列**：随后列出 6 组按键；同组内用空格分隔，不同组用分号分隔。
</span></span><span class=line><span class=cl>  - 每组最多 4 个按键。
</span></span><span class=line><span class=cl>  - 若某组无按键，留空但保留 ’;’。
</span></span><span class=line><span class=cl>3. 只输出符合上述格式的纯字符串，不换行、不加引号。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**按键命名规范**
</span></span><span class=line><span class=cl>  - 数字键 ’1–9’：使用小写英文，如 ’one’ 表示键盘上的 ’1’。
</span></span><span class=line><span class=cl>  - 功能键 ’F1–F12’：使用首字母大写英文单词，如 ’One’ 表示 ’F1’，’Two’ 表示 ’F2’，依此类推。
</span></span><span class=line><span class=cl>  - 其他按键（如字母、Shift、Tab、Space 等）：统一使用首字母大写的真实键盘名称，如 ’A’、’D’、’Shift’、
</span></span><span class=line><span class=cl>’Space’ 等。
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**当前目标**
</span></span><span class=line><span class=cl>&lt;cur_thought&gt;
</span></span></code></pre></div><h2 id=6-experiment-result>6. Experiment Result<a hidden class=anchor aria-hidden=true href=#6-experiment-result>#</a></h2><h3 id=61-scalling-result>6.1. Scalling Result<a hidden class=anchor aria-hidden=true href=#61-scalling-result>#</a></h3><p>Lumine 团队为了测试不同规模的模型在不同数据量下展现出的性能和理解在不同阶段 pretraining 学习到了什么能力，使用 Qwen2-VL-Base 系列的 2B、7B 两个模型在不同数据规模下做预训练，并在不同类型的任务上测试模型的成功率：</p><p><img alt="Scalling experiment" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/bc_scaling_loss.png></p><p>可以看到，随着数据量增加，7B 和 2B 模型的 Loss 都始终下降，7B 模型的 Loss 明显低于 2B 模型。虽然 2B 模型的 Loss 始终下降，但在 1.2k 小时数据后，模型的平均成果律反而下降，模型以及出现过拟合。</p><p>Lumine 团队用 7B 模型测试了不同数据量下，不同任务的表现：</p><p><img alt=bc_bmk_user_study loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/bc_bmk_user_study.png></p><p>可以看到，模型在 100h 左右就已经具备了基本的交互能力，在 1000h 左右具备了复杂的交互能力，而游戏交互和导航这种复杂问题则在 1800h 左右还有提升。</p><h3 id=62-泛化能力>6.2. 泛化能力<a hidden class=anchor aria-hidden=true href=#62-泛化能力>#</a></h3><p>试验展示，完成 response 训练的模型在未见过的任务和场景中表现出色。例如，在 “解谜” 任务中，模型能够识别并利用环境线索，成功解锁机关；在 “战斗” 任务中，模型展现出灵活的战术调整能力，应对不同敌人和地形变化。这表明 Lumine 不仅掌握了特定任务的操作技巧，还具备了迁移学习的能力，能够将学到的知识应用于新的情境中。</p><p>甚至在相同类型不同游戏（鸣潮）、不同类型类似画风游戏（崩坏星穹铁道）乃至几乎完全不同的 RPG 游戏（黑神话悟空）中，都展现出强大的适应能力。</p><p>鸣潮同样是一个开放世界 RPG 游戏。其中，Lumine 使用 107 min 完成了第一章主线任务，而人类玩家则平均需要 101 分钟。</p><p>崩坏星穹铁道 是一个箱庭策略回合制游戏，Lumine 在完全没有任何微调的情况下使用 7 小时完成第一章主线，而人类平均时间为 4.7 小时。制约 Lumine 表现的额核心制约是导航和战斗，也是 崩铁 和 原神 的核心区别。</p><p>黑神话悟空是一个拟真渲染的动作游戏。Lumine 可以完成最基本的移动和操作，但是无法很好的完成战斗。核心制约在于游戏间地图设计模式和战斗模式上的区别。</p><p><img alt="alt text" loading=lazy src=https://wangjv0812.cn/2025/11/lumine-training-an-agent-to-play-genshin/Images/odd%20gameplay.png></p><h2 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h2><h3 id=appendix-a-lumine-混合思考模式下的输入输出格式详解>Appendix A: Lumine 混合思考模式下的输入输出格式详解<a hidden class=anchor aria-hidden=true href=#appendix-a-lumine-混合思考模式下的输入输出格式详解>#</a></h3><table><thead><tr><th>内容</th><th>含义解释</th></tr></thead><tbody><tr><td><code>&lt;im_start>user</code></td><td>标记“<strong>用户侧输入片段的开始</strong>”：- “user”并非指真实人类用户，而是模型输入流程中“视觉观测数据的来源标识”；- 后续内容为模型接收的环境信息（主要是游戏像素帧），确保模型知道“接下来的信息是外部观测”。</td></tr><tr><td><code>&lt;im_start>assistant</code></td><td>标记“<strong>助手侧输出片段的开始</strong>”：- “assistant”对应 Lumine 模型自身的输出部分，告诉模型“接下来要生成推理或动作，用于与输入区分”；- 这是模型输出的“启动信号”，后续内容为决策相关的文本。</td></tr><tr><td><code>&lt;thought_start></code></td><td>标记“<strong>显式推理的开始</strong>”：- 仅在思考模式出现，与后续 <code>&lt;thought_end></code> 配合，界定“内心独白式推理”的边界；- 作用是让模型区分“推理逻辑”和“动作指令”，避免决策混乱。</td></tr><tr><td><code>{reasoning_content}</code></td><td>推理内容占位符：- 代表模型生成的显式推理文本（如“我需要先击败前方怪物，才能打开宝箱”“当前风障需要收集风种子激活风场才能进入”）；- 功能是引导后续动作生成，确保决策连贯（如推理“要击败怪物”后，动作会包含攻击操作），同时提升可解释性。</td></tr><tr><td><code>&lt;thought_end></code></td><td>与 <code>&lt;thought_start></code> 对应，标记“<strong>推理内容的结束</strong>”：- 明确推理部分收尾，避免与后续动作指令混淆；- 是“推理→动作”的过渡信号，告诉模型“接下来可以生成执行动作”。</td></tr><tr><td><code>&lt;action_start></code></td><td>标记“<strong>可执行动作指令的开始</strong>”：- 无论思考/非思考模式都存在，是动作生成的“启动标记”；- 后续内容为键盘鼠标操作的文本表示，格式严格遵循“鼠标位移+按键序列”（如“92 0 0 ; Shift W ; Shift W ; &mldr;”）。</td></tr><tr><td><code>{action_content}</code></td><td>动作内容占位符：- 代表具体的键盘鼠标操作，格式为“∆X ∆Y ∆Z ; K1 ; K2 ; K3 ; K4 ; K5 ; K6”（鼠标相对位移+6个33ms的按键块）；- 例如“0 0 0 ; F ; ; ; ; ; ”代表“按下F键（交互），无鼠标移动”，对应30Hz的交互频率。</td></tr><tr><td><code>&lt;action_end>&lt;im_end></code></td><td>- <code>&lt;action_end></code>：标记“动作指令的结束”，明确单次动作的范围；- <code>&lt;im_end></code>：与 <code>&lt;im_start>assistant</code> 对应，标记“助手侧输出片段的整体结束”；- 两者配合，完成一次“思考→动作”的输出闭环。</td></tr></tbody></table><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p><a href=https://arxiv.org/abs/2511.08892>Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://wangjv0812.cn/tags/ai-agent/>AI-Agent</a></li><li><a href=https://wangjv0812.cn/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/>大模型</a></li><li><a href=https://wangjv0812.cn/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://wangjv0812.cn/2025/12/flashattention-1-complexity-analysis-of-transformer/><span class=title>« Prev</span><br><span>FlashAttention-1 Complexity Analysis of Transformer</span>
</a><a class=next href=https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/><span class=title>Next »</span><br><span>From Diffusion to Diffusion Language Model</span></a></nav></footer><script src=https://giscus.app/client.js data-repo=WangJV0812/WangJV-Blog-Pages data-repo-id=R_kgDOPZMmQw data-category=comments data-category-id=DIC_kwDOPZMmQ84Czj63 data-mapping=url data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=zh-CN crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://wangjv0812.cn/>WangJV Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>