<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Denoising Score Matching | WangJV Blog</title><meta name=keywords content="神经网络,生成模型,Denoising Score Matching,diffusion,无监督学习"><meta name=description content="1. 动机
以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 NCE 中对于数据流形 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。
我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\theta$ 控制的概率分布 $q(x\mid \theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：

我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\{x_1, x_2, \cdots x_n\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\theta$，使得 $q(x\mid \theta)$ 尽可能接近 $p(x)$。
对于一个完成学习的分布 $p(x\mid \theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。

这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 Score Matching 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。
1.1. Denoising AutoEncoder
AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。
为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。
形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\epsilon \sim \mathcal N(x\mid 0, \sigma^2I)$，有被污染的数据：
$$
\tilde{x} = x + \epsilon
$$那么，损失可以写作：
$$
J_{DATA}(\theta) = \mathbb{E}\left(\left\|
\text{Decoder}(\text{Encoder} (x + \epsilon)) - x
\right\|^2\right)
$$1.2. Score Matching
我们之前在 Score Matching 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 Fisher Divergence。"><meta name=author content="WangJV"><link rel=canonical href=https://wangjv0812.cn/2025/10/denoising-score-matching/><link crossorigin=anonymous href=https://wangjv0812.cn/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://wangjv0812.cn/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wangjv0812.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wangjv0812.cn/favicon-32x32.png><link rel=apple-touch-icon href=https://wangjv0812.cn/apple-touch-icon.png><link rel=mask-icon href=https://wangjv0812.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://wangjv0812.cn/2025/10/denoising-score-matching/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},chtml:{scale:1,minScale:.5,matchFontHeight:!1,displayAlign:"center",displayIndent:"0",mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},svg:{scale:1,minScale:.5,mtextInheritFont:!1,merrorInheritFont:!0,mathmlSpacing:!1,skipHtmlTags:["script","noscript","style","textarea","pre","code","a"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},options:{enableMenu:!0,menuOptions:{settings:{zoom:"Click"}}},loader:{load:["ui/safe","a11y/assistive-mml"]},startup:{ready(){MathJax.startup.defaultReady();const e=new ResizeObserver(e=>{MathJax.typesetPromise()});e.observe(document.body)}}},window.innerWidth<=768&&(MathJax.chtml=MathJax.chtml||{},MathJax.chtml.scale=.9)</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><style>.MathJax{outline:0}@media(max-width:768px){.MathJax{font-size:90%!important}.MathJax_Display{overflow-x:auto;overflow-y:hidden;padding:0!important;margin:1em 0!important}.MathJax_CHTML{line-height:1.2!important}}mjx-container[jax=CHTML][display=true]{overflow-x:auto;overflow-y:hidden;padding:1px 0}</style><meta property="og:url" content="https://wangjv0812.cn/2025/10/denoising-score-matching/"><meta property="og:site_name" content="WangJV Blog"><meta property="og:title" content="Denoising Score Matching"><meta property="og:description" content="1. 动机 以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 NCE 中对于数据流形 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。
我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\theta$ 控制的概率分布 $q(x\mid \theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：
我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\{x_1, x_2, \cdots x_n\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\theta$，使得 $q(x\mid \theta)$ 尽可能接近 $p(x)$。 对于一个完成学习的分布 $p(x\mid \theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。 这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 Score Matching 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。
1.1. Denoising AutoEncoder AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。
为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。
形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\epsilon \sim \mathcal N(x\mid 0, \sigma^2I)$，有被污染的数据：
$$ \tilde{x} = x + \epsilon $$那么，损失可以写作：
$$ J_{DATA}(\theta) = \mathbb{E}\left(\left\| \text{Decoder}(\text{Encoder} (x + \epsilon)) - x \right\|^2\right) $$1.2. Score Matching 我们之前在 Score Matching 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 Fisher Divergence。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-13T20:40:25+08:00"><meta property="article:modified_time" content="2025-10-13T20:40:25+08:00"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="生成模型"><meta property="article:tag" content="Denoising Score Matching"><meta property="article:tag" content="Diffusion"><meta property="article:tag" content="无监督学习"><meta property="og:image" content="https://wangjv0812.cn/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wangjv0812.cn/"><meta name=twitter:title content="Denoising Score Matching"><meta name=twitter:description content="1. 动机
以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 NCE 中对于数据流形 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。
我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\theta$ 控制的概率分布 $q(x\mid \theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：

我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\{x_1, x_2, \cdots x_n\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\theta$，使得 $q(x\mid \theta)$ 尽可能接近 $p(x)$。
对于一个完成学习的分布 $p(x\mid \theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。

这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 Score Matching 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。
1.1. Denoising AutoEncoder
AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。
为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。
形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\epsilon \sim \mathcal N(x\mid 0, \sigma^2I)$，有被污染的数据：
$$
\tilde{x} = x + \epsilon
$$那么，损失可以写作：
$$
J_{DATA}(\theta) = \mathbb{E}\left(\left\|
\text{Decoder}(\text{Encoder} (x + \epsilon)) - x
\right\|^2\right)
$$1.2. Score Matching
我们之前在 Score Matching 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 Fisher Divergence。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wangjv0812.cn/posts/"},{"@type":"ListItem","position":2,"name":"Denoising Score Matching","item":"https://wangjv0812.cn/2025/10/denoising-score-matching/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Denoising Score Matching","name":"Denoising Score Matching","description":"1. 动机 以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 NCE 中对于数据流形 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。\n我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\\theta$ 控制的概率分布 $q(x\\mid \\theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：\n我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\\{x_1, x_2, \\cdots x_n\\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\\theta$，使得 $q(x\\mid \\theta)$ 尽可能接近 $p(x)$。 对于一个完成学习的分布 $p(x\\mid \\theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。 这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 Score Matching 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。\n1.1. Denoising AutoEncoder AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。\n为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。\n形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\\epsilon \\sim \\mathcal N(x\\mid 0, \\sigma^2I)$，有被污染的数据：\n$$ \\tilde{x} = x + \\epsilon $$那么，损失可以写作：\n$$ J_{DATA}(\\theta) = \\mathbb{E}\\left(\\left\\| \\text{Decoder}(\\text{Encoder} (x + \\epsilon)) - x \\right\\|^2\\right) $$1.2. Score Matching 我们之前在 Score Matching 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 Fisher Divergence。\n","keywords":["神经网络","生成模型","Denoising Score Matching","diffusion","无监督学习"],"articleBody":"1. 动机 以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 NCE 中对于数据流形 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。\n我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\\theta$ 控制的概率分布 $q(x\\mid \\theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：\n我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\\{x_1, x_2, \\cdots x_n\\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\\theta$，使得 $q(x\\mid \\theta)$ 尽可能接近 $p(x)$。 对于一个完成学习的分布 $p(x\\mid \\theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。 这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 Score Matching 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。\n1.1. Denoising AutoEncoder AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。\n为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。\n形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\\epsilon \\sim \\mathcal N(x\\mid 0, \\sigma^2I)$，有被污染的数据：\n$$ \\tilde{x} = x + \\epsilon $$那么，损失可以写作：\n$$ J_{DATA}(\\theta) = \\mathbb{E}\\left(\\left\\| \\text{Decoder}(\\text{Encoder} (x + \\epsilon)) - x \\right\\|^2\\right) $$1.2. Score Matching 我们之前在 Score Matching 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 Fisher Divergence。\n$$ J_{\\text{ESM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x\\sim p(x)}\\left(\\left\\| \\nabla_x \\log p(x) - \\nabla_x \\log q(x\\mid \\theta) \\right\\|^2\\right) $$但是问题在于，我们不知道真实分布的解析形式，有的只是对真实分布的采样，没法计算 Fisher Divergence。Divergence。Hyvärinen 为了解决这个问题，证明了在温和的条件下，Fisher Divergence 可以等价转换为：\n$$ J_{\\text{ISM}}(\\theta) = \\mathbb{E}_{x\\sim p(x)}\\left(\\Delta_x \\log q(x\\mid \\theta) + \\frac{1}{2}\\left\\| \\nabla_x \\log q(x\\mid \\theta) \\right\\|^2\\right) $$在对真实分布 $p(x)$ 有限的采样 $\\{x_1, x_2, \\cdots x_n\\}$，上面的的期望可以通过有限采样的均值来近似：\n$$ J_{ISM}(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left(\\Delta_x \\log q(x_i\\mid \\theta) + \\frac{1}{2}\\left\\| \\nabla_x \\log q(x_i\\mid \\theta) \\right\\|^2\\right) $$将 Score Function 记作 $\\psi(x, \\theta) = \\nabla_x \\log q(x\\mid \\theta)$，上面的形式可以写作：\n$$ J_{ISM}(\\theta) \\approx \\frac{1}{n} \\sum_{t=1}^n \\left( \\sum_{i=0}^d \\frac{\\partial \\psi(x^t, \\theta)}{\\partial x_i} + \\frac{1}{2}\\left\\| \\psi(x^t, \\theta) \\right\\|^2\\right) $$在计算上，$\\frac{\\partial \\psi(x^t, \\theta)}{\\partial x_i}$ 即不具备数值稳定性，也不具备计算效率。为了解决稳定问题，LeCun 提出在 ISM 的基础上添加一个关于这一项的正则化项：\n$$ J_{ISMreq}(\\theta) = J_{ISM}(\\theta) + \\lambda \\sum_{i=1}^d \\left(\\frac{\\partial \\psi(x, \\theta)}{\\partial x_i}\\right)^2 $$显然，添加正则化后，可以有效的让 score function 在几何上更加 “平缓”，避免学习到很极端和崎岖的 score function，从而提升数值稳定性。\n2. DSM 和 DAE 之间的联系 2.1. DSM（Denoising Score Matching） 对于通过一系列采样描述的数据 $\\{x_1, x_2, \\cdots, x_n\\}$，我们不妨用 Parzen Windows Density Estimation 来对数据分布进行平滑，给数据一个便于分析的解析形式。关于 Parzen Density Estimator 的内容可以参考 Appendix A。\n不妨选择受带宽参数 $\\sigma$ 控制的高斯函数作为核函数：\n$$ q_\\sigma(\\tilde{x} \\mid x) = \\frac{1}{(\\sigma \\sqrt{2\\pi})^d} \\exp\\left(-\\frac{\\|\\tilde{x} - x\\|^2}{2\\sigma^2}\\right) $$经过 Parzen Density Estimator 重建的数据分布为：\n$$ q_\\sigma(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n q_\\sigma(\\tilde{x} \\mid x_i) $$那么，Parzen Density Estimation 重建后进行的显式分数匹配目标函数为：\n$$ J_{ESM_{\\sigma}}(\\theta) = \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] $$阅读 Appendix A 可以发现，Parzen Density Estimation 事实上是对数据添加了高斯噪声。对于上面的形式，我们不妨展开，看看高斯噪声对数据的污染对 Score Function 学习产生了什么影响？\n2.2. DSM Loss 等价性证明 对于 $J_{ESM_{\\sigma}}(\\theta)$，不妨将二范数的形式展开：\n$$ \\begin{aligned} J_{ESM_{\\sigma}}(\\theta) \u0026= \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] \\\\ \u0026= \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\frac{1}{2} \\left( \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2 - 2\\left \\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle + \\left\\| \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right) \\right] \\\\ \u0026= \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2\\right] - \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle\\right] + \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[\\left\\| \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] \\end{aligned} $$显然，其中 $\\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[\\left\\| \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right]$ 与待优化参数 $\\theta$ 无关，可以忽略。第一项只包含模型，可以直接保留。关键在于第二项，不妨将其标注为 $S(\\theta)$。\n$$ \\begin{aligned} S(\\theta) \u0026= \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle\\right]\\\\ \u0026= \\int q_{\\sigma}(\\tilde{x}) \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle d\\tilde{x}\\\\ \\end{aligned} $$由求导法则 $\\frac{\\partial }{\\partial \\tilde x} \\log q_\\sigma(\\tilde x) = \\frac{1}{q_\\sigma(\\tilde x)} \\frac{\\partial}{\\partial \\tilde x}q_\\sigma(\\tilde x)$ 可知：\n$$ \\begin{aligned} S(\\theta) \u0026= \\int q_{\\sigma}(\\tilde{x}) \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x}q_\\sigma(\\tilde x) \\right \\rangle d\\tilde{x}\\\\ \\end{aligned} $$将 Parzen Density Estimation 的边缘分布形式带入，并根据莱布尼茨定理交换顺序：\n$$ \\begin{aligned} S(\\theta) \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x}q_\\sigma(\\tilde x) \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x}\\int_x p(\\tilde x \\mid x) q(x) dx \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\int_x q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\right \\rangle d\\tilde{x}\\\\ \\end{aligned} $$不妨展开内积符号，有：\n$$ \\begin{aligned} S(\\theta) \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\int_x q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int_{\\tilde x} \\sum_{i=1}^d \\psi^{\\top}_i(\\tilde{x}; \\theta) \\int_x q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\bigg |_i d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\sum_{i=1}^d \\int_x \\psi^{\\top}_i(\\tilde{x}; \\theta) q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\bigg |_i d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\int_x \\sum_{i=1}^d \\psi^{\\top}_i(\\tilde{x}; \\theta) q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\bigg |_i d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\int_x \\left\\langle \\psi(\\tilde{x}; \\theta), q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) \\right \\rangle dx \\ d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\int_x q(x) p(\\tilde x \\mid x) \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x) \\right \\rangle dx \\ d\\tilde x\\\\ \u0026= \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle\\right] \\end{aligned} $$整合形式，有：\n$$ J_{ESM_{\\sigma}}(\\theta) = \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2\\right] - \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle\\right] + C $$只需添加一个与待优化参数无关的量：\n$$ C = \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\| \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\|^2\\right] $$有：\n$$ \\begin{aligned} J_{ESM_{\\sigma}}(\\theta) \u0026= \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2\\right] - \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle\\right] + \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\| \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\|^2\\right]+ C \\\\ \u0026= \\mathbb{E}_{q(x, \\tilde x)} \\left[ \\frac 1 2 \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2 - \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle + \\left\\| \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\|^2\\right] + C\\\\ \u0026= \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}} \\right\\|^2 \\right]\\\\ \u0026= J_{DSMq}(\\theta) \\end{aligned} $$其中，$\\psi(\\tilde{x}; \\theta)$ 是我们需要学习的 Score Function。有趣的事，展开 $\\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}}$ 不难发现：\n$$ \\begin{aligned} \\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}} \u0026= \\frac 1 {\\sigma^2} (x - \\tilde{x})\\\\ \\end{aligned} $$Score Function 实际上跟踪的是我们添加噪声的反方向。换句话说，我们是在 “降噪”。从这里已经可以看到一些 Diffusion 的影子了。\n2.3. DSM 和 DAE 之间的关系 不妨先简单回顾一下，此处对于被噪声污染了的数据对 $\\{\\tilde x, x\\}$，DAE 的 Loss Funciton 可以写作:\n$$ J_{DAE}(\\theta) = \\mathbb{E}_{q(x), q_\\sigma(\\tilde{x}|x)}\\left[ \\left\\| r(\\tilde{x}, \\theta) - x \\right\\|^2 \\right] $$其中 $r(\\tilde{x}, \\theta)$ 是 DAE 的重建函数。根据上一节中的证明 Score Function 可以定义为：\n$$ \\psi(x, \\theta) = \\frac {r(\\tilde x, \\theta) - \\tilde{x}}{\\sigma^2} $$将这个形式带入 $J_{DSM}$ 中，不难发现：\n$$ \\begin{aligned} J_{DSM}(\\theta) \u0026= \\mathbb{E}_{q(x), q_\\sigma(\\tilde{x}|x)}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{x - \\tilde{x}}{\\sigma^2} \\right\\|^2 \\right]\\\\ \u0026= \\mathbb{E}_{q(x), q_\\sigma(\\tilde{x}|x)}\\left[ \\frac{1}{2\\sigma^4} \\left\\| r(\\tilde x, \\theta) - x \\right\\|^2 \\right]\\\\ \u0026= \\frac{1}{2\\sigma^4} J_{DAE}(\\theta) \\end{aligned} $$这与 DAE 的 Loss Function 只差一个常数倍数。\n3. Discussion Score Matching 在 Denoising Score Matching 之前，可以说是一个 “屠龙术”。确实，Score Matching 很好的解决了归一化问题。但是依然有很多缺陷：\nScore Matching 的 Loss Function 需要计算二阶导数，对于高维数据计算量巨大，且数值上不稳定。 Score Matching 只对数据采样点附近的信息梯度建模，导致 Langevin Annealing 的过程中很容易陷入局部最优。 Denoising Score Matching 很好的解决了这些问题。首先，DSM 通过添加噪声的方式，事实上建立了一条从噪声到数据的通路，我们的模型事实上见过从噪声重建数据的每一步操作，让模型的采样更加鲁棒。更大的好处是，在实际操作中，我们使用的 Loss 是：\n$$ \\begin{aligned} J_{DSMq}(\\theta) \u0026= \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}} \\right\\|^2 \\right]\\\\ \u0026= \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{1}{\\sigma^2} (x - \\tilde{x}) \\right\\|^2 \\right] \\end{aligned} $$假设对于数据 $x$，我们添加了噪声 $\\epsilon(\\tilde x)$，那么对于通过神经网络描述的 Score Function $\\psi(\\tilde x; \\theta)$，我们只需要最小化：\n$$ \\boxed{ J_{DSMq}(\\theta) = \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\epsilon(\\tilde x)}{\\sigma^2} \\right\\|^2 \\right] } $$事实上将一个复杂的无监督学习问题，转化为一个有监督学习问题，并直接使用一个简单的 ESM 作为 Loss Function。不论是代码还是训练难度，都简单了非常多。在实践中，使用 Pytorch，实现非常简单：\ndef denoising_score_matching_loss(self, x, sigma=0.01): batch_size = x.shape[0] # 添加噪声 noise = torch.randn_like(x) * sigma x_noisy = x + noise x_noisy.requires_grad_(True) # 计算 score score = self.score_network(x_noisy) # Target: -noise / sigma^2 (真实的去噪方向) target = -noise / (sigma ** 2) # 使用加权的 MSE Loss，避免小噪声时 loss 爆炸 weight = sigma ** 2 # 权重与 sigma^2 成正比，平衡不同噪声水平的损失 loss = weight * F.mse_loss(score, target) return loss 并且，从 DSM，已经可以看到 Diffusion 的雏形了。这篇文章也开启了后世扩散模型大发展的大门。\nAppendix A. Parzen Density Estimator 对于一个理想但是未知的分布 $p(x)$，我们对齐认识只有一系列采样点 $\\{x_1, x_2, \\cdots ,x_n\\}$。一个简单的甚至有些暴力的重建其密度的方法是：给每个数据点上固定一个概率分布（我们称之为重建核，常选高斯分布）,之后将所有的分布加起来统一归一化。就可以得到一个平滑的、连续的概率分布。Parzen Density Estimation 可以看作对直方图发的一种直觉的改进。当我们选择如下的高斯重建核：\n$$ p(\\tilde x \\mid x) = \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) $$那么 Parzen Density Estimation 重建的概率分布为：\n$$ p(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n p(\\tilde x \\mid x_i) = \\frac{1}{n} \\sum_{i=1}^n \\mathcal N(\\tilde x \\mid x_i, \\sigma^2 I) $$其中，高斯分布的协方差 $\\sigma$ 控制了重建分布的平滑程度。$\\sigma$ 越大，重建分布越平滑，$\\sigma$ 越小，重建分布越接近原始数据的离散采样。这里称之为 “带宽”。\n事实上可以通过一个更形式话的方式来定力 Parzen Density Estimation。对于一系列数据 $\\{x_1, x_2, \\cdots ,x_n\\}$，我们可以建立起其经验分布：\n$$ q(x) = \\frac{1}{n}\\sum_{i=1}^n \\delta(x - x_i) $$对于重建核，我们可以将其写成条件的形式：\n$$ p(\\tilde x \\mid x) = \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) $$Parzen Density Estimation 可以用边缘分布重写：\n$$ \\begin{aligned} p(\\tilde x) \u0026= \\int p(\\tilde x \\mid x) q(x) dx\\\\ \u0026= \\int \\sum_{i=1}^n \\delta(x - x_i) \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) dx\\\\ \u0026= \\sum_{i=1}^n \\int \\delta(x - x_i) \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) dx\\\\ \u0026= \\frac{1}{n} \\sum_{i=1}^n \\mathcal N(\\tilde x \\mid x_i, \\sigma^2 I) \\end{aligned} $$这个形式与直接的 Parzen Density Estimation 是等价的。这个理解在 DAE 中至关重要。\nReference [A connection between score matching and denoising autoencoders](@article{vincent2011connection, title={}, author={Vincent, Pascal}, journal={Neural computation}, volume={23}, number={7}, pages={1661–1674}, year={2011}, publisher={MIT Press} })\n","wordCount":"1263","inLanguage":"en","image":"https://wangjv0812.cn/","datePublished":"2025-10-13T20:40:25+08:00","dateModified":"2025-10-13T20:40:25+08:00","author":{"@type":"Person","name":"WangJV"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wangjv0812.cn/2025/10/denoising-score-matching/"},"publisher":{"@type":"Organization","name":"WangJV Blog","logo":{"@type":"ImageObject","url":"https://wangjv0812.cn/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wangjv0812.cn/ accesskey=h title="WangJV Blog (Alt + H)">WangJV Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://wangjv0812.cn/ title=Home><span>Home</span></a></li><li><a href=https://wangjv0812.cn/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://wangjv0812.cn/resources/ title=Resources><span>Resources</span></a></li><li><a href=https://wangjv0812.cn/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://wangjv0812.cn/search/ title="🔍 Search (Alt + /)" accesskey=/><span>🔍 Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://wangjv0812.cn/>Home</a>&nbsp;»&nbsp;<a href=https://wangjv0812.cn/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Denoising Score Matching</h1><div class=post-meta><span title='2025-10-13 20:40:25 +0800 +0800'>October 13, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1263 words&nbsp;·&nbsp;WangJV&nbsp;|&nbsp;<a href=https://github.com/WangJV0812/WangJV-Blog-Source/tree/master/content/posts/DenoisingScoreMatching/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-动机>1. 动机</a><ul><li><a href=#11-denoising-autoencoder>1.1. Denoising AutoEncoder</a></li><li><a href=#12-score-matching>1.2. Score Matching</a></li></ul></li><li><a href=#2-dsm-和-dae-之间的联系>2. DSM 和 DAE 之间的联系</a><ul><li><a href=#21-dsmdenoising-score-matching>2.1. DSM（Denoising Score Matching）</a></li><li><a href=#22-dsm-loss-等价性证明>2.2. DSM Loss 等价性证明</a></li><li><a href=#23-dsm-和-dae-之间的关系>2.3. DSM 和 DAE 之间的关系</a></li></ul></li><li><a href=#3-discussion>3. Discussion</a></li><li><a href=#appendix>Appendix</a><ul><li><a href=#a-parzen-density-estimator>A. Parzen Density Estimator</a></li></ul></li><li><a href=#reference>Reference</a></li></ul></nav></div></details></div><div class=post-content><h2 id=1-动机>1. 动机<a hidden class=anchor aria-hidden=true href=#1-动机>#</a></h2><p>以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 <a href=https://wangjv0812.cn/2025/08/noise-contrastive-estimation/>NCE 中对于数据流形</a> 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。</p><p>我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\theta$ 控制的概率分布 $q(x\mid \theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：</p><ol><li>我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\{x_1, x_2, \cdots x_n\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\theta$，使得 $q(x\mid \theta)$ 尽可能接近 $p(x)$。</li><li>对于一个完成学习的分布 $p(x\mid \theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。</li></ol><p>这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 <a href=https://wangjv0812.cn/2025/08/scorematching/>Score Matching</a> 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。</p><h3 id=11-denoising-autoencoder>1.1. Denoising AutoEncoder<a hidden class=anchor aria-hidden=true href=#11-denoising-autoencoder>#</a></h3><p>AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。</p><p>为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。</p><p>形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\epsilon \sim \mathcal N(x\mid 0, \sigma^2I)$，有被污染的数据：</p>$$
\tilde{x} = x + \epsilon
$$<p>那么，损失可以写作：</p>$$
J_{DATA}(\theta) = \mathbb{E}\left(\left\|
\text{Decoder}(\text{Encoder} (x + \epsilon)) - x
\right\|^2\right)
$$<h3 id=12-score-matching>1.2. Score Matching<a hidden class=anchor aria-hidden=true href=#12-score-matching>#</a></h3><p>我们之前在 <a href=https://wangjv0812.cn/2025/08/scorematching/>Score Matching</a> 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 <a href=https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/>Fisher Divergence</a>。</p>$$
J_{\text{ESM}}(\theta) = \frac{1}{2} \mathbb{E}_{x\sim p(x)}\left(\left\|
\nabla_x \log p(x) - \nabla_x \log q(x\mid \theta)
\right\|^2\right)
$$<p>但是问题在于，我们不知道真实分布的解析形式，有的只是对真实分布的采样，没法计算 Fisher Divergence。Divergence。Hyvärinen 为了解决这个问题，证明了在温和的条件下，Fisher Divergence 可以等价转换为：</p>$$
J_{\text{ISM}}(\theta) = \mathbb{E}_{x\sim p(x)}\left(\Delta_x \log q(x\mid \theta) + \frac{1}{2}\left\|
\nabla_x \log q(x\mid \theta)
\right\|^2\right)
$$<p>在对真实分布 $p(x)$ 有限的采样 $\{x_1, x_2, \cdots x_n\}$，上面的的期望可以通过有限采样的均值来近似：</p>$$
J_{ISM}(\theta) \approx \frac{1}{n} \sum_{i=1}^n \left(\Delta_x \log q(x_i\mid \theta) + \frac{1}{2}\left\|
\nabla_x \log q(x_i\mid \theta)
\right\|^2\right)
$$<p>将 Score Function 记作 $\psi(x, \theta) = \nabla_x \log q(x\mid \theta)$，上面的形式可以写作：</p>$$
J_{ISM}(\theta) \approx \frac{1}{n} \sum_{t=1}^n \left(
\sum_{i=0}^d \frac{\partial \psi(x^t, \theta)}{\partial x_i} + \frac{1}{2}\left\|
\psi(x^t, \theta)
\right\|^2\right)
$$<p>在计算上，$\frac{\partial \psi(x^t, \theta)}{\partial x_i}$ 即不具备数值稳定性，也不具备计算效率。为了解决稳定问题，LeCun 提出在 ISM 的基础上添加一个关于这一项的正则化项：</p>$$
J_{ISMreq}(\theta) = J_{ISM}(\theta) + \lambda \sum_{i=1}^d \left(\frac{\partial \psi(x, \theta)}{\partial x_i}\right)^2
$$<p>显然，添加正则化后，可以有效的让 score function 在几何上更加 “平缓”，避免学习到很极端和崎岖的 score function，从而提升数值稳定性。</p><h2 id=2-dsm-和-dae-之间的联系>2. DSM 和 DAE 之间的联系<a hidden class=anchor aria-hidden=true href=#2-dsm-和-dae-之间的联系>#</a></h2><h3 id=21-dsmdenoising-score-matching>2.1. DSM（Denoising Score Matching）<a hidden class=anchor aria-hidden=true href=#21-dsmdenoising-score-matching>#</a></h3><p>对于通过一系列采样描述的数据 $\{x_1, x_2, \cdots, x_n\}$，我们不妨用 Parzen Windows Density Estimation 来对数据分布进行平滑，给数据一个便于分析的解析形式。关于 Parzen Density Estimator 的内容可以参考 <a href=#a-parzen-density-estimator>Appendix A</a>。</p><p>不妨选择受带宽参数 $\sigma$ 控制的高斯函数作为核函数：</p>$$
q_\sigma(\tilde{x} \mid x) = \frac{1}{(\sigma \sqrt{2\pi})^d} \exp\left(-\frac{\|\tilde{x} - x\|^2}{2\sigma^2}\right)
$$<p>经过 Parzen Density Estimator 重建的数据分布为：</p>$$
q_\sigma(\tilde{x}) = \frac{1}{n} \sum_{i=1}^n q_\sigma(\tilde{x} \mid x_i)
$$<p>那么，Parzen Density Estimation 重建后进行的显式分数匹配目标函数为：</p>$$
J_{ESM_{\sigma}}(\theta) = \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right\|^2 \right]
$$<p>阅读 <a href=#a-parzen-density-estimator>Appendix A</a> 可以发现，Parzen Density Estimation 事实上是对数据添加了高斯噪声。对于上面的形式，我们不妨展开，看看高斯噪声对数据的污染对 Score Function 学习产生了什么影响？</p><h3 id=22-dsm-loss-等价性证明>2.2. DSM Loss 等价性证明<a hidden class=anchor aria-hidden=true href=#22-dsm-loss-等价性证明>#</a></h3><p>对于 $J_{ESM_{\sigma}}(\theta)$，不妨将二范数的形式展开：</p>$$
\begin{aligned}
J_{ESM_{\sigma}}(\theta)
&= \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right\|^2 \right] \\
&= \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \frac{1}{2} \left( \left\| \psi(\tilde{x}; \theta) \right\|^2 - 2\left \langle \psi(\tilde{x}; \theta), \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right \rangle + \left\| \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right\|^2 \right) \right] \\
&= \frac 1 2 \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \left\| \psi(\tilde{x}; \theta) \right\|^2\right] - \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \left\langle \psi(\tilde{x}; \theta), \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right \rangle\right] + \frac 1 2 \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[\left\| \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right\|^2 \right]
\end{aligned}
$$<p>显然，其中 $\frac 1 2 \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[\left\| \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right\|^2 \right]$ 与待优化参数 $\theta$ 无关，可以忽略。第一项只包含模型，可以直接保留。关键在于第二项，不妨将其标注为 $S(\theta)$。</p>$$
\begin{aligned}
S(\theta)
&= \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \left\langle \psi(\tilde{x}; \theta), \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right \rangle\right]\\
&= \int q_{\sigma}(\tilde{x}) \left\langle \psi(\tilde{x}; \theta), \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right \rangle d\tilde{x}\\
\end{aligned}
$$<p>由求导法则 $\frac{\partial }{\partial \tilde x} \log q_\sigma(\tilde x) = \frac{1}{q_\sigma(\tilde x)} \frac{\partial}{\partial \tilde x}q_\sigma(\tilde x)$ 可知：</p>$$
\begin{aligned}
S(\theta)
&= \int q_{\sigma}(\tilde{x}) \left\langle \psi(\tilde{x}; \theta), \frac{\partial \log q_{\sigma}(\tilde{x})}{\partial \tilde{x}} \right \rangle d\tilde{x}\\
&= \int \left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x}q_\sigma(\tilde x) \right \rangle d\tilde{x}\\
\end{aligned}
$$<p>将 Parzen Density Estimation 的边缘分布形式带入，并根据莱布尼茨定理交换顺序：</p>$$
\begin{aligned}
S(\theta)
&= \int_{\tilde x} \left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x}q_\sigma(\tilde x) \right \rangle d\tilde{x}\\
&= \int_{\tilde x} \left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x}\int_x p(\tilde x \mid x) q(x) dx \right \rangle d\tilde{x}\\
&= \int_{\tilde x} \left\langle \psi(\tilde{x}; \theta), \int_x q(x) \frac{\partial}{\partial \tilde x} p(\tilde x \mid x) dx \right \rangle d\tilde{x}\\
\end{aligned}
$$<p>不妨展开内积符号，有：</p>$$
\begin{aligned}
S(\theta)
&= \int_{\tilde x} \left\langle \psi(\tilde{x}; \theta), \int_x q(x) \frac{\partial}{\partial \tilde x} p(\tilde x \mid x) dx \right \rangle d\tilde{x}\\
&= \int_{\tilde x} \sum_{i=1}^d \psi^{\top}_i(\tilde{x}; \theta) \int_x q(x) \frac{\partial}{\partial \tilde x} p(\tilde x \mid x) dx \bigg |_i d\tilde x\\
&= \int_{\tilde x} \sum_{i=1}^d \int_x \psi^{\top}_i(\tilde{x}; \theta) q(x) \frac{\partial}{\partial \tilde x} p(\tilde x \mid x) dx \bigg |_i d\tilde x\\
&= \int_{\tilde x} \int_x \sum_{i=1}^d \psi^{\top}_i(\tilde{x}; \theta) q(x) \frac{\partial}{\partial \tilde x} p(\tilde x \mid x) dx \bigg |_i d\tilde x\\
&= \int_{\tilde x} \int_x \left\langle \psi(\tilde{x}; \theta), q(x) \frac{\partial}{\partial \tilde x} p(\tilde x \mid x) \right \rangle dx \ d\tilde x\\
&= \int_{\tilde x} \int_x q(x) p(\tilde x \mid x) \left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x) \right \rangle dx \ d\tilde x\\
&= \mathbb{E}_{q(x, \tilde x)} \left[\left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \rangle\right]
\end{aligned}
$$<p>整合形式，有：</p>$$
J_{ESM_{\sigma}}(\theta) = \frac 1 2 \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \left\| \psi(\tilde{x}; \theta) \right\|^2\right] - \mathbb{E}_{q(x, \tilde x)} \left[\left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \rangle\right] + C
$$<p>只需添加一个与待优化参数无关的量：</p>$$
C = \mathbb{E}_{q(x, \tilde x)} \left[\left\| \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \|^2\right]
$$<p>有：</p>$$
\begin{aligned}
J_{ESM_{\sigma}}(\theta)
&= \frac 1 2 \mathbb{E}_{q_{\sigma}(\tilde{x})}\left[ \left\| \psi(\tilde{x}; \theta) \right\|^2\right] - \mathbb{E}_{q(x, \tilde x)} \left[\left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \rangle\right] + \mathbb{E}_{q(x, \tilde x)} \left[\left\| \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \|^2\right]+ C \\
&= \mathbb{E}_{q(x, \tilde x)} \left[ \frac 1 2 \left\| \psi(\tilde{x}; \theta) \right\|^2 - \left\langle \psi(\tilde{x}; \theta), \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \rangle + \left\| \frac{\partial}{\partial \tilde x} \log p(\tilde x \mid x)\right \|^2\right] + C\\
&= \mathbb{E}_{q_{\sigma}(x, \tilde{x})}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{\partial \log q_{\sigma}(\tilde{x}|x)}{\partial \tilde{x}} \right\|^2 \right]\\
&= J_{DSMq}(\theta)
\end{aligned}
$$<p>其中，$\psi(\tilde{x}; \theta)$ 是我们需要学习的 Score Function。有趣的事，展开 $\frac{\partial \log q_{\sigma}(\tilde{x}|x)}{\partial \tilde{x}}$ 不难发现：</p>$$
\begin{aligned}
\frac{\partial \log q_{\sigma}(\tilde{x}|x)}{\partial \tilde{x}}
&= \frac 1 {\sigma^2} (x - \tilde{x})\\
\end{aligned}
$$<p>Score Function 实际上跟踪的是我们添加噪声的反方向。换句话说，我们是在 “降噪”。从这里已经可以看到一些 Diffusion 的影子了。</p><h3 id=23-dsm-和-dae-之间的关系>2.3. DSM 和 DAE 之间的关系<a hidden class=anchor aria-hidden=true href=#23-dsm-和-dae-之间的关系>#</a></h3><p>不妨先简单回顾一下，此处对于被噪声污染了的数据对 $\{\tilde x, x\}$，DAE 的 Loss Funciton 可以写作:</p>$$
J_{DAE}(\theta) = \mathbb{E}_{q(x), q_\sigma(\tilde{x}|x)}\left[ \left\| r(\tilde{x}, \theta) - x \right\|^2 \right]
$$<p>其中 $r(\tilde{x}, \theta)$ 是 DAE 的重建函数。根据<a href=#22-dsm-loss-%E7%AD%89%E4%BB%B7%E6%80%A7%E8%AF%81%E6%98%8E>上一节中的证明</a> Score Function 可以定义为：</p>$$
\psi(x, \theta) = \frac {r(\tilde x, \theta) - \tilde{x}}{\sigma^2}
$$<p>将这个形式带入 $J_{DSM}$ 中，不难发现：</p>$$
\begin{aligned}
J_{DSM}(\theta)
&= \mathbb{E}_{q(x), q_\sigma(\tilde{x}|x)}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{x - \tilde{x}}{\sigma^2} \right\|^2 \right]\\
&= \mathbb{E}_{q(x), q_\sigma(\tilde{x}|x)}\left[ \frac{1}{2\sigma^4} \left\| r(\tilde x, \theta) - x \right\|^2 \right]\\
&= \frac{1}{2\sigma^4} J_{DAE}(\theta)
\end{aligned}
$$<p>这与 DAE 的 Loss Function 只差一个常数倍数。</p><h2 id=3-discussion>3. Discussion<a hidden class=anchor aria-hidden=true href=#3-discussion>#</a></h2><p>Score Matching 在 Denoising Score Matching 之前，可以说是一个 “屠龙术”。确实，Score Matching 很好的解决了归一化问题。但是依然有很多缺陷：</p><ol><li>Score Matching 的 Loss Function 需要计算二阶导数，对于高维数据计算量巨大，且数值上不稳定。</li><li>Score Matching 只对数据采样点附近的信息梯度建模，导致 Langevin Annealing 的过程中很容易陷入局部最优。</li></ol><p>Denoising Score Matching 很好的解决了这些问题。首先，DSM 通过添加噪声的方式，事实上建立了一条从噪声到数据的通路，我们的模型事实上见过从噪声重建数据的每一步操作，让模型的采样更加鲁棒。更大的好处是，在实际操作中，我们使用的 Loss 是：</p>$$
\begin{aligned}
J_{DSMq}(\theta)
&= \mathbb{E}_{q_{\sigma}(x, \tilde{x})}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{\partial \log q_{\sigma}(\tilde{x}|x)}{\partial \tilde{x}} \right\|^2 \right]\\
&= \mathbb{E}_{q_{\sigma}(x, \tilde{x})}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{1}{\sigma^2} (x - \tilde{x}) \right\|^2 \right]
\end{aligned}
$$<p>假设对于数据 $x$，我们添加了噪声 $\epsilon(\tilde x)$，那么对于通过神经网络描述的 Score Function $\psi(\tilde x; \theta)$，我们只需要最小化：</p>$$
\boxed{
J_{DSMq}(\theta) = \mathbb{E}_{q_{\sigma}(x, \tilde{x})}\left[ \frac{1}{2} \left\| \psi(\tilde{x}; \theta) - \frac{\epsilon(\tilde x)}{\sigma^2} \right\|^2 \right]
}
$$<p>事实上将一个复杂的无监督学习问题，转化为一个有监督学习问题，并直接使用一个简单的 ESM 作为 Loss Function。不论是代码还是训练难度，都简单了非常多。在实践中，使用 Pytorch，实现非常简单：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>denoising_score_matching_loss</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>sigma</span><span class=o>=</span><span class=mf>0.01</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 添加噪声</span>
</span></span><span class=line><span class=cl>    <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=n>sigma</span>
</span></span><span class=line><span class=cl>    <span class=n>x_noisy</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>noise</span>
</span></span><span class=line><span class=cl>    <span class=n>x_noisy</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 计算 score</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>score_network</span><span class=p>(</span><span class=n>x_noisy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Target: -noise / sigma^2 (真实的去噪方向)</span>
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=o>-</span><span class=n>noise</span> <span class=o>/</span> <span class=p>(</span><span class=n>sigma</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 使用加权的 MSE Loss，避免小噪声时 loss 爆炸</span>
</span></span><span class=line><span class=cl>    <span class=n>weight</span> <span class=o>=</span> <span class=n>sigma</span> <span class=o>**</span> <span class=mi>2</span>  <span class=c1># 权重与 sigma^2 成正比，平衡不同噪声水平的损失</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>weight</span> <span class=o>*</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></div><p>并且，从 DSM，已经可以看到 Diffusion 的雏形了。这篇文章也开启了后世扩散模型大发展的大门。</p><h2 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h2><h3 id=a-parzen-density-estimator>A. Parzen Density Estimator<a hidden class=anchor aria-hidden=true href=#a-parzen-density-estimator>#</a></h3><p>对于一个理想但是未知的分布 $p(x)$，我们对齐认识只有一系列采样点 $\{x_1, x_2, \cdots ,x_n\}$。一个简单的甚至有些暴力的重建其密度的方法是：给每个数据点上固定一个概率分布（我们称之为重建核，常选高斯分布）,之后将所有的分布加起来统一归一化。就可以得到一个平滑的、连续的概率分布。Parzen Density Estimation 可以看作对直方图发的一种直觉的改进。当我们选择如下的高斯重建核：</p>$$
p(\tilde x \mid x) = \mathcal N(\tilde x \mid x, \sigma^2 I)
$$<p>那么 Parzen Density Estimation 重建的概率分布为：</p>$$
p(\tilde{x}) = \frac{1}{n} \sum_{i=1}^n p(\tilde x \mid x_i) = \frac{1}{n} \sum_{i=1}^n \mathcal N(\tilde x \mid x_i, \sigma^2 I)
$$<p>其中，高斯分布的协方差 $\sigma$ 控制了重建分布的平滑程度。$\sigma$ 越大，重建分布越平滑，$\sigma$ 越小，重建分布越接近原始数据的离散采样。这里称之为 “带宽”。</p><p>事实上可以通过一个更形式话的方式来定力 Parzen Density Estimation。对于一系列数据 $\{x_1, x_2, \cdots ,x_n\}$，我们可以建立起其经验分布：</p>$$
q(x) = \frac{1}{n}\sum_{i=1}^n \delta(x - x_i)
$$<p>对于重建核，我们可以将其写成条件的形式：</p>$$
p(\tilde x \mid x) = \mathcal N(\tilde x \mid x, \sigma^2 I)
$$<p>Parzen Density Estimation 可以用边缘分布重写：</p>$$
\begin{aligned}
p(\tilde x)
&= \int p(\tilde x \mid x) q(x) dx\\
&= \int \sum_{i=1}^n \delta(x - x_i) \mathcal N(\tilde x \mid x, \sigma^2 I) dx\\
&= \sum_{i=1}^n \int \delta(x - x_i) \mathcal N(\tilde x \mid x, \sigma^2 I) dx\\
&= \frac{1}{n} \sum_{i=1}^n \mathcal N(\tilde x \mid x_i, \sigma^2 I)
\end{aligned}
$$<p>这个形式与直接的 Parzen Density Estimation 是等价的。这个理解在 DAE 中至关重要。</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p>[A connection between score matching and denoising autoencoders](@article{vincent2011connection,
title={},
author={Vincent, Pascal},
journal={Neural computation},
volume={23},
number={7},
pages={1661&ndash;1674},
year={2011},
publisher={MIT Press}
})</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://wangjv0812.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li><li><a href=https://wangjv0812.cn/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>生成模型</a></li><li><a href=https://wangjv0812.cn/tags/denoising-score-matching/>Denoising Score Matching</a></li><li><a href=https://wangjv0812.cn/tags/diffusion/>Diffusion</a></li><li><a href=https://wangjv0812.cn/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/>无监督学习</a></li></ul><nav class=paginav><a class=next href=https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/><span class=title>Next »</span><br><span>Fisher Information and Fisher Divergence</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://wangjv0812.cn/>WangJV Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>