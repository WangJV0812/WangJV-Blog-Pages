<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>神经网络 on WangJV Blog</title><link>https://wangjv0812.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><description>Recent content in 神经网络 on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.153.2</generator><language>en-us</language><lastBuildDate>Tue, 04 Nov 2025 18:35:25 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml"/><item><title>From Diffusion to Diffusion Language Model</title><link>https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/</link><pubDate>Tue, 04 Nov 2025 18:35:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/</guid><description>&lt;p&gt;&lt;img alt="alt text" loading="lazy" src="https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/posts/From%20Diffusion%20to%20Diffusion%20Language%20Model/Images/DLMinrecentyear.png"&gt;&lt;/p&gt;
&lt;p&gt;对于现在的大模型，普遍使用自回归模型。对于一个长度为 $n$ 的文本，自回归模型将其分解为：&lt;/p&gt;
$$
P(x_{1:n}) = \prod_{i=1}^n P(x_i \mid x_{&lt;i})
$$&lt;p&gt;这也是在做自回归时，使用 Decoder Only 的原因。但是一个清晰的问题是，自回归模型是自然语言模型的唯一选择吗？答案可能不是100%的不。diffusion 似乎是一个很符合直接的推理过程，它会先写出一个文本的草稿，之后再对草稿不断修改（降噪），最终得到完整的文本。&lt;/p&gt;
&lt;h2 id="1-无监督学习和流形假说"&gt;1. 无监督学习和流形假说&lt;/h2&gt;
&lt;p&gt;对于经典的无监督学习任务，我们面对的是巨量的、常常高维的、没有人工提取特征的原始数据。无监督学习最核心的任务是在找到这些数据之间的相对关系提取出数据的内在结构。我们不妨以图像生成任务举例子，对于一个尺寸为 $1920\times 1080$ 的图片，其像素总量为 $2073600$。但是这些维度之不可能是独立的（因为如果完全独立，只能得到没有任何信息的纯噪声），数据的维度之间存在着复杂的相关性质。换句话说，对于图像数据，数据点并不是均匀的分布在 $2073600$ 维的空间中，它只分布在空间中的一小部分区域，或者用微分几何的语言描述，数据分布在一个高维空间中的一条流形（manifold）上。这就是流形假说（manifold hypothesis），它是无监督学习的理论基础。那么无监督学习的任务就变成了如何用一个神经网络来建模这条流形。一个很显然的流形建模工具是使用概率分布，对于不数据流形上的点，概率赋 $0$ 就好了。&lt;/p&gt;
&lt;p&gt;关于流形假说的更多讨论，可以参考 &lt;a href="https://wangjv0812.cn/2025/08/noise-contrastive-estimation/#1-%E5%8A%A8%E6%9C%BA"&gt;Noise Contrastive Estimation/1. 动机&lt;/a&gt; 中对于使用概率分布建模数据分布流形的讨论。&lt;/p&gt;
&lt;h2 id="2-continuous-diffusion"&gt;2. Continuous Diffusion&lt;/h2&gt;
&lt;p&gt;Diffusion 模型可能是目前最成功的高维连续数据的建模工具。相比于 &lt;a href="https://wangjv0812.cn/2025/10/variational-autoencoder/"&gt;VAE&lt;/a&gt;、GAN、&lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 等，Diffusion 很核心的优势在于&lt;strong&gt;它显示式的给出了一条图像生成的路径（从噪声到数据），大大减少了生成器所需要搜索的空间范围&lt;/strong&gt;。如果你现在对这句话一头雾水，不妨先对它有一个大概的印象，看完本节后很大概率会有一个跟深刻的认识。&lt;/p&gt;
&lt;p&gt;Diffusion 模型可以分为三个过程，分别是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Forward Process，向原始数据添加噪声，事实上可以理解为一个数据增强的过程。&lt;/li&gt;
&lt;li&gt;Backward Process，从噪声中恢复数据的过程，可以理解为 Diffusion 的推理。&lt;/li&gt;
&lt;li&gt;训练过程，训练一个神经网络来学习 Backward Process。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="21-forward-process"&gt;2.1. Forward Process&lt;/h3&gt;
&lt;p&gt;对于一个数据样本 $x$，不妨将其标记为 $x_0$，在后面的讨论中，可以将 $x_0$ 视作一个确定性变量。Diffusion 过程每一步都向原始数据添加一个很小的噪声，添加噪声的过程不妨用一个条件概率描述：&lt;/p&gt;
$$
p(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
$$&lt;p&gt;其中 $\beta_t \in (0, 1)$，是一个用于控制噪声大小的超参数。我们希望求得 $x_t$ 在 $x_0$ 下的条件分布：&lt;/p&gt;</description></item><item><title>Denoising Score Matching</title><link>https://wangjv0812.cn/2025/10/denoising-score-matching/</link><pubDate>Mon, 13 Oct 2025 20:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/10/denoising-score-matching/</guid><description>&lt;h2 id="1-动机"&gt;1. 动机&lt;/h2&gt;
&lt;p&gt;以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 &lt;a href="https://wangjv0812.cn/2025/08/noise-contrastive-estimation/"&gt;NCE 中对于数据流形&lt;/a&gt; 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。&lt;/p&gt;
&lt;p&gt;我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\theta$ 控制的概率分布 $q(x\mid \theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\{x_1, x_2, \cdots x_n\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\theta$，使得 $q(x\mid \theta)$ 尽可能接近 $p(x)$。&lt;/li&gt;
&lt;li&gt;对于一个完成学习的分布 $p(x\mid \theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 &lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。&lt;/p&gt;
&lt;h3 id="11-denoising-autoencoder"&gt;1.1. Denoising AutoEncoder&lt;/h3&gt;
&lt;p&gt;AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。&lt;/p&gt;
&lt;p&gt;为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。&lt;/p&gt;
&lt;p&gt;形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\epsilon \sim \mathcal N(x\mid 0, \sigma^2I)$，有被污染的数据：&lt;/p&gt;
$$
\tilde{x} = x + \epsilon
$$&lt;p&gt;那么，损失可以写作：&lt;/p&gt;
$$
J_{DATA}(\theta) = \mathbb{E}\left(\left\|
\text{Decoder}(\text{Encoder} (x + \epsilon)) - x
\right\|^2\right)
$$&lt;h3 id="12-score-matching"&gt;1.2. Score Matching&lt;/h3&gt;
&lt;p&gt;我们之前在 &lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 &lt;a href="https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/"&gt;Fisher Divergence&lt;/a&gt;。&lt;/p&gt;</description></item><item><title>DreamFusion</title><link>https://wangjv0812.cn/2024/12/dreamfusion/</link><pubDate>Wed, 18 Dec 2024 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2024/12/dreamfusion/</guid><description>&lt;h2 id="1-使用神经网路进行数据生成"&gt;1. 使用神经网路进行数据生成&lt;/h2&gt;
&lt;p&gt;使用神经网络生成一个高维度数据是机器学习中非常重要的一个工作。我们假设数据集 $\left\{\boldsymbol{x_1}, \boldsymbol{x_2}, \dots, \boldsymbol{x_n}\right\}$ 为一个大小为$n$的数据集，该数据集统一的服从一个概率分布 $p_{data}(\boldsymbol{x})$ 。我们假设对数据集的抽样都是独立同分布的，即：&lt;/p&gt;
$$
\left\{\boldsymbol{x_1}, \boldsymbol{x_2}, \dots, \boldsymbol{x_n}\right\} \sim p_{data}(\boldsymbol{x})
$$&lt;p&gt;那么丛现有数据生成新的数据的核心就是使用神经网络学习这个概率分布。不妨假设学习的概率分布为 $\hat p_\theta(\boldsymbol x)$。我们会希望 $\hat p_\theta(\boldsymbol x)$ 尽可能的接近 $p_{data}(\boldsymbol(x))$ 。为了衡量真是分布和我们学习的分布之间的差距，我们需要定义一个距离函数 $D(\cdot \mid \cdot)$ 我们可以定义优化目标：&lt;/p&gt;
$$
\hat \theta = \arg \min_{\theta} D\left(p_{data}(\boldsymbol{x}) \mid \hat p_\theta(\boldsymbol x) \right)
$$&lt;p&gt;关于距离函数，我们可以定义 $D(\cdot \mid \cdot)$ 为 f-divergence 定义为：&lt;/p&gt;
$$
D_f(p_{data}(\boldsymbol(x)) \mid \hat p_\theta(\boldsymbol x)) = \int p_\theta(\boldsymbol x) f \left(\frac{p_{data}(\boldsymbol x)}{p_\theta(\boldsymbol x)}\right) d\boldsymbol x
$$&lt;p&gt;不妨取 $f(x) = x\log x$ ，我们可以得到 KL 散度：&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(p_{data}(\boldsymbol(x)) \mid \hat p_\theta(\boldsymbol x))
&amp;= \int p_{data}(\boldsymbol x) \log \frac{p_{data}(\boldsymbol x)}{p_\theta(\boldsymbol x)} d\boldsymbol x\\
&amp;= \mathbb E_{p_{data}(\boldsymbol x)}\left[ \log \frac{p_{data}(\boldsymbol x)}{p_\theta(\boldsymbol x)} \right]
\end{aligned}
$$&lt;p&gt;我们可以用抽样的均值来代替期望，有：&lt;/p&gt;</description></item></channel></rss>