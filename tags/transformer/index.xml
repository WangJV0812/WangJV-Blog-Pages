<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Transformer on WangJV Blog</title><link>https://wangjv0812.cn/tags/transformer/</link><description>Recent content in Transformer on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.153.2</generator><language>en-us</language><lastBuildDate>Tue, 16 Dec 2025 10:11:25 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/tags/transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/</link><pubDate>Tue, 16 Dec 2025 10:11:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/</guid><description>&lt;h2 id="1-transformer-复杂度分析"&gt;1. Transformer 复杂度分析&lt;/h2&gt;
&lt;h3 id="11-矩阵运算复杂度分析"&gt;1.1. 矩阵运算复杂度分析&lt;/h3&gt;
&lt;p&gt;Transformer 模型事实上是矩阵乘法的堆叠。让我们先从基础的向量乘法的复杂度分析开始，一步步扩展到对张量运算的复杂度有清晰的认识。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于向量 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$，那么向量之间的点积需要进行 $n$ 次乘法和 $n$ 次加法，总的时间复杂度为 $O(2n)$。&lt;/li&gt;
&lt;li&gt;对于矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和 $\mathbf{x} \in \mathbb{R}^{n}$，矩阵向量乘法 $\mathbf{Ax}$ 需要进行 $m$ 次向量点积，每次点积的复杂度为 $O(2n)$，因此总的时间复杂度为 $O(2mn)$。&lt;/li&gt;
&lt;li&gt;对于矩阵 $\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$，矩阵乘法 $\mathbf{AB}$ 需要进行 $m \times p$ 次向量点积，每次点积的复杂度为 $O(2n)$，因此总的时间复杂度为 $O(2mnp)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面讨论了基础矩阵运算的时间复杂度，下面不妨上上强度，看看张量运算的复杂度。对于张量 $\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$，其中维度 $\color{red}GH$ 是 batch 维度，$\color{blue} KL$ 是被吸收 (Contracting) 的维度。那么可以定义 &lt;a href="https://docs.pytorch.org/docs/stable/generated/torch.einsum.html"&gt;einsum&lt;/a&gt; 操作如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;einsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ghijkl,ghmnkl-&amp;gt;ghijmn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的 einsum 操作中，张量 $\mathbf{A}$ 和 $\mathbf{B}$ 在维度 $\color{red}GH$ 上是对齐的 (Aligned)，在维度 $\color{blue}KL$ 上是被吸收的 (Contracting)。那么这个 einsum 操作的时间复杂度为：&lt;/p&gt;</description></item><item><title>From Transformer to VGGT</title><link>https://wangjv0812.cn/2025/04/from-transformer-to-vggt/</link><pubDate>Fri, 18 Apr 2025 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/04/from-transformer-to-vggt/</guid><description>&lt;h2 id="1-preliminary-attention-and-vit"&gt;1. Preliminary: Attention and ViT&lt;/h2&gt;
&lt;p&gt;我们先来回顾一下经典的 Transformer 结构，之后从 Transformer 的角度来理解 ViT，这样大家能更好的理解 VGGT 和 MASt3R、DUSt3R 之类工作的苦恼之处。&lt;/p&gt;
&lt;p&gt;&lt;img alt="VGGT Attention pipline" loading="lazy" src="https://wangjv0812.cn/2025/04/from-transformer-to-vggt/Images/VGGT%20Attention%20pipline.png"&gt;&lt;/p&gt;
&lt;h3 id="11-encoder-and-decoder"&gt;1.1. Encoder and Decoder&lt;/h3&gt;
&lt;p&gt;深度学习名著 《&lt;a href="https://arxiv.org/pdf/1706.03762"&gt;Attention is all you need&lt;/a&gt;》 提出的古典派 Attention（这么说是因为由于 Transformer 的大火，Attention 机制的变种已经太多了，我们只关注最经典的架构就好，其他都大同小异）。最经典的 Transformer 致力于解决翻译问题，是一个十分经典的 nlp 问题，采用了最经典的 Encoder-Decoder 结构。&lt;/p&gt;
&lt;p&gt;Encoder 由 6 个完全相同的层堆叠而成，每个层由两个子层组成。第一个层负责实现 multi-head self-attention 机制；第二个层是一个简单的全连接前馈网络。为了避免在训练中出现梯度消失的问题，Transformer 在子层间采用了残差链接，之后对子层的输出做归一化（即 Add&amp;amp;Norm）那个块。因此，每个子层的输出可以表示为：&lt;/p&gt;
$$
\text{LayerNorm}(x+\text{Sublayer}(x))
$$&lt;p&gt;其中 $\text{Sublayer}(x)$ 是每个子层具体的实现。为了方便残差链接，每层的输出和输入（包括 embedding layers）都被约定为 $d_{module} = 512$。（至少 Attention is all you need 是这样的）。&lt;/p&gt;
&lt;p&gt;Decoder 也是由完全相同的层堆叠而成，和 Encoder 不同的是 Decoder 的每个层也有三个子层。Decoder 的第一个子层是 masked multi-head self-attention，负责对输入的 embedding 做自注意力机制，之后将输入的 embedding 和 encoder 编码的结合结合起来。后面的层和 Encoder 一样，都是 multi-head self-attention 和前馈网络的组合。&lt;/p&gt;</description></item></channel></rss>