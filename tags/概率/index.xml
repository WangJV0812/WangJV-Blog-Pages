<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>概率 on WangJV Blog</title><link>https://wangjv0812.cn/tags/%E6%A6%82%E7%8E%87/</link><description>Recent content in 概率 on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.151.0</generator><language>en-us</language><lastBuildDate>Sat, 13 Sep 2025 17:13:56 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/tags/%E6%A6%82%E7%8E%87/index.xml" rel="self" type="application/rss+xml"/><item><title>Fisher Information and Fisher Divergence</title><link>https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/</link><pubDate>Sat, 13 Sep 2025 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/</guid><description>&lt;p&gt;在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。&lt;/li&gt;
&lt;li&gt;Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面则给出一些不那么直观的，数学形式上的解释。&lt;/p&gt;
&lt;h2 id="1-statistical-manifold"&gt;1. Statistical Manifold&lt;/h2&gt;
&lt;p&gt;和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。&lt;/p&gt;
&lt;p&gt;那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。&lt;/p&gt;
&lt;h2 id="2-score-function"&gt;2. Score Function&lt;/h2&gt;
&lt;p&gt;对于一个受到参数 $\theta$ 控制，关于随机变量 $x$ 的分布 $q(x; \theta)$，我们可以定义其 Score Function：&lt;/p&gt;
$$
\begin{gather}
s_\theta(x, \theta) = \nabla_\theta \log q(x; \theta)\\
s_x(x, \theta) = \nabla_x \log q(x; \theta)\\
\end{gather}
$$&lt;p&gt;对于 score function，我们可以从两个 level 理解它。&lt;/p&gt;
&lt;p&gt;首先，直观的、几何的讲，对于 score function $s_x(x, \theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\theta(x, \theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\theta$ 的概率密度对数增长最快的方向。&lt;/p&gt;</description></item></channel></rss>