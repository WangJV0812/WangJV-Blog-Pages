<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>GPU on WangJV Blog</title><link>https://wangjv0812.cn/tags/gpu/</link><description>Recent content in GPU on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.153.3</generator><language>en-us</language><lastBuildDate>Tue, 16 Dec 2025 10:11:25 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/tags/gpu/index.xml" rel="self" type="application/rss+xml"/><item><title>Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link>https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/</link><pubDate>Tue, 16 Dec 2025 10:11:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/12/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness/</guid><description>&lt;h2 id="1-transformer-复杂度分析"&gt;1. Transformer 复杂度分析&lt;/h2&gt;
&lt;h3 id="11-矩阵运算复杂度分析"&gt;1.1. 矩阵运算复杂度分析&lt;/h3&gt;
&lt;p&gt;Transformer 模型事实上是矩阵乘法的堆叠。让我们先从基础的向量乘法的复杂度分析开始，一步步扩展到对张量运算的复杂度有清晰的认识。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于向量 $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$，那么向量之间的点积需要进行 $n$ 次乘法和 $n$ 次加法，总的时间复杂度为 $O(2n)$。&lt;/li&gt;
&lt;li&gt;对于矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和 $\mathbf{x} \in \mathbb{R}^{n}$，矩阵向量乘法 $\mathbf{Ax}$ 需要进行 $m$ 次向量点积，每次点积的复杂度为 $O(2n)$，因此总的时间复杂度为 $O(2mn)$。&lt;/li&gt;
&lt;li&gt;对于矩阵 $\mathbf{A} \in \mathbb{R}^{m\times n}, \mathbf{B} \in \mathbb{R}^{n\times p}$，矩阵乘法 $\mathbf{AB}$ 需要进行 $m \times p$ 次向量点积，每次点积的复杂度为 $O(2n)$，因此总的时间复杂度为 $O(2mnp)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面讨论了基础矩阵运算的时间复杂度，下面不妨上上强度，看看张量运算的复杂度。对于张量 $\mathbf{A} \in \mathbb{R}^{{\color{red}{GH}} IJ \color{blue}{KL}}, \mathbf{B} \in \mathbb{R}^{{\color{red}{GH}} MN \color{blue}{KL}}$，其中维度 $\color{red}GH$ 是 batch 维度，$\color{blue} KL$ 是被吸收 (Contracting) 的维度。那么可以定义 &lt;a href="https://docs.pytorch.org/docs/stable/generated/torch.einsum.html"&gt;einsum&lt;/a&gt; 操作如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;einsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ghijkl,ghmnkl-&amp;gt;ghijmn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的 einsum 操作中，张量 $\mathbf{A}$ 和 $\mathbf{B}$ 在维度 $\color{red}GH$ 上是对齐的 (Aligned)，在维度 $\color{blue}KL$ 上是被吸收的 (Contracting)。那么这个 einsum 操作的时间复杂度为：&lt;/p&gt;</description></item></channel></rss>