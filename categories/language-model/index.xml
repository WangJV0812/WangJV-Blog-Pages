<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Language Model on WangJV Blog</title><link>https://wangjv0812.cn/categories/language-model/</link><description>Recent content in Language Model on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.153.2</generator><language>en-us</language><lastBuildDate>Tue, 04 Nov 2025 18:35:25 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/categories/language-model/index.xml" rel="self" type="application/rss+xml"/><item><title>From Diffusion to Diffusion Language Model</title><link>https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/</link><pubDate>Tue, 04 Nov 2025 18:35:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/</guid><description>&lt;p&gt;&lt;img alt="alt text" loading="lazy" src="https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/posts/From%20Diffusion%20to%20Diffusion%20Language%20Model/Images/DLMinrecentyear.png"&gt;&lt;/p&gt;
&lt;p&gt;对于现在的大模型，普遍使用自回归模型。对于一个长度为 $n$ 的文本，自回归模型将其分解为：&lt;/p&gt;
$$
P(x_{1:n}) = \prod_{i=1}^n P(x_i \mid x_{&lt;i})
$$&lt;p&gt;这也是在做自回归时，使用 Decoder Only 的原因。但是一个清晰的问题是，自回归模型是自然语言模型的唯一选择吗？答案可能不是100%的不。diffusion 似乎是一个很符合直接的推理过程，它会先写出一个文本的草稿，之后再对草稿不断修改（降噪），最终得到完整的文本。&lt;/p&gt;
&lt;h2 id="1-无监督学习和流形假说"&gt;1. 无监督学习和流形假说&lt;/h2&gt;
&lt;p&gt;对于经典的无监督学习任务，我们面对的是巨量的、常常高维的、没有人工提取特征的原始数据。无监督学习最核心的任务是在找到这些数据之间的相对关系提取出数据的内在结构。我们不妨以图像生成任务举例子，对于一个尺寸为 $1920\times 1080$ 的图片，其像素总量为 $2073600$。但是这些维度之不可能是独立的（因为如果完全独立，只能得到没有任何信息的纯噪声），数据的维度之间存在着复杂的相关性质。换句话说，对于图像数据，数据点并不是均匀的分布在 $2073600$ 维的空间中，它只分布在空间中的一小部分区域，或者用微分几何的语言描述，数据分布在一个高维空间中的一条流形（manifold）上。这就是流形假说（manifold hypothesis），它是无监督学习的理论基础。那么无监督学习的任务就变成了如何用一个神经网络来建模这条流形。一个很显然的流形建模工具是使用概率分布，对于不数据流形上的点，概率赋 $0$ 就好了。&lt;/p&gt;
&lt;p&gt;关于流形假说的更多讨论，可以参考 &lt;a href="https://wangjv0812.cn/2025/08/noise-contrastive-estimation/#1-%E5%8A%A8%E6%9C%BA"&gt;Noise Contrastive Estimation/1. 动机&lt;/a&gt; 中对于使用概率分布建模数据分布流形的讨论。&lt;/p&gt;
&lt;h2 id="2-continuous-diffusion"&gt;2. Continuous Diffusion&lt;/h2&gt;
&lt;p&gt;Diffusion 模型可能是目前最成功的高维连续数据的建模工具。相比于 &lt;a href="https://wangjv0812.cn/2025/10/variational-autoencoder/"&gt;VAE&lt;/a&gt;、GAN、&lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 等，Diffusion 很核心的优势在于&lt;strong&gt;它显示式的给出了一条图像生成的路径（从噪声到数据），大大减少了生成器所需要搜索的空间范围&lt;/strong&gt;。如果你现在对这句话一头雾水，不妨先对它有一个大概的印象，看完本节后很大概率会有一个跟深刻的认识。&lt;/p&gt;
&lt;p&gt;Diffusion 模型可以分为三个过程，分别是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Forward Process，向原始数据添加噪声，事实上可以理解为一个数据增强的过程。&lt;/li&gt;
&lt;li&gt;Backward Process，从噪声中恢复数据的过程，可以理解为 Diffusion 的推理。&lt;/li&gt;
&lt;li&gt;训练过程，训练一个神经网络来学习 Backward Process。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="21-forward-process"&gt;2.1. Forward Process&lt;/h3&gt;
&lt;p&gt;对于一个数据样本 $x$，不妨将其标记为 $x_0$，在后面的讨论中，可以将 $x_0$ 视作一个确定性变量。Diffusion 过程每一步都向原始数据添加一个很小的噪声，添加噪声的过程不妨用一个条件概率描述：&lt;/p&gt;
$$
p(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
$$&lt;p&gt;其中 $\beta_t \in (0, 1)$，是一个用于控制噪声大小的超参数。我们希望求得 $x_t$ 在 $x_0$ 下的条件分布：&lt;/p&gt;</description></item></channel></rss>