<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>数学 on WangJV Blog</title><link>https://wangjv0812.cn/categories/%E6%95%B0%E5%AD%A6/</link><description>Recent content in 数学 on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.152.2</generator><language>en-us</language><lastBuildDate>Sat, 13 Sep 2025 17:13:56 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/categories/%E6%95%B0%E5%AD%A6/index.xml" rel="self" type="application/rss+xml"/><item><title>Fisher Information and Fisher Divergence</title><link>https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/</link><pubDate>Sat, 13 Sep 2025 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/</guid><description>&lt;p&gt;在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。&lt;/li&gt;
&lt;li&gt;Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面则给出一些不那么直观的，数学形式上的解释。&lt;/p&gt;
&lt;h2 id="1-statistical-manifold"&gt;1. Statistical Manifold&lt;/h2&gt;
&lt;p&gt;和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。&lt;/p&gt;
&lt;p&gt;那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。&lt;/p&gt;
&lt;h2 id="2-score-function"&gt;2. Score Function&lt;/h2&gt;
&lt;p&gt;对于一个数据集 $\{x_1, x_2, \cdots, x_n\}$，我们假设其服从于一个理想的概率分布 $p(x)$，我们不知道 $p(x)$ 的具体形式，只知道它的一系列采样（就是数据集）。我们希望可以通过一个受到参数 $q(x, \theta)$ 控制的分布族来近似它。那么一个很容易想到办法是，通过 KL 散度来衡量 $p(x)$ 和 $q(x, \theta)$ 之间的差异：&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(p(x) \| q(x, \theta))
&amp;= \mathbb{E}_{x\sim p(x)} \bigg[\log p(x) -\log q(x, \theta)\bigg]\\
&amp;= \mathbb{E}_{x\sim p(x)} \bigg[\log p(x)\bigg] - \mathbb{E}_{x\sim p(x)} \bigg[\log q(x, \theta)\bigg]\\
\end{aligned}
$$&lt;p&gt;其中 $\mathbb{E}_{x\sim p(x)} \bigg[\log p(x)\bigg]$ 是 $p(x)$ 的熵，与待优化参数 $\theta$ 无关，因此：&lt;/p&gt;</description></item></channel></rss>