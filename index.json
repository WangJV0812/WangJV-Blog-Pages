[{"content":"应用级的机器人学李代数介绍，没有深入介绍数学基础，更倾向于技术应用\n","permalink":"https://wangjv0812.cn/resources/mathematicalintroductionofroboticmanipulation/","summary":"\u003cp\u003e应用级的机器人学李代数介绍，没有深入介绍数学基础，更倾向于技术应用\u003c/p\u003e","title":"A Mathematical Introduction of Robotic Manipulation"},{"content":"著名的《虎书》，计算机图形学的基础教材，涵盖了图形学的基本原理和应用\n","permalink":"https://wangjv0812.cn/resources/fundamentalsofcomputergraphicsfourthedition/","summary":"\u003cp\u003e著名的《虎书》，计算机图形学的基础教材，涵盖了图形学的基本原理和应用\u003c/p\u003e","title":"Fundamentals of Computer Graphics Fourth Edition"},{"content":"Lie Group 和 Lie Algebra 入门教材，没有规避深刻的数学模型，但是使用的方法很处等\n","permalink":"https://wangjv0812.cn/resources/naivelietheory/","summary":"\u003cp\u003eLie Group 和 Lie Algebra 入门教材，没有规避深刻的数学模型，但是使用的方法很处等\u003c/p\u003e","title":"Naive Lie Theory"},{"content":"机器人状态估计权威教材，包含十分深刻的数学推导。从滤波器设计、三维刚体几何、李群李代数等方面对机器人状态估计进行了系统的介绍。\n","permalink":"https://wangjv0812.cn/resources/state-estimation-for-robotics-copy/","summary":"\u003cp\u003e机器人状态估计权威教材，包含十分深刻的数学推导。从滤波器设计、三维刚体几何、李群李代数等方面对机器人状态估计进行了系统的介绍。\u003c/p\u003e","title":"State Estimation for Robotics"},{"content":"1. 测地线（Geodesic） 对于一个曲率不为 $0$ 的流形，其上任意两点之间的最短路径无法简单的像是欧式几何上一样，直接通过一条平直曲线描述。因此在一个曲面上，我们定一任意两点之间最短的路径为测地线（Geodesic）。我们不妨记在 $SO(3)$ 上受到参数 $t$ 控制的任意曲线为 $R(t)$。对于一个 Riemann 度量 $\\| \\cdot \\|$，我们可以定义曲线的弧长：\n$$ \\begin{gather} L(R)\\int_a^b \\sqrt{\\| dR \\|^2} = \\int_a^b \\sqrt{\\| \\dot R \\|^2} dt \\end{gather} $$这就是在一个流形（或者说曲面）上关于任意曲线的弧长泛函。对于上个泛函，我们只需要对 $R(t)$ 变分，得到任意两点之间弧长泛函最小的曲线 $R(t)$ 满足的形式，测地线问题就解决了。但是如果你对变分和 Eular-Lagrange 方程熟悉的话，会自然的发现上面这个形式要处理并不容易。我们一般会扔掉 $\\sqrt{\\cdot}$，直接取曲线的能量泛函：\n$$ \\begin{gather} E(R) = \\frac{1}{2} \\int_a^b \\| \\dot R \\|^2 dt \\end{gather} $$1.1. SO(3) 上的 Riemann 度量 在任意的 Riemann 流形 $M$ 上的每个点 $p \\in M$，切空间 $T_pM$ 是一个向量空间。我们在这个空间上定义一个内积：\n$$ \\langle \\cdot, \\cdot \\rangle_p : T_pM \\times T_pM \\to \\mathbb{R} $$内积需要满足四个性质：\n对称性：$\\langle X, Y \\rangle_p = \\langle Y, X \\rangle_p$ 双线性：线性于两个分量 正定性：$\\langle X, X \\rangle_p \u003e 0$ 当 $X \\neq 0$ 平滑性：对于任意光滑向量场 $X, Y$，$\\langle X, Y \\rangle$ 是一个光滑函数 当我们将每个点的内积拼接起来时，可以得到一个光滑的张量场：\n$$ \\begin{gather} g_p(X, Y) = \\langle X, Y \\rangle_p\\\\ g : TM \\times TM \\to \\mathbb{R} \\end{gather} $$我们 $g$ 是黎曼度量（Riemann Metric）。那么诱导的，我们需要在 SO(3) 上定义一个合理的内积，就解决了能量泛函中所需要的 $\\| \\dot R \\|^2$ 的问题。不妨用内积来描述能量泛函更佳优雅：\n$$ \\begin{gather} E(R) = \\frac{1}{2} \\int_a^b \\langle \\dot R, \\dot R \\rangle dt \\end{gather} $$在 SO(3) 上，我们知道，切空间 $T_{R(t)}SO(3)$ 是 $3 \\times 3$ 反对称矩阵和叉乘构成的群，其同构于三维欧式空间 $\\mathbb{R}^3$ 与加法构成的群。因此对于任意 $R \\in SO(3)$，$\\dot R \\in T_{R(t)}SO(3)$，$\\dot R$ 同构于 $\\omega \\in \\mathbb{R}^3$。一个自然的内积定义是：\n$$ \\langle \\omega_1, \\omega_2 \\rangle = \\frac{1}{2} \\omega_1^T \\omega_2 $$很容易证明，该内积在 $T_{R(t)}SO(3)$ 中等价于：\n$$ \\begin{gather} \\langle \\dot R_1, \\dot R_2 \\rangle = \\frac{1}{2} \\text{tr}(\\dot R_1^T \\dot R_2) \\end{gather} $$这个内积有很棒的性质，它在左作用和右作用下都是不变的，我们称之为双边不变（bi-invariant）内积。也就是说对于任意 $A, B \\in SO(3)$，都有：\n$$ \\langle A \\dot R_1 B, A \\dot R_2 B \\rangle = \\langle \\dot R_1, \\dot R_2 \\rangle $$双不变性代表了对于任意群元 $A, B\\in SO(3)$，不论将其变换到何处，其内积都保持不变。SO(3) 中存在双不变度量，恰恰对应于 $SO(3)$ 代表刚体运动与位置无关的物理意义。利用这个内积，我们可以将能量泛函写成：\n$$ \\begin{gather}\\begin{aligned} E(R) \u0026= \\frac{1}{2} \\int_a^b \\langle \\dot R, \\dot R \\rangle dt\\\\ \u0026= \\frac{1}{4} \\int_a^b \\text{tr}(\\dot R^T \\dot R) dt \\end{aligned}\\end{gather} $$对于优化问题，不妨把形式前的 $\\frac{1}{2}$ 忽略掉，它并不影响优化结果：\n$$ \\begin{gather} E(R) = \\int_a^b \\text{tr}(\\dot R^T \\dot R) dt \\end{gather} $$1.2. 引入约束 由于 $R\\in SO(3)$，需要满足正交条件：\n$$ R^T R = I $$因此不能直接优化上面的能量泛函。我们需要通过拉格朗日乘子法 (Lagrange Multiplier Method) 引入约束。假设存在对称矩阵 $\\Lambda(t)$，我们可以构造约束量：\n$$ \\begin{gather} \\begin{aligned} E(R) \u0026= \\int_a^b \\text{tr}(\\dot R^T \\dot R) dt + \\int_a^b \\text{tr}(\\Lambda^T (R^T R - I)) dt\\\\ \u0026= \\int_a^b \\text{tr}(\\dot R^T \\dot R) + \\text{tr}(\\Lambda^T (R^T R - I)) dt\\\\ \u0026= \\int_a^b \\mathcal L(R, \\dot R, \\Lambda) dt \\end{aligned} \\end{gather} $$其中 $L(R, \\dot R, \\Lambda)$ 是拉格朗日量，有：\n$$ L(R, \\dot R, \\Lambda) = \\text{tr}(\\dot R^T \\dot R) + \\text{tr}(\\Lambda^T (R^T R - I)) $$1.3. 变分并得到 Euler–Lagrange 方程 对于 $L(R, \\dot R, \\Lambda)$，我们希望对 $R$ 变分，得到满足测地线条件的 $R(t)$。考虑一个小扰动 $\\epsilon$：\n$$ \\begin{aligned} \u0026R(t) = R(t) + \\epsilon \\delta R(t)\u0026 \\text{where: }\\sigma R(a) = \\delta R(b) = 0 \\end{aligned} $$那么拉格朗日量 $L(R, \\dot R, \\Lambda)$ 对于 $R$ 的变分为：\n$$ \\delta L = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(R + \\epsilon \\delta R, \\dot R + \\epsilon \\delta \\dot R, \\Lambda) $$不妨分块分析。对于能量项，有：\n$$ \\text{tr}\\left((\\dot R + \\epsilon \\delta \\dot R)^T (\\dot R + \\epsilon \\delta \\dot R)\\right) = \\text{tr}(\\dot R^T \\dot R) + 2\\epsilon \\text{tr}(\\dot R^T \\delta \\dot R) + O(\\epsilon^2) $$那么：\n$$ \\begin{gather}\\begin{aligned} \u0026\\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\text{tr}\\left((\\dot R + \\epsilon \\delta \\dot R)^T (\\dot R + \\epsilon \\delta \\dot R)\\right)\\\\ =\u0026 \\lim_{\\epsilon\\to 0}\\frac{\\text{tr}\\left((\\dot R + \\epsilon \\delta \\dot R)^T (\\dot R + \\epsilon \\delta \\dot R)\\right) - \\text{tr}(\\dot R^T \\dot R)}{\\epsilon}\\\\ =\u0026 2 \\text{tr}(\\dot R^T \\delta \\dot R) \\end{aligned}\\end{gather} $$对于约束项，有：\n$$ \\begin{gather} \\begin{aligned} \u0026\\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\text{tr}\\left(\\Lambda^T \\left((R(t) + \\epsilon \\delta R(t))^T (R(t) + \\epsilon \\delta R(t)) - I\\right)\\right)\\\\ =\u0026 \\lim_{\\epsilon\\to 0} \\frac{\\text{tr}\\left(\\Lambda^T \\left((R(t) + \\epsilon \\delta R(t))^T (R(t) + \\epsilon \\delta R(t)) - I\\right)\\right) - \\text{tr}\\left(\\Lambda^T (R(t)^T R(t) - I)\\right)}{\\epsilon}\\\\ =\u0026 2\\text{tr}\\left(\\Lambda^T R^T\\delta R \\right) \\end{aligned} \\end{gather} $$那么有：\n$$ \\begin{gather} \\delta \\mathcal L = 2 \\text{tr}(\\dot R^T \\delta \\dot R) + 2\\text{tr}\\left(\\Lambda^T R^T\\delta R \\right) \\end{gather} $$那么有：\n$$ \\begin{gather}\\begin{aligned} \\delta E \u0026= \\int_a^b \\delta \\mathcal L dt \\\\ \u0026= \\int_a^b 2 \\text{tr}(\\dot R^T \\delta \\dot R) + 2\\text{tr}\\left(\\Lambda^T R^T\\delta R \\right) dt \\end{aligned}\\end{gather} $$对于第一项，可以使用分布积分计算：\n$$ \\begin{gather} \\begin{aligned} \\int_a^b 2 \\text{tr}(\\dot R^T \\delta \\dot R) dt \u0026= 2 \\text{tr}(\\dot R^T \\delta R) \\bigg|_a^b - \\int_a^b 2 \\text{tr}(\\ddot R^T \\delta R) dt\\\\ \u0026= - \\int_a^b 2 \\text{tr}(\\ddot R^T \\delta R) dt \\end{aligned} \\end{gather} $$那么有：\n$$ \\begin{aligned} \\sigma E \u0026= \\int_a^b -\\text{tr}(\\ddot R^T \\delta R) + 2\\text{tr}\\left(\\Lambda^T R^T\\delta R \\right)dt\\\\ \u0026= \\int_a^b 2\\text{tr}\\left( \\Lambda^T R^T\\delta R - \\ddot R^T \\delta R \\right)dt\\\\ \u0026= \\int_a^b 2\\text{tr}\\left( (\\Lambda^T R^T - \\ddot R^T) \\delta R \\right)dt \\end{aligned} $$泛函最优条件是：\n$$ \\delta E = 0 $$那么要对任意 $\\delta R$ 都成立，必须有：\n$$ \\begin{aligned} \\Lambda^T R^T - \\ddot R^T \u0026= 0\\\\ \\ddot R \u0026= R \\Lambda \\end{aligned} $$这就是我们的测地线方程！\n$$ \\begin{gather} \\boxed{\\ddot R = R \\Lambda} \\end{gather} $$2. 从测地线到指数映射 在 SO(3) 上，不妨假设存在一条通过单位元 $I$ 的曲线 $R(t)$，满足：\n$$ R(0) = I $$那么应有：\n$$ R^T(t)R(t) = I $$两边对 $t$ 求导，则有：\n$$ \\begin{aligned} \\dot R^T(t) R(t) + R^T(t) \\dot R(t) \u0026= 0\\\\ R(t)^T\\dot R^T(t) \u0026= -\\left(R(t)^T\\dot R^T(t)\\right)^T \\end{aligned} $$可以知道 $\\dot R^T(t) R(t)$ 是一个反对称矩阵，不妨将这个反对称矩阵记作 $\\Omega(t)$，则有：\n$$ \\begin{aligned} R(t)^T\\dot R^T(t) \u0026= \\Omega(t)\\\\ \\end{aligned} $$对上面的形式再次求导，则有：\n$$ \\begin{aligned} \\dot \\Omega(t) = \\dot R^T\\dot R + R^T \\ddot R \\end{aligned} $$根据约束条件，不难得出：\n$$ \\dot R = -R \\dot R^T R $$那么第一项可以变换为：\n$$ \\dot R^T\\dot R = -\\dot R^T R \\dot R^T R = -\\Omega^2 $$那么，有：\n$$ \\dot \\Omega = R^T \\ddot R - \\Omega^2 $$带入测地线方程，则有：\n$$ \\dot \\Omega = \\Lambda - \\Omega^2 $$我们知道：\n形式 性质 $\\Omega$ 反对称矩阵 $\\Lambda$ 对称矩阵 $\\Omega^2$ 对称矩阵 $\\dot \\Omega$ 反对称矩阵 $\\Lambda - \\Omega^2$ 对称矩阵 这要求 $\\dot \\Omega$ 即是反对称矩阵，还是对称矩阵。因此唯一的可能形式：\n$$ \\dot \\Omega = 0 $$那么 $\\Omega(t)$ 则与 $t$ 无关，是一个常矩阵，不妨记作 $\\Omega$。那么有：\n$$ \\dot R(t) = R(t) \\Omega $$如果你对常微分方程熟悉不难看出，对于上面这个简单的一阶常微分方程，其解可以很容易的写出：\n$$ R(t) = R_0\\exp(t\\Omega) $$这恰恰是指数映射！\n3. 讨论 在3d-kinematics-and-dynamics 一文中我们讨论过旋转矩阵导数与角速度之间的关系。不难发现事实上 $\\Omega$ 就是角速度向量的反对称矩阵的形式。\n那么 $\\dot \\Omega = 0$ 的物理含义就很好理解了。这事实上对应了三维刚体上的匀速旋转。这也是测地线的物理意义。对于一个刚体来说，在没有外力矩作用下，其会以匀速旋转的形式运动。而这个匀速旋转恰恰对应了 SO(3) 上的测地线。\n那么对于指数映射 $R = \\exp(\\omega^\\times)$，可以解释为：从单位元 $I$ 出发，在单位时间以角速度 $\\omega$ 旋转所到达的角度。\n那么李代数所同构的单参数子群的物理含义也很好解释了：事实上表达的是从单位元出发，以全部角速度出发，在单位时间内的所有测地线的总和。\n","permalink":"https://wangjv0812.cn/2025/10/geodesic-and-lie-algebra-in-so3/","summary":"\u003ch2 id=\"1-测地线geodesic\"\u003e1. 测地线（Geodesic）\u003c/h2\u003e\n\u003cp\u003e对于一个曲率不为 $0$ 的流形，其上任意两点之间的最短路径无法简单的像是欧式几何上一样，直接通过一条平直曲线描述。因此在一个曲面上，我们定一任意两点之间最短的路径为测地线（Geodesic）。我们不妨记在 $SO(3)$ 上受到参数 $t$ 控制的任意曲线为 $R(t)$。对于一个 Riemann 度量 $\\| \\cdot \\|$，我们可以定义曲线的弧长：\u003c/p\u003e\n$$\n\\begin{gather}\nL(R)\\int_a^b \\sqrt{\\| dR \\|^2} = \\int_a^b \\sqrt{\\| \\dot R \\|^2} dt\n\\end{gather}\n$$\u003cp\u003e这就是在一个流形（或者说曲面）上关于任意曲线的弧长泛函。对于上个泛函，我们只需要对 $R(t)$ 变分，得到任意两点之间弧长泛函最小的曲线 $R(t)$ 满足的形式，测地线问题就解决了。但是如果你对变分和 Eular-Lagrange 方程熟悉的话，会自然的发现上面这个形式要处理并不容易。我们一般会扔掉 $\\sqrt{\\cdot}$，直接取曲线的能量泛函：\u003c/p\u003e\n$$\n\\begin{gather}\nE(R) = \\frac{1}{2} \\int_a^b \\| \\dot R \\|^2 dt\n\\end{gather}\n$$\u003ch3 id=\"11-so3-上的-riemann-度量\"\u003e1.1. SO(3) 上的 Riemann 度量\u003c/h3\u003e\n\u003cp\u003e在任意的 Riemann 流形 $M$ 上的每个点 $p \\in M$，切空间 $T_pM$ 是一个向量空间。我们在这个空间上定义一个内积：\u003c/p\u003e\n$$\n\\langle \\cdot, \\cdot \\rangle_p : T_pM \\times T_pM \\to \\mathbb{R}\n$$\u003cp\u003e内积需要满足四个性质：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e对称性：$\\langle X, Y \\rangle_p = \\langle Y, X \\rangle_p$\u003c/li\u003e\n\u003cli\u003e双线性：线性于两个分量\u003c/li\u003e\n\u003cli\u003e正定性：$\\langle X, X \\rangle_p \u003e 0$ 当 $X \\neq 0$\u003c/li\u003e\n\u003cli\u003e平滑性：对于任意光滑向量场 $X, Y$，$\\langle X, Y \\rangle$ 是一个光滑函数\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e当我们将每个点的内积拼接起来时，可以得到一个光滑的张量场：\u003c/p\u003e","title":"Geodesic and Lie Algebra in SO(3)"},{"content":"1. 动机 对于一个状态 $x$，在通过传感器进行一次观测 $y$ 后，我们希望可以计算出后验分布 $p(x\\mid y)$，即在观测到 $y$ 的情况下，状态 $x$ 的分布。为了计算后验分布的形式，我们可以使用贝叶斯公式：\n$$ \\begin{aligned} p(x\\mid y) \u0026= \\frac{p(y\\mid x) p(x)}{p(y)}\\\\ \u0026= \\frac{p(y\\mid x) p(x)}{\\int p(y\\mid x) p(x) \\mathrm{d}x} \\end{aligned} $$对于状态估计问题，我们只需要找到使得 $p(x\\mid y)$ 最大的状态 $x$。 由于 $p(y)$ 是一个常数，我们可以将其忽略，只需要最大化分子部分。但是一些应用场景中，还需要我们计算出具体的后验分布的形式。但是这往往非常的难，边缘似然 $p(y)$ 的计算往往是几乎不可能的。因此一个或许可行的办法是，使用一个相对简单的分布 $q(x)$ 代替后验分布。那么原本的贝叶斯问题变成一个变分优化问题，不妨用 KL 散度衡量差异。\n2. KL 散度的不对称性 但是一个很核心的问题是，KL 散度不具有对称性，$D_{KL}(p\\| q) \\neq D_{KL}(q\\| p)$，在实际操作中，我们应该如何选择？不妨先观察这两个对称的 KL 散度的具体形式：\n$$ \\begin{aligned} D_{KL}(p \\| q) \u0026= -\\int p(x\\mid y) \\log \\frac{q(x)}{p(x\\mid y)} \\mathrm{d}x\\\\ \u0026= -\\int p(x\\mid y) \\log q(x) \\mathrm{d}x + \\int p(x\\mid y) \\log p(x\\mid y) \\mathrm{d}x \\end{aligned} $$其中，$\\int p(x\\mid y) \\log q(x) \\mathrm{d}x$ 实质上是 $p$ 与 $q$ 之间的交叉熵，不妨定义 $H(p, q) = \\int p(x\\mid y) \\log q(x) \\mathrm{d}x$。而 $\\int p(x\\mid y) \\log p(x\\mid y) \\mathrm{d}x$ 则是关于 $p$ 的熵。$p$ 的熵实质上是一个常数，因此实质上：\n$$ \\begin{aligned} \\arg \\min_q D_{KL}(p \\| q) \u0026= \\arg \\max \\int p(x\\mid y) \\log q(x) \\mathrm{d}x\\\\ \u0026= \\arg \\max H(p, q) \\end{aligned} $$因此优化 $D_{KL}(p \\| q)$ 实质上相当于优化 $p$ 与 $q$ 之间的交叉熵。观察交叉熵的形式不难发现，我们事实上是在一个固定的权重 $p$ 下，最大化 $-\\log q$。因此分布 $p$ 不能偷懒，需要在 $p$ 所有足够大的地方让 $q$ 也足够大。我们不难推导出，$D_{KL}(p \\| q)$ 会强迫 $q$ 尽可能的覆盖 $p$ 的所有高概率区域。\n在这个理解的基础上，我们不妨观察 $D_{KL}(q\\| p)$，有：\n$$ \\begin{aligned} D_{KL}(q\\| p) \u0026= -\\int q(x) \\log \\frac{p(x\\mid y)}{q(x)} \\mathrm{d}x\\\\ \u0026= -\\int q(x) \\log p(x\\mid y) \\mathrm{d}x + \\int q(x) \\log q(x) \\mathrm{d}x \\end{aligned} $$因此，优化\n同样地，定义 $H(q, p) = \\int q(x) \\log p(x\\mid y) \\mathrm{d}x$，q 的熵有 $H(q) =-\\int q(x) \\log q(x) \\mathrm{d}x$。那么优化这个形式下的 KL 散度的形式可以写成：\n$$ \\begin{aligned} \\arg \\min_q D_{KL}(q\\| p) \u0026= \\argmax_q H(q, p)\\\\ \u0026= \\argmax_q \\int q(x) \\log p(x\\mid y) \\mathrm{d}x + \\int q(x) \\log q(x) \\mathrm{d}x\\\\ \u0026= \\argmax_q H(q, p) + H(q) \\end{aligned} $$类似的角度分析，优化 $D_{KL}(p \\| q)$ 实质上是在最大化 $q$ 与 $p$ 的交叉熵的基础上增加一个负熵的正则化，交叉熵 $H(q, p)$ 实质上是在固定了信息分布 $\\log p$ 的情况下，通过优化权重分布 $q$ 来最大化期望。此时 $q$ 有更大的自由度，可以不需要尽可能的完全覆盖，对于无法完美覆盖的区域我们可以直接通过降低权重忽略掉。但是这带来一个问题，如果只用交叉熵损失，$q$ 有一个讨巧的方法，可以学习一个 Dirac Distribution，直接把所有的概率质量都集中在 $p$ 的最大值处。为了避免这个情况出现，负熵正则化 $H(q)$ 则强迫模 $q$ 不那么集中，学习一个更 “宽” 的分布。因此，$D_{KL}(q\\| p)$ 会强迫 $q$ 尽可能地集中在 $p$ 的某一个高概率区域，而忽略掉其他的高概率区域。\n如果 $p(x)$ 是一个多峰分布，那么选择不同的 KL 散度对 $q(x)$ 的影响很明显：\n其中 图(a) 是通过 $D_{KL}(p\\| q)$ 驱动学习的 $q(x)$，而 (b) 和 (c) 则是在不同初始下通过 $D_{KL}(q\\| p)$ 学习的 $q(x)$。可以看到 (a) 中的 $q(x)$ 覆盖了 $p(x)$ 的所有高概率区域，而 (b) 和 (c) 则分别集中在 $p(x)$ 的不同峰值处。\n从这个角度分析，不难得出这样的结论：\n如果我们希望我们的模型 $q$ 需要尽可能的覆盖真实分布 $p$，为此可以牺牲模型一定的准确性，则应该选择 $D_{KL}(p\\| q)$。 如果我们希望模型 $q$ 尽可能的准确，为了准确性可以放弃一些 corner case，则可以选择 $D_{KL}(q\\| p)$。 因为 $q$ 是解析的，$D_{KL}(q\\| p)$ 通常更容易计算。 3. Bayesian 推断和 ELBO 对于 Bayesian Inference 问题，我们无法计算后验分布 $p(x\\mid y)$，也无法很容易的对其采样。想直接计算 $D_{KL}(p \\| q)$ 是非常困难的。因此我们选择优化 $D_{KL}(q\\| p)$。但是这个形式无法直接计算，不妨展开这个形式分析：\n$$ \\begin{aligned} D_{KL}(q\\| p) \u0026= -\\int q(x) \\log \\frac{p(x\\mid y)}{q(x)} \\mathrm{d}x\\\\ \u0026= -\\int q(x) \\bigg[\\log p(x\\mid y) - \\log q(x)\\bigg] \\mathrm{d}x\\\\ \u0026= -\\int q(x) \\bigg[\\log p(y\\mid x) + \\log p(x) - \\log p(y)- \\log q(x)\\bigg] \\mathrm{d}x\\\\ \u0026= \\mathbb{E}_{x\\sim q}\\bigg[\\log p(y\\mid x) + \\log p(x) - \\log q(x)\\bigg] - \\mathbb{E}_{x\\sim q}\\bigg[\\log p(y)\\bigg] \\end{aligned} $$其中，显然有：\n$$ \\begin{aligned} \\mathbb{E}_{x\\sim q}\\big[ \\log p(y)\\big] = \\log p(y) \\end{aligned} $$那么，这个形式可以变换为：\n$$ \\begin{aligned} D_{KL}(q\\| p) \u0026= \\mathbb{E}_{x\\sim q}\\bigg[\\log p(y\\mid x)\\bigg] - D_{KL}(q(x) \\| p(x)) - \\log p(y) \\end{aligned} $$显然 $\\log p(y)$ 是一个常数，因此优化 $D_{KL}(q\\| p)$ 等价于优化：\n$$ D_{KL}(q\\| p) \\sim \\mathbb{E}_{x\\sim q}\\bigg[\\log p(y\\mid x)\\bigg] - D_{KL}(q(x) \\| p(x)) $$这个形式就是 ELBO (Evidence Lower Bound)。我们可以定义：\n$$ \\text{ELBO}(q) = \\mathbb{E}_{x\\sim q}\\bigg[\\log p(y\\mid x)\\bigg] - D_{KL}(q(x) \\| p(x)) $$对于 ELBO，我们还有一个等价的形式。观察上面的推导过程，不难发现：\n$$ \\text{ELBO}(q) = \\mathbb{E}_{x\\sim q}\\bigg[\\log \\frac{p(x, y)}{q(x)}\\bigg] $$此时，$\\text{ELBO}(q)$ 的形式我们都有已经了解了。对于一个受到参数控制分布族 $q$，我们可以通过优化上面的形式来获得对后验最优的估计。\n4. Factorized Variational Inference 4.1. 对于 $D_{KL}(q\\|p)$ 的 Factorized Variational Inference 但是此时对于具体如何优化 ELBO 还没有给出一个具体的方法。下面我们要引入一个比较弱的简化假设。我们假设状态 $x$ 之间不是完全耦合的，可以分解成几个独立的子状态 $x = (x_1, x_2, \\ldots, x_n)$。那么假设的模型分布 $q(x)$ 可以写作：\n$$ q(x) = \\prod_{i=1}^n q_i(x_i) $$要注意的是，我们没有对 $q(x)$ 的具体形式做任何假设，只假设 $q(x)$ 之间有可以分解的独立分量。将假设的 $q(x)$ 的分解形式带入 ELBO 中，有：\n$$ \\begin{aligned} \\text{ELBO}(q) \u0026= \\mathbb{E}_{x\\sim q}\\bigg[\\log \\frac{p(x, y)}{q(x)}\\bigg]\\\\ \u0026= \\int \\prod_{i=1}^n q_i(x_i) \\bigg[\\log p(x, y) - \\sum_i \\log q_i(x_i) \\bigg] \\mathrm{d}x \\end{aligned} $$此处不妨用用 Fubini\u0026rsquo;s theorem，将 $x_j$ 的分量提取出来，先对 $x_j$ 积分：\n$$ \\begin{aligned} \\text{ELBO}(q) \u0026= \\int_{x_j} q_j(x_j) \\int_{x_{i\\neq j}} \\prod_{i\\neq j} q_i(x_i) \\bigg[\\log p(x, y) - \\sum_{i\\neq j} \\log q_i(x_i) - \\log q_j(x_j) \\bigg] \\mathrm{d}x_{i\\neq j} \\mathrm{d}x_j\\\\ \u0026= \\int_{x_j} q_j(x_j) \\bigg \\{\\int_{x_{i\\neq j}} \\prod_{i\\neq j} q_i(x_i) \\bigg[\\log p(x, y)\\bigg] \\mathrm{d}x_{i\\neq j} \\\\ \u0026- \\int_{x_{i\\neq j}} \\prod_{i\\neq j} q_i(x_i) \\bigg[\\sum_{i\\neq j} \\log q_i(x_i) \\bigg] \\mathrm{d}x_{i\\neq j} - \\int_{x_{i\\neq j}} \\prod_{i\\neq j} q_i(x_i) \\bigg[ \\log q_j(x_j) \\bigg] \\mathrm{d}x_{i\\neq j} \\bigg \\}\\mathrm{d}x_j\\\\ \\end{aligned} $$其中，我们不难发现 $\\int_{x_{i\\neq j}} \\prod_{i\\neq j} q_i(x_i) \\bigg[\\sum_{i\\neq j} \\log q_i(x_i) \\bigg] \\mathrm{d}x_{i\\neq j}$ 是一个与 $x_j$ 无关的常数，不妨直接记作 $C$。此外还有：\n$$ \\begin{aligned} \\int_{x_{i\\neq j}} \\prod_{i\\neq j} q_i(x_i) \\bigg[ \\log qq_j(x_j) \\bigg] \\mathrm{d}x_{i\\neq j} \u0026= \\mathbb{E}_{x\\sim \\prod_{i\\neq j} q_i(x_i)} \\bigg[\\log q_j(x_j)\\bigg]\\\\ \u0026= \\log q_j(x_j) \\end{aligned} $$那么上面的形式可以化简为：\n$$ \\begin{aligned} \\text{ELBO}(q) \u0026= \\int_{x_j} q_j(x_j) \\left \\{\\int_{x_{i\\neq j}} \\log p(x, y) \\left(\\prod_{i\\neq j} q_i(x_i) \\mathrm{d}x_i\\right) - \\log q_j(x_j) \\right\\}\\mathrm{d}x_j \\end{aligned} $$不难看出，上式中的第一项是一个期望的形式，那么有：\n$$ \\int_{x_{i\\neq j}} \\log p(x, y) \\left(\\prod_{i\\neq j} q_i(x_i) \\mathrm{d}x_i\\right) = \\mathbf{E}_{x\\sim \\prod_{i\\neq j} q_i(x_i)} \\big[\\log p(x, y)\\big] $$那么根据玻尔兹曼分布，可以定义一定存在一个分布 $q^*(x_j, y)$，满足：\n$$ \\log q^*(x_j, y) = \\mathbf{E}_{x\\sim \\prod_{i\\neq j} q_i(x_i)} \\big[\\log p(x, y)\\big] + C' $$那么 $\\text{ELBO}(q)$ 可以写成这样的形式：\n$$ \\begin{aligned} \\text{ELBO}(q) = \\int_{x_j} q_j(x_j) \\left[\\log q^*(x_j, y) - \\log q_j(x_j) \\right]\\mathrm{d}x_j \\end{aligned} $$你一定注意到了，这事实上是一个 KL 散度！那么可以写作：\n$$ \\text{ELBO}(q) = -D_{KL}\\big(q_j(x_j) \\| q^*(x_j, y)\\big) $$对上面的形式求最优，事实上就是希望：\n$$ D_{KL}\\big(q_j(x_j) \\| q^*(x_j, y)\\big) = 0 $$那么有：\n$$ \\begin{aligned} \\log q_j(x_j) \u0026= \\log q^*(x_j, y)\\\\ \u0026= \\mathbf{E}_{x\\sim \\prod_{i\\neq j} q_i(x_i)} \\big[\\log p(x, y)\\big] + C' \\end{aligned} $$cool！我们成功的将一个最优化问题变为了一个可以 Monto Carlo 甚至直接解析的解决的期望问题。但是现在还面临一个小麻烦。我们在计算第 $i$ 个字空间的分布时，以来于其他所有字空间的分布。这似乎成了一个先有鸡还是先有蛋的死局。但是精通数值分析的你一定不陌生，我们可以先对所有子空间的分布做一个足够的初始，之后迭代计算即可。\n","permalink":"https://wangjv0812.cn/2025/10/variational-bayesian-inference/","summary":"\u003ch2 id=\"1-动机\"\u003e1. 动机\u003c/h2\u003e\n\u003cp\u003e对于一个状态 $x$，在通过传感器进行一次观测 $y$ 后，我们希望可以计算出后验分布 $p(x\\mid y)$，即在观测到 $y$ 的情况下，状态 $x$ 的分布。为了计算后验分布的形式，我们可以使用贝叶斯公式：\u003c/p\u003e\n$$\n\\begin{aligned}\np(x\\mid y)\n\u0026= \\frac{p(y\\mid x) p(x)}{p(y)}\\\\\n\u0026= \\frac{p(y\\mid x) p(x)}{\\int p(y\\mid x) p(x) \\mathrm{d}x}\n\\end{aligned}\n$$\u003cp\u003e对于状态估计问题，我们只需要找到使得 $p(x\\mid y)$ 最大的状态 $x$。 由于 $p(y)$ 是一个常数，我们可以将其忽略，只需要最大化分子部分。但是一些应用场景中，还需要我们计算出具体的后验分布的形式。但是这往往非常的难，边缘似然 $p(y)$ 的计算往往是几乎不可能的。因此一个或许可行的办法是，使用一个相对简单的分布 $q(x)$ 代替后验分布。那么原本的贝叶斯问题变成一个变分优化问题，不妨用 KL 散度衡量差异。\u003c/p\u003e\n\u003ch2 id=\"2-kl-散度的不对称性\"\u003e2. KL 散度的不对称性\u003c/h2\u003e\n\u003cp\u003e但是一个很核心的问题是，KL 散度不具有对称性，$D_{KL}(p\\| q) \\neq D_{KL}(q\\| p)$，在实际操作中，我们应该如何选择？不妨先观察这两个对称的 KL 散度的具体形式：\u003c/p\u003e\n$$\n\\begin{aligned}\nD_{KL}(p \\| q)\n\u0026= -\\int p(x\\mid y) \\log \\frac{q(x)}{p(x\\mid y)} \\mathrm{d}x\\\\\n\u0026= -\\int p(x\\mid y) \\log q(x) \\mathrm{d}x + \\int p(x\\mid y) \\log p(x\\mid y) \\mathrm{d}x\n\\end{aligned}\n$$\u003cp\u003e其中，$\\int p(x\\mid y) \\log q(x) \\mathrm{d}x$ 实质上是 $p$ 与 $q$ 之间的交叉熵，不妨定义 $H(p, q) = \\int p(x\\mid y) \\log q(x) \\mathrm{d}x$。而 $\\int p(x\\mid y) \\log p(x\\mid y) \\mathrm{d}x$ 则是关于 $p$ 的熵。$p$ 的熵实质上是一个常数，因此实质上：\u003c/p\u003e","title":"Variational Bayesian Inference"},{"content":"1. 动机和推导 对于无监督的样本生成问题，我们之前已经提到过很多次，对于一系列同类型的数据 $\\{x_1, x_2, \\cdots, x_n\\}$，我们假设存在一个理想的分布 $p(x)$，这些数据都是从这个分布中采样得到的。但是想直接学习这个分布非常难。那么是否有可能通过一个足够复杂的神经网络来近似这个理想分布？此外，有一定神经经验的朋友往往有一个信念：“压缩即智能”。那么是否可以先学习一个编码器 $p(z\\mid x)$ 将原有的随机向量 $x$ 变换到一个低维的潜空间 $z$，然后再通过一个解码器 $p(x\\mid z)$ 将潜空间的向量 $z$ 重新映射回原始空间，从而实现对数据的生成和重构？\n变分自编码器（Variational AutoEncoder）就是这个思路的具体实现。显然，通过解码器重建分布 $p(x)$ 可以描述为：\n$$ p_\\theta(x) = \\int p_\\theta(x\\mid z) p(z) \\, dz $$那么训练 $p_\\theta(x)$ 的一个显然的方法是优化其与真实分布 $p(x)$ 之间的 KL 散度：\n$$ \\theta^* = \\arg \\min_\\theta D_{KL}\\bigg(p(x) \\| p_\\theta(x)\\bigg) $$不妨展开 KL 散度，容易发现：\n$$ \\begin{aligned} \\theta^* \u0026= \\arg \\min_\\theta D_{KL}\\left(p(x) \\| p_\\theta(x)\\right)\\\\ \u0026= \\arg \\min_\\theta \\bigg\\{\\mathbb{E}_{x\\sim p(x)}\\left[\\log p(x) \\right]- \\mathbb{E}_{x\\sim p(x)}\\left[\\log p_\\theta(x)\\right]\\bigg\\} \\end{aligned} $$其中 $\\mathbb{E}_{x\\sim p(x)}\\left[\\log p(x) \\right]$ 实际上是真实分布 $p(x)$ 的熵，不包含可以优化的参数。可以直接扔掉。那么优化目标可以写作：\n$$ \\theta^* = \\arg \\max_\\theta \\mathbb{E}_{x\\sim p(x)}\\left[\\log p_\\theta(x)\\right] $$离散的，对于一批数据 $\\{x_1, x_2, \\cdots, x_n\\}$，我们可以将优化目标改写为：\n$$ \\theta^* = \\arg \\max_\\theta \\frac{1}{n}\\sum_{i=1}^n \\log p_\\theta(x_i) $$但是此时，计算 $\\log p_\\theta(x_i)$ 仍然是一个难点。不妨将 $\\log p_\\theta(x_i)$ 的形式展开，有：\n$$ \\log p_\\theta(x_i) = \\log \\int p_\\theta(x_i, z) \\, dz $$想直接计算这个积分几乎是不可能的，一个显然的思路是 Monto Carlo 积分，将这个积分的形式转换为一个可以对随机变量有效采样的期望的形式，通过不断对随机变量采样，就可以很容易的计算出积分的结果了。\n盯着这个式子，不难发现，既然是对 $z$ 做积分，那么这个期望所依赖的随机变量一定是关于 $z$ 的。此时有两个选择，要么直接使用 $p(z)$，要么使用编码器 $p_\\psi(z\\mid x_i)$。如果选择 $p(z)$，那么上面的形式可以写成：\n$$ \\begin{aligned} \\log p_\\theta(x_i) \u0026= \\log \\int p_\\theta(x_i, z) \\, dz\\\\ \u0026= \\log \\mathbb{E}_{z\\sim p(z)}\\left[p_\\theta(x_i \\mid z)\\right] \\end{aligned} $$这相当于希望训练出一个 “过于强大” 的编码。希望训练出一个从任意的 $z$ 到 $x$ 的映射。我们抛弃了编码器带来的，从数据 $x_i$ 到 $z_i$ 之间的信息。如果直接这样训练，会导致 $z_i$ 到 $x_i$ 的映射是完全随机的，模型无法学习到一个稳定、连续且有意义的映射关系。因此我们选择使用编码器 $p_\\psi(z\\mid x_i)$ 来构造期望，保留一些从数据 $x_i$ 到潜变量 $z$ 的信息。那么上面的形式可以写成：\n$$ \\begin{aligned} \\log p_\\theta(x_i) \u0026= \\log \\int p_\\theta(x_i, z) \\, dz\\\\ \u0026= \\log \\int p_\\psi(z \\mid x_i) \\frac{p_\\theta(x_i, z)}{p_\\psi(z \\mid x_i)} \\, dz\\\\ \u0026= \\log \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)}\\left[\\frac{p_\\theta(x_i, z)}{p_\\psi(z \\mid x_i)}\\right] \\end{aligned} $$我们知道，$\\log$ 是一个凹函数，而期望算子本质上是一个加性操作。那根据 Jensen 不等式，有：\n$$ \\begin{aligned} \\log p_\\theta(x_i) \u0026= \\log \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)}\\left[\\frac{p_\\theta(x_i, z)}{p_\\psi(z \\mid x_i)}\\right]\\\\ \u0026\\geq \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)}\\left[\\log \\frac{p_\\theta(x_i, z)}{p_\\psi(z \\mid x_i)}\\right]\\\\ \u0026= \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)}\\bigg[\\log p_\\theta(x_i \\mid z) + \\log p(z) - \\log p_\\psi(z \\mid x_i)\\bigg]\\\\ \u0026= \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log p_\\theta(x_i \\mid z)\\bigg] - \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log \\frac{p_\\psi(z \\mid x_i)}{p(z)}\\bigg]\\\\ \u0026= \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log p_\\theta(x_i \\mid z)\\bigg] - D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p(z)\\bigg) \\end{aligned} $$这个形式事实上就是 ELBO（Evidence Lower Bound）的形式。对于上面这个形式，我们做这样的理解：\n$$ \\begin{array}{ll} \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log p_\\theta(x_i \\mid z)\\bigg]\u0026 重构项\\\\ D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p(z)\\bigg)\u0026 KL 散度项\\\\ \\end{array} $$2. 工程实践 VAE 假设潜变量层的先验分布 $p(z)$ 是一个高斯分布 $\\mathcal{N}(0, I)$，那么一个合理的假设是编码器 $p_\\psi(z\\mid x)$ 必然很接近于高斯分布，不妨将其建设为一个高斯分布族。此外为了方便处理，不妨同样认定解码器 $p_\\theta(x\\mid z)$ 也是一个高斯分布族。那么在工程上我们可以做这样的处理：\n对于编码器，我们使用一个神经网络，输入数据 $x$，输出高斯分布族 $p(z\\mid x)$ 的均值 $\\mu_\\psi(x)$ 和方差 $\\sigma_\\psi(x)$。 对于解码器，我们使用一个神经网络，输入潜变量 $z$，输出高斯分布族 $p(x\\mid z)$ 的均值 $\\mu_\\theta(z)$ 和方差 $\\sigma_\\theta(z)$。 那么对于第一项，重构项可以写作：\n$$ \\begin{aligned} \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log p_\\theta(x_i \\mid z)\\bigg] \u0026= \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log \\mathcal{N}(x_i \\mid \\mu_\\theta(z), \\sigma_\\theta(z))\\bigg]\\\\ \u0026\\sim - \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log \\sigma_\\theta(z)\\bigg] - \\frac{1}{2}\\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\frac{(x_i - \\mu_\\theta(z))^2}{\\sigma_\\theta(z)^2}\\bigg] \\end{aligned} $$对于 KL 散度项，由于 Gaussian 分布的 KL 散度有一个解析解，因此可以直接写作：\n$$ \\begin{aligned} D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p(z)\\bigg) \u0026= D_{KL}\\bigg(\\mathcal{N}(\\mu_\\psi(x_i), \\sigma_\\psi(x_i)) \\| \\mathcal{N}(0, I)\\bigg)\\\\ \u0026= \\frac{1}{2} \\sum_{j=1}^d \\left( \\sigma_\\psi(x_i)_j^2 + \\mu_\\psi(x_i)_j^2 - 1 - \\log \\sigma_\\psi(x_i)_j^2 \\right) \\end{aligned} $$最终的 Loss 形式可以写作：\n$$ \\begin{aligned} L_{VAE}(x_i) \u0026=\\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log p_\\theta(x_i \\mid z)\\bigg] - D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p(z)\\bigg)\\\\ \u0026\\sim - \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log \\sigma_\\theta(z)\\bigg]- \\frac{1}{2}\\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\frac{(x_i - \\mu_\\theta(z))^2}{\\sigma_\\theta(z)^2}\\bigg] - \\frac{1}{2} \\sum_{j=1}^d \\left( \\sigma_\\ psi(x_i)_j^2 + \\mu_\\psi(x_i)_j^2 - 1 - \\log \\sigma_\\psi(x_i)_j^2 \\right) \\end{aligned} $$对这个形式吗，我们有一个直观的理解：\n$\\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log \\sigma_\\theta(z)\\bigg]$ 是一个正则化项，鼓励模型尽可能的缩小预测出的协方差的大小，让解码器预测出的结果更加确定。 $\\frac{1}{2}\\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\frac{(x_i - \\mu_\\theta(z))^2}{\\sigma_\\theta(z)^2}\\bigg]$ 是一个由不确定性 （$\\sigma$） 加权的重构误差，事实上就是一个 MLE。 $D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p(z)\\bigg)$ 也是一个正则化项，它鼓励模型尽可能的贴近高斯分布。具体为什么要这样我们后续会讨论。 此时，算法流程是这样的：\n从数据集中任选一个样本 $x_i$ 通过编码器，计算出一组潜变量的分布参数 $\\mu_\\psi(x_i), \\sigma_\\psi(x_i)$ 根据 $\\mu_\\psi(x_i), \\sigma_\\psi(x_i)$ 我们可以直接计算出 KL 散度项 同样根据 $\\mu_\\psi(x_i), \\sigma_\\psi(x_i)$，我们获得了期望所依赖的分布 $p_\\psi(z\\mid x) \\sim \\mathcal N(\\mu_\\psi(x_i), \\sigma_\\psi(x_i))$。 对 $\\mathcal N(\\mu_\\psi(x_i), \\sigma_\\psi(x_i))$ 采样，即可计算出重构项 计算 Loss，更新参数 3. 讨论 要理解 VAE 为什么要这样设计，需要先理解此前 AutoEncoder 面对着什么问题:\nAutoEncoder 训练常常面临学习出恒等映射这个问题，即编码器和解码器并没有学习到任何有意义的对数据的压缩和重构，而是简单的将对应关系记忆了下来。 我们希望 Encoder 学习到的浅变量之间是连续、稠密且可插值的，但是模型往往不随人所愿。对于模型而言，最省事的方法是将不同模式之间的浅变量尽可能的拉远避免混淆，不同模式的浅变量之间就出现了很多无意义的空隙，导致差值毫无意义。 为了解决第一个问题，我们并不直接将 Encoder 的输出交给 Decoder 来解码，而是让 Encoder 输出一个分布，之后在这个分布中做一次采样，将采样交给 Decoder 来解码。这样从根本上杜绝了学习恒等映射的可能性。\n更为巧妙的是 VAE 同样很好的解决了第二个问题。我们不妨思考一下，如果去掉 KL 散度项会怎样？事实上模型会天然的将 Latent Space 的协方差渐小，消除不确定性，这样预测的结果会更加简单确定。由此退化为一个经典的 AE。那么此时不同模式的浅变量层之间的空隙会变大，差值效果就会很差。KL 散度项事实上强制保证了 Latent Space 具有足够的随机性（协方差接近 I），同时还保证 Latent Space 分布的均值还得足够 “稠密” 的分布在原点附近。这个方法强迫模型学习到一个连续、稠密且可插值的 Latent Space，从而保证了差值的有效性和学习的效率。时是上，VAE 中的 Variational 正是指的这种通过 KL 散度来正则化潜变量分布的思想。\n4. ELBO 的不等号究竟少了什么？ 对于 ELBO，我们有：\n$$ \\log p_\\theta(x_i) \\geq \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)}\\left[\\log \\frac{p_\\theta(x_i, z)}{p_\\psi(z \\mid x_i)}\\right] $$为了思考在什么时候 $\\geq$ 可以取等号，不妨将上式左右相减：\n$$ \\begin{aligned} \u0026\\log p_\\theta(x_i) - \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)}\\left[\\log \\frac{p_\\theta(x_i, z)}{p_\\psi(z \\mid x_i)}\\right]\\\\ =\u0026\\log p_\\theta(x_i) - \\int p_\\psi(z\\mid x_i) \\bigg[\\log p_\\theta(x_i, z) - \\log p_\\psi(z\\mid x_i)\\bigg] dz\\\\ =\u0026 \\int p_\\psi(z\\mid x_i) \\bigg[\\log p_\\theta(x_i) - \\log p_\\theta(x_i, z) + \\log p_\\psi(z\\mid x_i)\\bigg] dz\\\\ =\u0026 \\int p_\\psi(z\\mid x_i) \\bigg[\\log p_\\theta(x_i) - \\log p_\\theta(z\\mid x_i) - \\log_\\theta(x_i) + \\log p_\\psi(z\\mid x_i)\\bigg] dz\\\\ =\u0026 \\int p_\\psi(z\\mid x_i) \\bigg[\\log p_\\psi(z\\mid x_i) - \\log p_\\theta(z\\mid x_i)\\bigg] dz\\\\ =\u0026 D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p_\\theta(z \\mid x_i)\\bigg) \\end{aligned} $$那么我们获得了一个关于 “升级版” 的 ELBO：\n$$ \\begin{aligned} \u0026\\log p_\\theta(x_i)\\\\ =\u0026 \\mathbb{E}_{z\\sim p_\\psi(z\\mid x_i)} \\bigg[\\log p_\\theta(x_i \\mid z)\\bigg] - D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p(z)\\bigg) + D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p_\\theta(z \\mid x_i)\\bigg) \\end{aligned} $$显然，KL 散度项总是非负的，因此 ELBO 的不等号成立。并且当且仅当：\n$$ D_{KL}\\bigg(p_\\psi(z \\mid x_i) \\| p_\\theta(z \\mid x_i)\\bigg) = 0 $$时，等号成立。换句话说，当我们的解码器可以完美的还原编码器时，换句话说，我们学习到了一个完美的生成模型时，或者说我们的模型越好，越接近等号。\nReferences ","permalink":"https://wangjv0812.cn/2025/10/variational-autoencoder/","summary":"\u003ch2 id=\"1-动机和推导\"\u003e1. 动机和推导\u003c/h2\u003e\n\u003cp\u003e对于无监督的样本生成问题，我们之前已经提到过很多次，对于一系列同类型的数据 $\\{x_1, x_2, \\cdots, x_n\\}$，我们假设存在一个理想的分布 $p(x)$，这些数据都是从这个分布中采样得到的。但是想直接学习这个分布非常难。那么是否有可能通过一个足够复杂的神经网络来近似这个理想分布？此外，有一定神经经验的朋友往往有一个信念：“\u003cem\u003e压缩即智能\u003c/em\u003e”。那么是否可以先学习一个编码器 $p(z\\mid x)$ 将原有的随机向量 $x$ 变换到一个低维的潜空间 $z$，然后再通过一个解码器 $p(x\\mid z)$ 将潜空间的向量 $z$ 重新映射回原始空间，从而实现对数据的生成和重构？\u003c/p\u003e\n\u003cp\u003e变分自编码器（Variational AutoEncoder）就是这个思路的具体实现。显然，通过解码器重建分布 $p(x)$ 可以描述为：\u003c/p\u003e\n$$\np_\\theta(x) = \\int p_\\theta(x\\mid z) p(z) \\, dz\n$$\u003cp\u003e那么训练 $p_\\theta(x)$ 的一个显然的方法是优化其与真实分布 $p(x)$ 之间的 KL 散度：\u003c/p\u003e\n$$\n\\theta^* = \\arg \\min_\\theta D_{KL}\\bigg(p(x) \\| p_\\theta(x)\\bigg)\n$$\u003cp\u003e不妨展开 KL 散度，容易发现：\u003c/p\u003e\n$$\n\\begin{aligned}\n\\theta^*\n\u0026= \\arg \\min_\\theta D_{KL}\\left(p(x) \\| p_\\theta(x)\\right)\\\\\n\u0026= \\arg \\min_\\theta \\bigg\\{\\mathbb{E}_{x\\sim p(x)}\\left[\\log p(x) \\right]- \\mathbb{E}_{x\\sim p(x)}\\left[\\log p_\\theta(x)\\right]\\bigg\\}\n\\end{aligned}\n$$\u003cp\u003e其中 $\\mathbb{E}_{x\\sim p(x)}\\left[\\log p(x) \\right]$ 实际上是真实分布 $p(x)$ 的熵，不包含可以优化的参数。可以直接扔掉。那么优化目标可以写作：\u003c/p\u003e\n$$\n\\theta^* = \\arg \\max_\\theta \\mathbb{E}_{x\\sim p(x)}\\left[\\log p_\\theta(x)\\right]\n$$\u003cp\u003e离散的，对于一批数据 $\\{x_1, x_2, \\cdots, x_n\\}$，我们可以将优化目标改写为：\u003c/p\u003e","title":"Variational AutoEncoder"},{"content":"1. 动机 以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 NCE 中对于数据流形 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。\n我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\\theta$ 控制的概率分布 $q(x\\mid \\theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：\n我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\\{x_1, x_2, \\cdots x_n\\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\\theta$，使得 $q(x\\mid \\theta)$ 尽可能接近 $p(x)$。 对于一个完成学习的分布 $p(x\\mid \\theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。 这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 Score Matching 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。\n1.1. Denoising AutoEncoder AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。\n为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。\n形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\\epsilon \\sim \\mathcal N(x\\mid 0, \\sigma^2I)$，有被污染的数据：\n$$ \\tilde{x} = x + \\epsilon $$那么，损失可以写作：\n$$ J_{DATA}(\\theta) = \\mathbb{E}\\left(\\left\\| \\text{Decoder}(\\text{Encoder} (x + \\epsilon)) - x \\right\\|^2\\right) $$1.2. Score Matching 我们之前在 Score Matching 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 Fisher Divergence。\n$$ J_{\\text{ESM}}(\\theta) = \\frac{1}{2} \\mathbb{E}_{x\\sim p(x)}\\left(\\left\\| \\nabla_x \\log p(x) - \\nabla_x \\log q(x\\mid \\theta) \\right\\|^2\\right) $$但是问题在于，我们不知道真实分布的解析形式，有的只是对真实分布的采样，没法计算 Fisher Divergence。Divergence。Hyvärinen 为了解决这个问题，证明了在温和的条件下，Fisher Divergence 可以等价转换为：\n$$ J_{\\text{ISM}}(\\theta) = \\mathbb{E}_{x\\sim p(x)}\\left(\\Delta_x \\log q(x\\mid \\theta) + \\frac{1}{2}\\left\\| \\nabla_x \\log q(x\\mid \\theta) \\right\\|^2\\right) $$在对真实分布 $p(x)$ 有限的采样 $\\{x_1, x_2, \\cdots x_n\\}$，上面的的期望可以通过有限采样的均值来近似：\n$$ J_{ISM}(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left(\\Delta_x \\log q(x_i\\mid \\theta) + \\frac{1}{2}\\left\\| \\nabla_x \\log q(x_i\\mid \\theta) \\right\\|^2\\right) $$将 Score Function 记作 $\\psi(x, \\theta) = \\nabla_x \\log q(x\\mid \\theta)$，上面的形式可以写作：\n$$ J_{ISM}(\\theta) \\approx \\frac{1}{n} \\sum_{t=1}^n \\left( \\sum_{i=0}^d \\frac{\\partial \\psi(x^t, \\theta)}{\\partial x_i} + \\frac{1}{2}\\left\\| \\psi(x^t, \\theta) \\right\\|^2\\right) $$在计算上，$\\frac{\\partial \\psi(x^t, \\theta)}{\\partial x_i}$ 即不具备数值稳定性，也不具备计算效率。为了解决稳定问题，LeCun 提出在 ISM 的基础上添加一个关于这一项的正则化项：\n$$ J_{ISMreq}(\\theta) = J_{ISM}(\\theta) + \\lambda \\sum_{i=1}^d \\left(\\frac{\\partial \\psi(x, \\theta)}{\\partial x_i}\\right)^2 $$显然，添加正则化后，可以有效的让 score function 在几何上更加 “平缓”，避免学习到很极端和崎岖的 score function，从而提升数值稳定性。\n2. DSM 和 DAE 之间的联系 2.1. DSM（Denoising Score Matching） 对于通过一系列采样描述的数据 $\\{x_1, x_2, \\cdots, x_n\\}$，我们不妨用 Parzen Windows Density Estimation 来对数据分布进行平滑，给数据一个便于分析的解析形式。关于 Parzen Density Estimator 的内容可以参考 Appendix A。\n不妨选择受带宽参数 $\\sigma$ 控制的高斯函数作为核函数：\n$$ q_\\sigma(\\tilde{x} \\mid x) = \\frac{1}{(\\sigma \\sqrt{2\\pi})^d} \\exp\\left(-\\frac{\\|\\tilde{x} - x\\|^2}{2\\sigma^2}\\right) $$经过 Parzen Density Estimator 重建的数据分布为：\n$$ q_\\sigma(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n q_\\sigma(\\tilde{x} \\mid x_i) $$那么，Parzen Density Estimation 重建后进行的显式分数匹配目标函数为：\n$$ J_{ESM_{\\sigma}}(\\theta) = \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] $$阅读 Appendix A 可以发现，Parzen Density Estimation 事实上是对数据添加了高斯噪声。对于上面的形式，我们不妨展开，看看高斯噪声对数据的污染对 Score Function 学习产生了什么影响？\n2.2. DSM Loss 等价性证明 对于 $J_{ESM_{\\sigma}}(\\theta)$，不妨将二范数的形式展开：\n$$ \\begin{aligned} J_{ESM_{\\sigma}}(\\theta) \u0026= \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] \\\\ \u0026= \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\frac{1}{2} \\left( \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2 - 2\\left \\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle + \\left\\| \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right) \\right] \\\\ \u0026= \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2\\right] - \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle\\right] + \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[\\left\\| \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right] \\end{aligned} $$显然，其中 $\\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[\\left\\| \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right\\|^2 \\right]$ 与待优化参数 $\\theta$ 无关，可以忽略。第一项只包含模型，可以直接保留。关键在于第二项，不妨将其标注为 $S(\\theta)$。\n$$ \\begin{aligned} S(\\theta) \u0026= \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle\\right]\\\\ \u0026= \\int q_{\\sigma}(\\tilde{x}) \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle d\\tilde{x}\\\\ \\end{aligned} $$由求导法则 $\\frac{\\partial }{\\partial \\tilde x} \\log q_\\sigma(\\tilde x) = \\frac{1}{q_\\sigma(\\tilde x)} \\frac{\\partial}{\\partial \\tilde x}q_\\sigma(\\tilde x)$ 可知：\n$$ \\begin{aligned} S(\\theta) \u0026= \\int q_{\\sigma}(\\tilde{x}) \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial \\log q_{\\sigma}(\\tilde{x})}{\\partial \\tilde{x}} \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x}q_\\sigma(\\tilde x) \\right \\rangle d\\tilde{x}\\\\ \\end{aligned} $$将 Parzen Density Estimation 的边缘分布形式带入，并根据莱布尼茨定理交换顺序：\n$$ \\begin{aligned} S(\\theta) \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x}q_\\sigma(\\tilde x) \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x}\\int_x p(\\tilde x \\mid x) q(x) dx \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\int_x q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\right \\rangle d\\tilde{x}\\\\ \\end{aligned} $$不妨展开内积符号，有：\n$$ \\begin{aligned} S(\\theta) \u0026= \\int_{\\tilde x} \\left\\langle \\psi(\\tilde{x}; \\theta), \\int_x q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\right \\rangle d\\tilde{x}\\\\ \u0026= \\int_{\\tilde x} \\sum_{i=1}^d \\psi^{\\top}_i(\\tilde{x}; \\theta) \\int_x q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\bigg |_i d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\sum_{i=1}^d \\int_x \\psi^{\\top}_i(\\tilde{x}; \\theta) q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\bigg |_i d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\int_x \\sum_{i=1}^d \\psi^{\\top}_i(\\tilde{x}; \\theta) q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) dx \\bigg |_i d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\int_x \\left\\langle \\psi(\\tilde{x}; \\theta), q(x) \\frac{\\partial}{\\partial \\tilde x} p(\\tilde x \\mid x) \\right \\rangle dx \\ d\\tilde x\\\\ \u0026= \\int_{\\tilde x} \\int_x q(x) p(\\tilde x \\mid x) \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x) \\right \\rangle dx \\ d\\tilde x\\\\ \u0026= \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle\\right] \\end{aligned} $$整合形式，有：\n$$ J_{ESM_{\\sigma}}(\\theta) = \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2\\right] - \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle\\right] + C $$只需添加一个与待优化参数无关的量：\n$$ C = \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\| \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\|^2\\right] $$有：\n$$ \\begin{aligned} J_{ESM_{\\sigma}}(\\theta) \u0026= \\frac 1 2 \\mathbb{E}_{q_{\\sigma}(\\tilde{x})}\\left[ \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2\\right] - \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle\\right] + \\mathbb{E}_{q(x, \\tilde x)} \\left[\\left\\| \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\|^2\\right]+ C \\\\ \u0026= \\mathbb{E}_{q(x, \\tilde x)} \\left[ \\frac 1 2 \\left\\| \\psi(\\tilde{x}; \\theta) \\right\\|^2 - \\left\\langle \\psi(\\tilde{x}; \\theta), \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\rangle + \\left\\| \\frac{\\partial}{\\partial \\tilde x} \\log p(\\tilde x \\mid x)\\right \\|^2\\right] + C\\\\ \u0026= \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}} \\right\\|^2 \\right]\\\\ \u0026= J_{DSMq}(\\theta) \\end{aligned} $$其中，$\\psi(\\tilde{x}; \\theta)$ 是我们需要学习的 Score Function。有趣的事，展开 $\\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}}$ 不难发现：\n$$ \\begin{aligned} \\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}} \u0026= \\frac 1 {\\sigma^2} (x - \\tilde{x})\\\\ \\end{aligned} $$Score Function 实际上跟踪的是我们添加噪声的反方向。换句话说，我们是在 “降噪”。从这里已经可以看到一些 Diffusion 的影子了。\n2.3. DSM 和 DAE 之间的关系 不妨先简单回顾一下，此处对于被噪声污染了的数据对 $\\{\\tilde x, x\\}$，DAE 的 Loss Funciton 可以写作:\n$$ J_{DAE}(\\theta) = \\mathbb{E}_{q(x), q_\\sigma(\\tilde{x}|x)}\\left[ \\left\\| r(\\tilde{x}, \\theta) - x \\right\\|^2 \\right] $$其中 $r(\\tilde{x}, \\theta)$ 是 DAE 的重建函数。根据上一节中的证明 Score Function 可以定义为：\n$$ \\psi(x, \\theta) = \\frac {r(\\tilde x, \\theta) - \\tilde{x}}{\\sigma^2} $$将这个形式带入 $J_{DSM}$ 中，不难发现：\n$$ \\begin{aligned} J_{DSM}(\\theta) \u0026= \\mathbb{E}_{q(x), q_\\sigma(\\tilde{x}|x)}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{x - \\tilde{x}}{\\sigma^2} \\right\\|^2 \\right]\\\\ \u0026= \\mathbb{E}_{q(x), q_\\sigma(\\tilde{x}|x)}\\left[ \\frac{1}{2\\sigma^4} \\left\\| r(\\tilde x, \\theta) - x \\right\\|^2 \\right]\\\\ \u0026= \\frac{1}{2\\sigma^4} J_{DAE}(\\theta) \\end{aligned} $$这与 DAE 的 Loss Function 只差一个常数倍数。\n3. Discussion Score Matching 在 Denoising Score Matching 之前，可以说是一个 “屠龙术”。确实，Score Matching 很好的解决了归一化问题。但是依然有很多缺陷：\nScore Matching 的 Loss Function 需要计算二阶导数，对于高维数据计算量巨大，且数值上不稳定。 Score Matching 只对数据采样点附近的信息梯度建模，导致 Langevin Annealing 的过程中很容易陷入局部最优。 Denoising Score Matching 很好的解决了这些问题。首先，DSM 通过添加噪声的方式，事实上建立了一条从噪声到数据的通路，我们的模型事实上见过从噪声重建数据的每一步操作，让模型的采样更加鲁棒。更大的好处是，在实际操作中，我们使用的 Loss 是：\n$$ \\begin{aligned} J_{DSMq}(\\theta) \u0026= \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\partial \\log q_{\\sigma}(\\tilde{x}|x)}{\\partial \\tilde{x}} \\right\\|^2 \\right]\\\\ \u0026= \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{1}{\\sigma^2} (x - \\tilde{x}) \\right\\|^2 \\right] \\end{aligned} $$假设对于数据 $x$，我们添加了噪声 $\\epsilon(\\tilde x)$，那么对于通过神经网络描述的 Score Function $\\psi(\\tilde x; \\theta)$，我们只需要最小化：\n$$ \\boxed{ J_{DSMq}(\\theta) = \\mathbb{E}_{q_{\\sigma}(x, \\tilde{x})}\\left[ \\frac{1}{2} \\left\\| \\psi(\\tilde{x}; \\theta) - \\frac{\\epsilon(\\tilde x)}{\\sigma^2} \\right\\|^2 \\right] } $$事实上将一个复杂的无监督学习问题，转化为一个有监督学习问题，并直接使用一个简单的 ESM 作为 Loss Function。不论是代码还是训练难度，都简单了非常多。在实践中，使用 Pytorch，实现非常简单：\ndef denoising_score_matching_loss(self, x, sigma=0.01): batch_size = x.shape[0] # 添加噪声 noise = torch.randn_like(x) * sigma x_noisy = x + noise x_noisy.requires_grad_(True) # 计算 score score = self.score_network(x_noisy) # Target: -noise / sigma^2 (真实的去噪方向) target = -noise / (sigma ** 2) # 使用加权的 MSE Loss，避免小噪声时 loss 爆炸 weight = sigma ** 2 # 权重与 sigma^2 成正比，平衡不同噪声水平的损失 loss = weight * F.mse_loss(score, target) return loss 并且，从 DSM，已经可以看到 Diffusion 的雏形了。这篇文章也开启了后世扩散模型大发展的大门。\nAppendix A. Parzen Density Estimator 对于一个理想但是未知的分布 $p(x)$，我们对齐认识只有一系列采样点 $\\{x_1, x_2, \\cdots ,x_n\\}$。一个简单的甚至有些暴力的重建其密度的方法是：给每个数据点上固定一个概率分布（我们称之为重建核，常选高斯分布）,之后将所有的分布加起来统一归一化。就可以得到一个平滑的、连续的概率分布。Parzen Density Estimation 可以看作对直方图发的一种直觉的改进。当我们选择如下的高斯重建核：\n$$ p(\\tilde x \\mid x) = \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) $$那么 Parzen Density Estimation 重建的概率分布为：\n$$ p(\\tilde{x}) = \\frac{1}{n} \\sum_{i=1}^n p(\\tilde x \\mid x_i) = \\frac{1}{n} \\sum_{i=1}^n \\mathcal N(\\tilde x \\mid x_i, \\sigma^2 I) $$其中，高斯分布的协方差 $\\sigma$ 控制了重建分布的平滑程度。$\\sigma$ 越大，重建分布越平滑，$\\sigma$ 越小，重建分布越接近原始数据的离散采样。这里称之为 “带宽”。\n事实上可以通过一个更形式话的方式来定力 Parzen Density Estimation。对于一系列数据 $\\{x_1, x_2, \\cdots ,x_n\\}$，我们可以建立起其经验分布：\n$$ q(x) = \\frac{1}{n}\\sum_{i=1}^n \\delta(x - x_i) $$对于重建核，我们可以将其写成条件的形式：\n$$ p(\\tilde x \\mid x) = \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) $$Parzen Density Estimation 可以用边缘分布重写：\n$$ \\begin{aligned} p(\\tilde x) \u0026= \\int p(\\tilde x \\mid x) q(x) dx\\\\ \u0026= \\int \\sum_{i=1}^n \\delta(x - x_i) \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) dx\\\\ \u0026= \\sum_{i=1}^n \\int \\delta(x - x_i) \\mathcal N(\\tilde x \\mid x, \\sigma^2 I) dx\\\\ \u0026= \\frac{1}{n} \\sum_{i=1}^n \\mathcal N(\\tilde x \\mid x_i, \\sigma^2 I) \\end{aligned} $$这个形式与直接的 Parzen Density Estimation 是等价的。这个理解在 DAE 中至关重要。\nReference [A connection between score matching and denoising autoencoders](@article{vincent2011connection, title={}, author={Vincent, Pascal}, journal={Neural computation}, volume={23}, number={7}, pages={1661\u0026ndash;1674}, year={2011}, publisher={MIT Press} })\n","permalink":"https://wangjv0812.cn/2025/10/denoising-score-matching/","summary":"\u003ch2 id=\"1-动机\"\u003e1. 动机\u003c/h2\u003e\n\u003cp\u003e以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 \u003ca href=\"https://wangjv0812.cn/2025/08/noise-contrastive-estimation/\"\u003eNCE 中对于数据流形\u003c/a\u003e 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。\u003c/p\u003e\n\u003cp\u003e我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\\theta$ 控制的概率分布 $q(x\\mid \\theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\\{x_1, x_2, \\cdots x_n\\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\\theta$，使得 $q(x\\mid \\theta)$ 尽可能接近 $p(x)$。\u003c/li\u003e\n\u003cli\u003e对于一个完成学习的分布 $p(x\\mid \\theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 \u003ca href=\"https://wangjv0812.cn/2025/08/scorematching/\"\u003eScore Matching\u003c/a\u003e 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。\u003c/p\u003e\n\u003ch3 id=\"11-denoising-autoencoder\"\u003e1.1. Denoising AutoEncoder\u003c/h3\u003e\n\u003cp\u003eAutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。\u003c/p\u003e\n\u003cp\u003e为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。\u003c/p\u003e\n\u003cp\u003e形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\\epsilon \\sim \\mathcal N(x\\mid 0, \\sigma^2I)$，有被污染的数据：\u003c/p\u003e\n$$\n\\tilde{x} = x + \\epsilon\n$$\u003cp\u003e那么，损失可以写作：\u003c/p\u003e\n$$\nJ_{DATA}(\\theta) = \\mathbb{E}\\left(\\left\\|\n\\text{Decoder}(\\text{Encoder} (x + \\epsilon)) - x\n\\right\\|^2\\right)\n$$\u003ch3 id=\"12-score-matching\"\u003e1.2. Score Matching\u003c/h3\u003e\n\u003cp\u003e我们之前在 \u003ca href=\"https://wangjv0812.cn/2025/08/scorematching/\"\u003eScore Matching\u003c/a\u003e 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 \u003ca href=\"https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/\"\u003eFisher Divergence\u003c/a\u003e。\u003c/p\u003e","title":"Denoising Score Matching"},{"content":"在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。\nFisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。 Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。 下面则给出一些不那么直观的，数学形式上的解释。\n1. Statistical Manifold 和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\\sigma^2, \\mu$ 控制。那么所有的高斯分布的参数 $\\sigma^2, \\mu$ 所构成的空间便形成一个 “统计流形”。\n那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。\n2. Score Function 对于一个数据集 $\\{x_1, x_2, \\cdots, x_n\\}$，我们假设其服从于一个理想的概率分布 $p(x)$，我们不知道 $p(x)$ 的具体形式，只知道它的一系列采样（就是数据集）。我们希望可以通过一个受到参数 $q(x, \\theta)$ 控制的分布族来近似它。那么一个很容易想到办法是，通过 KL 散度来衡量 $p(x)$ 和 $q(x, \\theta)$ 之间的差异：\n$$ \\begin{aligned} D_{KL}(p(x) \\| q(x, \\theta)) \u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log p(x) -\\log q(x, \\theta)\\bigg]\\\\ \u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log p(x)\\bigg] - \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log q(x, \\theta)\\bigg]\\\\ \\end{aligned} $$其中 $\\mathbb{E}_{x\\sim p(x)} \\bigg[\\log p(x)\\bigg]$ 是 $p(x)$ 的熵，与待优化参数 $\\theta$ 无关，因此：\n$$ D_{KL}(p(x) \\| q(x, \\theta)) \\sim \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log q(x, \\theta)\\bigg] $$在优化中，我们需要求 $D_{KL}(p(x) \\| q(x, \\theta))$ 对参数 $\\theta$ 的梯度，那么有：\n$$ \\begin{aligned} \\frac{\\partial D_{KL}(p \\| q)}{\\partial \\theta} \u0026= \\frac{\\partial}{\\partial \\theta}\\mathbb{E}_{x\\sim p(x)} \\bigg[\\log q(x, \\theta)\\bigg]\\\\ \u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[\\frac{\\partial}{\\partial \\theta} \\log q(x, \\theta)\\bigg]\\\\ \\end{aligned} $$其中的 $\\frac{\\partial}{\\partial \\theta} \\log q(x, \\theta)$ 就是 score function，我们不妨定义：\n$$ \\begin{gather} s_\\theta(x, \\theta) = \\nabla_\\theta \\log q(x; \\theta)\\\\ s_x(x, \\theta) = \\nabla_x \\log q(x; \\theta)\\\\ \\end{gather} $$对于 score function，我们可以从两个角度理解它。\n首先，直观的、几何的讲，对于 score function $s_x(x, \\theta)$ 可以理解为定义在数据空间上的切向量场。不妨想象一下，概率密度 $q(x, \\theta)$ 在数据空间中形成了一座 “高山”，向量 $s_x(x, \\theta)$ 方向指向的是概率密度对数增长最快的方向。$s_x(x, \\theta)$ 告诉我们数据点向哪个方向 ”移动“，概率变大的最快。类似的，$s_\\theta(x, \\theta)$ 则是在参数空间中的切向量场，指向的是关于参数 $\\theta$ 的概率密度对数增长最快的方向。\n但是这个直观的几何理解并没有解释为什么其中包含一个 $\\log$ 的形式，为什么一定是 $\\nabla \\log$ 而非 $\\nabla$。如果你的数学直觉比较强的化可能已经意识到了，这关联到了对于一个概率分布 信息 的衡量。但是如果想要深刻的理解这个问题，涉及到 信息几何 和 微分几何方面的知识。我会尽量在工科数学的范围内给出一些直观的解释。我们不妨先思考一下，直接使用 $\\nabla_xq(x)$ 为什么不够好？不妨思考一个简单的 高斯分布。我们分别在高概率位置 $x_1$ 和 低概率位置 $x_2$ 采样，计算 $\\nabla_\\theta q(x_1)$ 和 $\\nabla_\\theta q(x_2)$。那么自然的：\n对于 $x_1，q(x_1)$ 很大，它的梯度 $\\nabla_\\theta q(x_1, \\theta)$ 可能也比较大。 对于 $x_2，q(x_2)$ 几乎为零，它的梯度 $\\nabla_\\theta q(x_2, \\theta)$ 也几乎为零。 这导致尾部的、意外的、低概率的数据点 $x_2$ 对模型参数 $\\theta$ 几乎没有贡献。但是这是完全违背了统计直觉。从信息论角度分析，意外的、小概率的事件包含更多的信息，它们应该对参数调整产生更大的影响才对。信息几何告诉我们，由参数 $\\theta$ 定义的概率分布族（例如，所有可能的高斯分布）构成了一个空间，但这不是一个平坦的欧几里得空间，而是一个弯曲的统计流形。此时如果直接使用欧式几何的距离定义并不能争取的反应在参数流形上的距离关系。我们不加证明的给出结论，通过 \u0026ldquo;信息化转化算子\u0026rdquo; $\\log$，抵消了概率变化导致的对参数空间的 “扭曲” 影响。\n简而言之，从信息几何的角度出发，$\\log$ 的加入将原本非平直的参数空间 “压平” 了。这样让在参数空间上直接通过类似于欧氏距离的方式度量分布成为了可能。$\\log$ 是我们对抗参数空间内在扭曲性、将其“拉平”以便于我们理解和优化的第一步，也是最关键的一步。\n3. Fisher Information Matrix 我们已经理解了 Score Function，Fisher Information Matrix 则是定义在参数 Score Function $s_\\theta(x, \\theta)$ 的协方差矩阵：\n$$ \\begin{gather} I(\\theta) = \\mathbb{E}_{x\\sim q(x, \\theta)}\\big[ s_\\theta(x, \\theta) s_\\theta(x, \\theta)^\\top \\big] \\end{gather} $$但是这个形式不够直观，我们不妨将其变为一个更加常见的形式。但是展具体的证明前，需要先给出一个引理的证明：\nLemma 1. $\\mathbb E_{x\\sim q(x, \\theta)}\\big[s_\\theta(x, \\theta)\\big] = 0$\n有:\n$$ \\begin{aligned} \u0026\\nabla_\\theta \\log q(x, \\theta) = \\frac{\\nabla_\\theta q(x, \\theta)}{q(x, \\theta)}\\\\ \\longrightarrow \u0026 q(x, \\theta) \\nabla_\\theta \\log q(x, \\theta) = \\nabla_\\theta q(x, \\theta)\\\\ \\end{aligned} $$可以推出：\n$$ \\begin{gather} \\begin{aligned} \\mathbb E_{x\\sim q(x, \\theta)}\\big[s_\\theta(x, \\theta)] \u0026= \\int q(x, \\theta) \\nabla_\\theta \\log q(x, \\theta) dx\\\\ \u0026= \\int \\nabla_\\theta q(x, \\theta) dx\\\\ \\end{aligned} \\end{gather} $$在温和的条件下，可以交换 $\\nabla$ 和 $\\int$，有：\n$$ \\begin{gather} \\begin{aligned} \\mathbb E_{x\\sim q(x, \\theta)}\\big[s_\\theta(x, \\theta)] \u0026= \\nabla_\\theta \\int q(x, \\theta) dx\\\\ \u0026= \\nabla_\\theta 1\\\\ \u0026= 0 \\end{aligned} \\end{gather} $$那么从 Lemma 1. 出发，等式两边增加梯度算子 $\\nabla_\\theta$：\n$$ \\begin{gather} \\int q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\nabla_\\theta \\int q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\int \\nabla_\\theta \\bigg(q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) \\bigg)dx = 0\\\\ \\int \\nabla_\\theta q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) + q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\int \\nabla_\\theta q(x, \\theta)\\nabla_\\theta \\log q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) + q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\end{gather} $$分配积分符号，有：\n$$ \\begin{gather} \\int \\nabla_\\theta q(x, \\theta)\\nabla_\\theta \\log q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx + \\int q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = 0\\\\ \\end{gather} $$其中有：\n$$ \\begin{gather} \\int \\nabla_\\theta q(x, \\theta)\\nabla_\\theta \\log q(x, \\theta) \\nabla_\\theta^\\top \\log q(x, \\theta) dx = \\mathbb{E}_{x\\sim q(x, \\theta)}(s(x, \\theta)s(x, \\theta)^\\top)\\\\ \\int q(x, \\theta) \\nabla_\\theta \\nabla_\\theta^\\top \\log q(x, \\theta) dx = \\mathbb{E}_{x\\sim q(x, \\theta)}\\big[\\nabla^2_\\theta \\log q(x, \\theta) \\big] \\end{gather} $$证毕。有结论：\n$$ \\begin{gather}\\boxed{ \\begin{aligned} I(\\theta) \u0026= \\mathbb{E}_{x\\sim q(x, \\theta)}\\bigg[s_\\theta(x, \\theta)s_\\theta(x, \\theta)^\\top\\bigg]\\\\ \u0026= \\mathbb{E}_{x\\sim q(x, \\theta)}\\bigg[\\nabla^2_\\theta \\log q(x, \\theta) \\bigg] \\end{aligned} }\\end{gather} $$我们知道，Fisher Information Matrix 本质上是概率分布 $q(x, \\theta)$ 对参数 $\\theta$ 的 Hessian 矩阵。对于有一定的场论基础的同学肯定已经意识到了，Hessian 矩阵衡量了一个标量场在 $\\theta$ 处的曲率。我们不妨分类讨论：\n对于对角线上的元素： $$ I(\\theta)_{ii} = \\mathbb{E}_{x\\sim q(x, \\theta)} \\bigg[ \\frac{\\partial^2 }{\\partial \\theta_{ii}} \\log q(x, \\theta) \\bigg] $$这代表了对数似然函数在 $\\theta_i$ 这个参数轴方向上的平均曲率。如果 $I(\\theta)_{ii}$ 很大，说明对数函数（信息函数）$\\log q(x, \\theta)$ 在 $\\theta_{i}$ 方向上是很陡峭的。这意味着只要 $\\theta_i$ 发生一丁点变化，$\\log q(x, \\theta)$ 都会发生很大的变化。这说明数据对 $\\theta_i$ 的约束是很强的，我们可以很精确的估计出 $\\theta_i$。该参数的估计方差会很小\n非对角线的元素： $$ I(\\theta)_{ij} = \\mathbb{E}_{x\\sim q(x, \\theta)} \\bigg[ \\frac{\\partial^2 }{\\partial \\theta_i \\partial \\theta_j} \\log q(x, \\theta) \\bigg] $$这代表了 $\\theta_i 和 \\theta_j$ 之间发生 “混淆的可能性”。几何的讲，$I(\\theta)_{ij}$ 代表了在局部，当 $\\theta_j$ 方向移动时，对数似然函数在 $\\theta_i$ 方向上的斜率是如何变化的。\n如果 $I(\\theta)_{ij} = 0$，这是很理想的情况，此时 $\\theta_i$ 和 $\\theta_j$ 是相互独立的，改变 $\\theta_j$ 的值，不会影响对 $\\theta_i$ 的最优估计 如果 $I(\\theta)_{ij} \\neq 0$，这意味着 $\\theta_i$ 和 $\\theta_j$ 之间是存在混淆的。这代表如果我们修正了 $\\theta_i$ 的值，$\\theta_j$ 如果不变化的话，此时已经不是最优了。如果 $I(\\theta)_{ij} \u003e 0$，说明 $\\theta_i$ 和 $\\theta_j$ 是负相关的，如果增加了 $\\theta_i$，需要适当的减小 $\\theta_j$ 来达到最优。相反，如果 $I(\\theta)_{ij} \u003c 0$，说明 $\\theta_i$ 和 $\\theta_j$ 是正相关的，如果增加了 $\\theta_i$，需要适当的增加 $\\theta_j$ 来达到最优。 总而言之，$I(\\theta)$ 是定义在参数流形上的一个度量张量。它是一个局部量，在特定参数点 $\\hat{I(\\theta)}$ 上，衡量了整个概率分布 $p(x, \\theta)$ 对于参数 $\\theta$ 无穷小变化的敏感程度。\n3.1. 信息几何角度理解 下面我们给出一些更高视角下关于 Fisher Information Matrix 的思考。你会惊讶的发现，和上面的几何的观点不谋而合。\n对于一个统计流形（Statistical Manifold），对于其上两个十分接近的分布 $p(x\\mid \\theta)$ 和 $p(x\\mid \\theta + d\\theta)$，不妨用 KL 散度评估两个分布之间的 “距离”。那么有：\n$$ \\begin{gather} \\begin{aligned} D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta)) \u0026= \\int p(x\\mid \\theta) \\log \\frac{p(x\\mid \\theta)}{p(x\\mid \\theta + d\\theta)} dx\\\\ \\end{aligned} \\end{gather} $$对于分布 $\\log p(x\\mid \\theta + d\\theta)$，我们不妨在 $\\theta$ 处做泰勒展开：\n$$ \\log p(x; \\theta + d\\theta) \\approx \\log p(x; \\theta) + d\\theta^T \\nabla_{\\theta} \\log p(x; \\theta) + \\frac{1}{2} d\\theta^T H_{\\theta}(\\log p(x; \\theta)) d\\theta $$将上式带入 KL Divergence 中，有：\n$$ \\begin{gather} \\begin{aligned} D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta)) \u0026= \\int p(x\\mid \\theta) \\bigg[ -d\\theta^T \\nabla_{\\theta} \\log p(x; \\theta) - \\frac{1}{2} d\\theta^T H_{\\theta}(\\log p(x; \\theta)) d\\theta\\bigg] \\end{aligned} \\end{gather} $$不妨重新分配各项，有：\n$$ \\begin{gather} \\begin{aligned} \u0026D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta))\\\\ \u0026= -d\\theta^T \\int p(x\\mid \\theta) \\bigg[\\nabla_{\\theta} \\log p(x; \\theta)\\bigg]d\\theta - \\frac{1}{2} d\\theta^T \\int p(x\\mid \\theta) \\bigg[H_{\\theta}(\\log p(x; \\theta))\\bigg] d\\theta \\end{aligned} \\end{gather} $$对于第一项 $\\int p(x\\mid \\theta) \\bigg[\\nabla_{\\theta} \\log p(x; \\theta)\\bigg]d\\theta$，不难注意到，$\\nabla_{\\theta} \\log p(x; \\theta)$ 是 Score Function，而 Score Function 的期望为零（见 Lemma 1.）。因此第一项为零。\n对于第二项，有 Fisher Information Matrix $I(\\theta) = \\int p(x\\mid \\theta) \\bigg[H_{\\theta}(\\log p(x; \\theta))\\bigg] d\\theta$。\n我们可以总结出一个有趣的结论。对于 Statistical Manifold 上的一个基于 KL 散度的度量无穷小量 $ds^2 = \\lim_{d\\theta \\to 0} D_{KL}(p(x\\mid \\theta) \\mid p(x\\mid \\theta + d\\theta))$，有：\n$$ \\begin{gather} ds^2 = \\frac{1}{2}d\\theta^\\top I(\\theta) d\\theta \\end{gather} $$这个结论告诉我们，统计流形是一个非平直的空间，上式告诉我们，在统计流形这样一个非平直空间上上走一小步 $ds$，那么在平直空间 $d\\theta$ 上，我们走了 $I(\\theta)$\n换一个更形式化的语言，对于两个无限接近的概率分布，它们之间的KL散度（信息论上的差异度量）在二阶近似下，等于由费雪信息矩阵定义的二次型的一半。换句话会所，它是在统计流形上，由KL散度自然导出的一个度量张量（Metric Tensor）。\n这恰恰和我们之前讨论的，参数空间的 “陡峭程度” 和训练的稳定性不谋而合。\n4. Fisher Divergence Fisher Divergence 衡量的是两个概率分布 $p(x)$ 和 $q(x)$ 的信息场之间的梯度差。更简单的说，是两个分布的 Score Funciton 之间的均方误差：\n$$ \\begin{gather} D_F(p \\mid q) \u0026= \\int p(x) \\bigg\\| \\nabla_\\theta \\log p(x) - \\nabla_\\theta \\log q(x, \\theta) \\bigg\\|^2 dx\\\\ \u0026= \\mathbb{E}_{x\\sim p(x)}\\bigg[\\big\\| \\nabla_\\theta \\log p(x) - \\nabla_\\theta \\log q(x, \\theta) \\big\\|^2\\bigg] \\end{gather} $$但是对 Fisher Divergence 的理解停留在形式上是不够的。不妨先回顾一下，我们可能更常用的 KL Divergence。对于两个分布 $p(x)$ 和 $q(x)$，有 KL Divergence：\n$$ \\begin{gather} \\begin{aligned} D_{KL}(p \\mid q) \u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[ \\log \\frac{p(x)}{q(x)} \\bigg]\\\\ \u0026= \\mathbb{E}_{x\\sim p(x)} \\big[ \\log p(x) - \\log q(x) \\big] \\end{aligned} \\end{gather} $$reference Using Fisher Information to bound KL divergence\n","permalink":"https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/","summary":"\u003cp\u003e在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。\u003c/li\u003e\n\u003cli\u003eFisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e下面则给出一些不那么直观的，数学形式上的解释。\u003c/p\u003e\n\u003ch2 id=\"1-statistical-manifold\"\u003e1. Statistical Manifold\u003c/h2\u003e\n\u003cp\u003e和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\\sigma^2, \\mu$ 控制。那么所有的高斯分布的参数 $\\sigma^2, \\mu$ 所构成的空间便形成一个 “统计流形”。\u003c/p\u003e\n\u003cp\u003e那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。\u003c/p\u003e\n\u003ch2 id=\"2-score-function\"\u003e2. Score Function\u003c/h2\u003e\n\u003cp\u003e对于一个数据集 $\\{x_1, x_2, \\cdots, x_n\\}$，我们假设其服从于一个理想的概率分布 $p(x)$，我们不知道 $p(x)$ 的具体形式，只知道它的一系列采样（就是数据集）。我们希望可以通过一个受到参数 $q(x, \\theta)$ 控制的分布族来近似它。那么一个很容易想到办法是，通过 KL 散度来衡量 $p(x)$ 和 $q(x, \\theta)$ 之间的差异：\u003c/p\u003e\n$$\n\\begin{aligned}\nD_{KL}(p(x) \\| q(x, \\theta))\n\u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log p(x) -\\log q(x, \\theta)\\bigg]\\\\\n\u0026= \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log p(x)\\bigg] - \\mathbb{E}_{x\\sim p(x)} \\bigg[\\log q(x, \\theta)\\bigg]\\\\\n\\end{aligned}\n$$\u003cp\u003e其中 $\\mathbb{E}_{x\\sim p(x)} \\bigg[\\log p(x)\\bigg]$ 是 $p(x)$ 的熵，与待优化参数 $\\theta$ 无关，因此：\u003c/p\u003e","title":"Fisher Information and Fisher Divergence"},{"content":"1. 动机 在无监督学习时，我们往往都需要处理维度非常大的数据。例如无监督学习的一个经典案例：图像生成。对于一个尺寸为 $1920 \\times 1080$ 的图片，其像素总量为 $2073600$，生成一张这样的照片时，我们需要对一个维度为 $2073600$ 的随机分布建模和采样。这些维度之间不可能是完全独立的，这个原因很显然，如果所有的维度完全独立，生成的数据就是完全随机的，不会包含任何信息。相邻像素的颜色、纹理高度相关，真正需要被建模的自由度远小于像素个数\n可以说，我们希望通过无监督学习学习到的数据之间的规律，就建模在数据维度之间的约束中，或者换一个更常用的说法，数据之间的规律。由于数据维度之间约束的存在，数据的维度一定是小于（甚至可以说远远小于）其随机向量的维度。我们希望建模的数据事实上存在于一个高维空间上的流形上，而无监督学习实质上是通过神经网络，建模这个高位空间中数据分布的流形。\n有了上面的理解，原本的如何从数据中挖掘关系这个问题就被转换为如何对数据所在的流形建模。这个问题并不容易，流形的复杂性和高维数据的稀疏性都给建模带来了挑战。目前一个主流的思路是通过概率分布对流形建模。显然，概率分布可以很方便的表达流形上的几何结构；对于一个维度为 $d$ 的概率分布，可以理解为一个从 $R^d \\to R$ 的映射，当然这个映射需要满足非负和归一化。那么对于不属于流形上的点，映射到 $0$ 就好了。当然，概率分布带给我们的好处远不止于此：\n概率分布本身可以很方便的处理噪声，可以很方便的用于处理真实的，带噪声的真实数据。 概率分布的采样和优化十分方便，有很多现成的研究成果 概率分布本身赋予流形一个 “软边界”。这让模型的泛化能力有保障。 但是概率模型依然不是完美的。一般而言，我们假设存在一个理想分布 $p_d(x)$，它表达了完美的，真实的数据的分布。这是一个 “可望而不可达” 的理想分布，我们所用的数据集 $x_1, x_2, \\cdots x_n$ 可以认为从分布 $p_d(x)$ 中做的采样。（一种柏拉图式的哲学）。我们希望可以通过一个受到参数 $\\theta$ 控制的模型分布 $p(x, \\theta)$ 来逼近和代替真实分布 $p_d(x)$。\n我们的神经网络不可能直接建模非归一化模型，模型本身几乎一定是非归一化的。假设我们只能建模一个非归一化模型 $q(x\\mid \\theta)$，则需要通过归一化系数将其转换为归一化的。\n$$ \\begin{aligned} p(x\\mid \\theta) = \\frac{1}{Z(\\theta)}q(x\\mid \\theta)\\\\ \\text{where: } Z(\\theta) = \\int q(x\\mid \\theta) dx \\end{aligned} $$但是归一化系数 $Z(\\theta) = \\int q(x\\mid \\theta) dx$ 对于高维分布几乎是不可能直接计算的。我们希望能找到一些办法，避免对归一化系数的直接计算，Noise Contrastive Estimation 就是为此而提出的一种方法。（当然，其他方法还可以参考 Score Matching 和 使用神经网路进行数据生成）\n2. 通过比较来估计密度（Density Estimation by Comparison） 当我们希望从一系列离散的，从一个未知分布的采样 $\\{X\\}_n$ 估计出具体的分布时，我们几乎一定会去提取分布的中特征。或者换句话说，我们是通过刻画分布的性质来估计未知分布的密度的。甚至可以说刻画其性质是第一性的。因此一个显然的思路是，如果我们提供一个已知的、与目标分布显著不同的噪声分布 $p_n(x)$。之后训练一个神经网络，用于区分数据来自于数据分布 $p_d(x)$ 还是噪声分布 $p_n(x)$，神经网络自然而然的就学习到了数据分布的特征。这个过程实质上就实现了数据生成的目标，即学习数据的潜在分布。而这个过程则将原本的无监督学习转换为一个简单的，有负样本的，简单的而分类问题。换句话说，NCE不直接估计概率密度，而是通过学习区分“真实数据”和“人工产生的噪声”来间接地学习模型参数。\n更具体的说，我们可以证明，这个二分类问题的边界受到 $\\frac{p_d(x)}{p_n(x)}$ 控制。在我们对 $p_n(x)$ 有清晰的认知的情况下，$p_d(x)$ 是可以直接计算的。\n2.1. 的动机和推导 我们有来自于数据分布 $p_d(x)$ 的一组采样 $\\{X\\} = \\{X_1, X_2, \\cdots X_n\\}$ 和来自于噪声分布 $p_n(x)$ 的一组采样 $\\{Y\\} = \\{Y_1, Y_2, \\cdots, Y_m\\}$。来自数据和来自噪声的采样的比例有 $\\nu = \\frac{m}{n}$。将数据 $\\{X\\},\\{Y\\}$ 混合，得到新数据集 $\\{U\\} = \\{X\\} \\cup \\{Y\\}$。从中任意抽取一个数据点，如果属于采样 $\\{X\\}$，那么认为标签为 $D = 1$ (Real)，否则为 $D = 0$ (Noise)。\n根据贝叶斯定理，有：\n$$ \\begin{aligned} p(D = 1 \\mid x) = \\frac{p(x\\mid D = 1) p(D = 1)}{p(x)}\\\\ p(D = 0 \\mid x) = \\frac{p(x\\mid D = 0) p(D = 0)}{p(x)} \\end{aligned} $$现在，我们的任务是求出上面的每一项，这件事并不难：\n目标 形式 直观 $p(x\\mid D = 1)$ $p_d(x)$ 已知数据标签下数据的分布 $p(x\\mid D = 0)$ $p_n(x)$ 已知噪声标签下噪声的分布 $p(D = 1)$ $\\frac{1}{\\nu + 1}$ 任取数据，得到标签 1 的概率 $p(D = 0)$ $\\frac{\\nu}{\\nu + 1}$ 任取数据，得到标签 0 的概率 $p(x)$ $\\frac{1}{\\nu + 1}p_d(x) + \\frac{\\nu}{\\nu + 1}p_n(x)$ 混合分布 代入之前提到的贝叶斯形式，有：\n$$ \\begin{aligned} p(D = 1 \\mid x) = \\frac{p_d(x)}{p_d(x) + \\nu p_n(x)}\\\\ p(D = 0 \\mid x) = \\frac{\\nu p_n(x)}{p_d(x) + \\nu p_n(x)}\\\\ \\end{aligned} $$那么，对于一个二分类问题，一个天然的边界是看对比后验概率 $p(D = 1 \\mid x)$ 和 $p(D = 0 \\mid x)$ 谁的概率大。如果 $p(D = 1 \\mid x)$ 概率更大则认为是数据。不妨取其比值：\n$$ \\frac{p(D = 1 \\mid x)}{p(D = 0 \\mid x)} = \\frac{p_d(x)}{\\nu p_n(x)} $$针对这个分类问题，这是一个天然的 Score Function，等于 $1$ 时，恰好是分类问题的边界。这恰好是我们希望的形式，我们说过，解释了为什么我们前面说分类问题的边界受到 $\\frac{p_d(x)}{p_n(x)}$ 控制。但是这个形式我们仍然无法直接处理，不妨用一个经典 Trick，取对数：\n$$ \\ln \\left[\\frac{p(D = 1 \\mid x)}{p(D = 0 \\mid x)}\\right] = \\ln p_d(x) - \\ln p_n(x) - \\ln \\nu $$2.2. 参数化 此时，形式中包含一个无法处理的、我们假设的数据分布 $p_d(x)$，不妨用一个受到参数 $\\theta$ 控制的非归一化模型代替：\n$$ p_d(x) \\simeq \\frac{f(x, \\theta)}{Z(\\theta)} $$其中，$Z(\\theta)$ 是归一化参数，如果你还记得第一章中的介绍，这个参数是我们进行无监督学习中的巨大阻碍。我们先将该形式代入概率比值的形式中，并定义 $c = - \\ln Z(\\theta)$，并将其作为一个学习的参数：\n$$ \\begin{aligned} \\ln \\left[\\frac{p(D = 1 \\mid x)}{p(D = 0 \\mid x)}\\right] \u0026\\simeq \\ln \\left[\\frac{p(D = 1 \\mid x, \\theta)}{p(D = 0 \\mid x)}\\right]\\\\ \u0026=\\ln f(x, \\theta) + c - \\ln p_n(x) - \\ln \\nu\\\\ \u0026= G(x, \\theta, c) \\end{aligned} $$其中的 $G(x, \\theta, c)$ 就是我们要学习的得分函数，可以通过一个神经网络描述。通过 sigmoid 函数可以恢复后验概率：\n$$ p(D=1 \\mid x, \\theta) = \\sigma(G(x, \\theta, c)) = \\frac{1}{1 - \\exp\\left[-G(x, \\theta, c)\\right]} $$2.3. 构建损失函数 不妨假设标签之间独立，这个而分类问题可以直接通过一个交叉熵损失来训练。假设存在：\n$$ \\begin{aligned} J(\\theta, c) \u0026= \\sum_{i = 0}^{m + n}\\left[ D_t \\ln p(D = 1 \\mid x_i, \\theta) + (1 - D_t) \\ln p(D = 0 \\mid x_i, \\theta) \\right] \\\\ \u0026= \\sum_{i = 0}^{m + n}\\left[ D_t \\ln p(D = 1 \\mid x_i, \\theta) + (1 - D_t) \\ln \\left[ 1 - p(D = 1 \\mid x_i, \\theta) \\right]\\right] \\\\ \\end{aligned} $$代入我们学习的 Score Function $G(x, \\theta, c)$，有：\n$$ \\begin{aligned} `J(\\theta, c) `\u0026= \\frac{1}{T_d}\\sum_{i = 0}^{n}\\bigg\\{ \\ln \\sigma(G(x, \\theta, c)) \\bigg\\} + \\frac{1}{T_d}\\sum_{i = 0}^{m} \\bigg\\{\\ln \\big[ 1 - \\sigma(G(y_i, \\theta, c)) \\big]\\bigg\\}\\\\ \u0026= \\boxed{\\frac{1}{T_d}\\sum_{i = 0}^{n}\\bigg\\{ \\ln \\sigma(G(x, \\theta, c)) \\bigg\\} + \\nu \\frac{1}{T_n}\\sum_{i = 0}^{m} \\bigg\\{\\ln \\big[ 1 - \\sigma(G(y_i, \\theta, c)) \\big]\\bigg\\}} \\end{aligned} $$通过优化 $J(\\theta, c)$，就可以训练 $G(x, \\theta, c)$ 了。\n2.4. 估计的性质 问题似乎解决了，但是事实并非如此。不难发现，在解决归一化参数时我们挖了一个大坑。我们直接将归一化参数作为一个可学习参数交给优化器优化了。但是优化优化模型参数时的一个隐藏假设是所有待优化参数都是独立的。可事实并非如此，归一化参数隐藏了一个约束假设: $Z(\\theta) = \\int_{x\\in R^n} f(x, \\theta) dx$。我们希望通过优化 $J(\\theta, c)$ 我们希望对 $c$ 可以提供了一个对 $\\ln Z(\\theta)$ 的好的估计。粗暴的假设其独立，不引入约束直接优化，如何保证优化结果可以满足收到积分形式约束呢？\n事实上，从数学上可以证明，在理想条件下（数据集无限，噪声样本无限），上面的目标函数 $J(\\theta, c)$ 的极值 $(\\hat \\theta, \\hat c)$ 满足：\n$\\hat \\theta$ 是使得 $p_m(\\mathbf{x}; \\theta)$ 最接近 $p_d(\\mathbf{x})$ 的参数（即最大似然估计的解）。 $\\hat c$ 是 $-\\ln Z(\\theta)$ 的估计最优的参数。 下面我们证明这个结论。\n3. 最优性证明 推导之前，我们不妨再化简一下上面的形式，对于 $1 - \\sigma(G(y, \\theta, c))$，有：\n$$ \\begin{aligned} 1 - \\sigma(G(y, \\theta, c)) \u0026= 1 - \\frac{1}{1 + \\exp\\left[-G(y, \\theta, c)\\right]}\\\\ \u0026= \\frac{\\exp\\left[-G(y, \\theta, c)\\right]}{1 + \\exp\\left[-G(y, \\theta, c)\\right]}\\\\ \u0026= \\frac{1}{1 + \\exp\\left[G(y, \\theta, c)\\right]}\\\\ \u0026= \\sigma(-G(y, \\theta, c)) \\end{aligned} $$当有 $m \\to \\infty, n \\to \\infty$ 时，有：\n$$ \\begin{aligned} \\frac{1}{T_d}\\sum_{i = 0}^{n}\\bigg\\{\\ln \\sigma(G(x, \\theta, c)) \\bigg\\} \u0026= \\mathbb{E}_{x\\sim p_d(x)}\\big[ \\ln \\sigma(G(x, \\theta, c))\\big]\\\\ \\frac{1}{T_n}\\sum_{i = 0}^{m} \\ln \\big[1 - \\sigma(G(y_i, \\theta, c))\\big] \u0026= \\mathbb{E}_{y\\sim p_n(y)}\\big[ \\ln \\big[- \\sigma(G(y, \\theta, c))\\big]\\big]\\\\ \\end{aligned} $$那么有：\n$$ J(\\theta, c) = \\mathbb{E}_{x\\sim p_d(x)}\\big[ \\ln \\sigma(G(x, \\theta, c))\\big] + \\nu \\mathbb{E}_{y\\sim p_n(y)}\\big[ \\ln \\big[-\\sigma(G(y, \\theta, c))\\big]\\big] $$不妨令上式对 $c$ 求导：\n$$ \\begin{aligned} \\frac{\\partial J(\\theta, c)}{\\partial c} \u0026= \\mathbb{E}_{x\\sim p_d(x)}\\big[ \\frac{\\partial}{\\partial c}\\ln \\sigma(G(x, \\theta, c))\\big] + \\nu \\mathbb{E}_{y\\sim p_n(y)}\\big[\\frac{\\partial}{\\partial c} \\ln \\big[- \\sigma(G(y, \\theta, c))\\big]\\big]\\\\ \u0026= \\mathbb{E}_{x \\sim p_d}\\left[ \\frac{1}{\\sigma(G)} \\cdot \\sigma(G)(1-\\sigma(G)) \\cdot \\frac{\\partial G}{\\partial c} \\right] + \\nu \\mathbb{E}_{y \\sim p_n}\\left[ \\frac{1}{\\sigma(-G)} \\cdot (-\\sigma(-G)(1-\\sigma(-G))) \\cdot \\frac{\\partial G}{\\partial c} \\right]\\\\ \u0026= \\mathbb{E}_{x \\sim p_d}\\left[ (1-\\sigma(G(x))) \\cdot \\frac{\\partial G}{\\partial c} \\right] + \\nu \\mathbb{E}_{y \\sim p_n}\\left[ - (1-\\sigma(-G(y))) \\cdot \\frac{\\partial G}{\\partial c} \\right] \\end{aligned} $$对于 $\\frac{\\partial G}{\\partial c}$，有：\n$$ \\begin{aligned} \\frac{\\partial G}{\\partial c} \u0026= \\frac{\\partial\\ln f(x, \\theta) + c - \\ln p_n(x) - \\ln \\nu}{\\partial c}\\\\ \u0026= 1 \\end{aligned} $$代入上式，并用 $1 - \\sigma(G) = \\sigma(-G)$ 替换，有：\n$$ \\begin{aligned} \\frac{\\partial J(\\theta, c)}{\\partial c} \u0026= \\mathbb{E}_{x \\sim p_d}\\bigg[ 1 - \\sigma(G(x)) \\bigg] - \\nu \\mathbb{E}_{y \\sim p_n}\\bigg[\\sigma(G(y)) \\bigg] \\end{aligned} $$对于优化极值，应有上式为 $0$，可得：\n$$ \\mathbb{E}_{x \\sim p_d}\\big[1 - \\sigma(G(x)) \\big] = \\nu \\mathbb{E}_{y \\sim p_n}\\big[\\sigma(G(y)) \\big] $$不妨将 $G(x)$ 的具体形式代入，有：\n$$ \\sigma(G(x)) = \\frac{1}{1 + \\exp(-G(x))} = \\frac{1}{1 + \\frac{\\nu p_n(\\mathbf{x}) e^{-c}}{f(\\mathbf{x};\\theta)}} $$有：\n$$ \\begin{aligned} \\mathbb{E}_{x \\sim p_d}\\big[1 - \\sigma(G(x)) \\big] \u0026= \\mathbb{E}_{x \\sim p_d} \\big[ \\frac{\\nu p_n(x)}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)}\\big]\\\\ \\mathbb{E}_{y \\sim p_n}\\big[\\sigma(G(y)) \\big] \u0026= \\mathbb{E}_{y \\sim p_n}\\big[ \\frac{\\frac{f(x, \\theta)}{e^{-c}}}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)} \\big] \\end{aligned} $$将期望符号展开为积分形式：\n$$ \\begin{aligned} \\mathbb{E}_{x \\sim p_d} \\big[ \\frac{\\nu p_n(x)}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)}\\big] \u0026= \\int \\frac{p_d(x)\\nu p_n(x)}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)} dx\\\\ \\nu\\mathbb{E}_{y \\sim p_n}\\big[ \\frac{\\frac{f(x, \\theta)}{e^{-c}}}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)} \\big] \u0026= \\int \\frac{\\nu p_n(x)\\frac{f(x, \\theta)}{e^{-c}}}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)}dx \\end{aligned} $$对比上下两个形式，其相等的一个充分条件为：\n$$ \\begin{aligned} \\frac{p_d(x)\\nu p_n(x)}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)} \u0026= \\frac{\\nu p_n(x)\\frac{f(x, \\theta)}{e^{-c}}}{\\frac{f(x, \\theta)}{e^{-c}} + \\nu p_n(x)}\\\\ p_d(x) \u0026= \\frac{f(x, \\theta)}{e^{-c}} \\end{aligned} $$Cool，我们完成了证明。所以，只要达到最优，就可以认为训练出的归一化参数是满足归一化性质的。\n4. 讨论 下面我们解决一些 NCE 的细节。主要想讨论两个点：\n在选择噪声时应该如何考虑 NCE 和 负样本学习的关系 4.1. 如何选择噪声 对于噪声选择，可以总结出下面几个原则：\n我们最好可以选择一个 $\\ln p_n(x)$ 有优雅解析形式的噪声 我们最好可以找到一个与数据分布 $p_d(x)$ 有一定相似性的噪声。因为 NCE 本质还是一个二分类问题，如果噪声与数据差距过大，任务会变得过于简单，导致学不到数据的内在特征。最优的噪声应使最优分类器的错误率接近 50%，这迫使模型可以学习数据特征的细节。 噪声需要可以覆盖数据分布 $p_d(x)$ 的支撑集。如果覆盖不全，对于某些数据点 $\\mathbf{x} \\sim p_d$，如果 $p_n(\\mathbf{x}) = 0$，那么在计算 $G(\\mathbf{x}) = \\ln f(\\mathbf{x};\\theta) + c - \\ln p_n(\\mathbf{x}) - \\ln \\nu$ 时，$\\ln p_n(\\mathbf{x})$ 会趋于 $-\\infty$，导致 $G(\\mathbf{x}) \\to +\\infty$。这使得 $\\sigma(G(\\mathbf{x})) \\approx 1$，模型无需学习就会给这些点非常高的分数，破坏了学习过程。 噪声最好有高方差：一个方差较大的 $p_n(\\mathbf{x})$（例如方差较大的高斯分布）通常比一个方差很小的分布更好。因为它更有可能产生一些“ challenging negatives”，即那些与真实数据点很接近的噪声样本，这有助于模型学习更精细的决策边界。 我们还可以不加证明的给出一个理论性的分析：\nNCE 的渐近正态性：$\\sqrt{T_d}(\\hat \\theta_T -\\theta^*)$ 服从渐近正态分布，其均值为 $0$，协方差矩阵为 $\\Sigma$，即：\n$$ \\Sigma = I^{-1}_\\nu - \\left(1 + \\frac{1}{\\nu}\\right) I^{-1}_\\nu E(P_\\nu g) E(P_\\nu g)^T I^{-1}_\\nu $$其中，$E(P_\\nu g) = \\int P_\\nu (u)g(u)p_d(u)du$。\n4.2. 与负采样（Negative Sampling）的关系 负采样（NEG） 是 NCE 的一个特例和近似。NCE 的原始目标是同时估计密度模型和归一化常数。而 Mikolov 等人提出的 NEG 做了一个关键的简化：\n它固定 $c = 0$（即完全假设模型是自我归一化的，$Z=1$）。 它修改了目标函数，不再精确等价于对数似然最大化。 NEG 的目标是： $$ J_{\\text{NEG}}(\\theta) = \\sum_{t=1}^{T} \\left[\\begin{array}{l} \\ln \\sigma(\\ln f(\\mathbf{x}_t; \\theta) - \\ln p_n(\\mathbf{x}_t) - \\ln k) \\\\+ \\sum_{j=1}^{k} \\ln \\sigma(-(\\ln f(\\mathbf{y}_{t,j}; \\theta) - \\ln p_n(\\mathbf{y}_{t,j}) - \\ln k)) \\end{array}\\right] $$NEG 的计算更简单，并且在像词向量学习这样的任务中效果非常好，但它不再是一个严格正确的最大似然估计器。可以认为 NEG 是 NCE 的一种高效、实用的工程近似。\n4.3. 梯度如何驱动 $c$ 最直观的理解方式是：\n参数 $c$ 在模型的对数概率 $\\ln p_m(x; \\theta, c) = \\ln f(x, \\theta) + c$ 中，充当一个全局的偏置（bias）。 如果 $c$ 太大，会导致模型 $p_m$ 的值整体偏高。在分类任务中，这意味着模型会过于自信地将所有样本（包括噪声样本）都判断为真实数据（$D=1$）。这会导致在噪声样本上的损失（即 $-\\ln(1-\\sigma)$ 部分）变得非常大。为了降低总损失，优化算法必须减小 $c$。 如果 $c$ 太小，会导致模型 $p_m$ 的值整体偏低。这意味着模型会倾向于将所有样本（包括真实数据）都判断为噪声（$D=0$）。这会导致在真实数据上的损失（即 $-\\ln \\sigma$ 部分）变得非常大。为了降低总损失，优化算法必须增大 $c$。 因此，优化过程本身形成了一个负反馈循环。损失函数通过“惩罚”在噪声样本上的过度自信和在真实样本上的不自信，迫使参数 $c$ 进行调整。这个调整的最终平衡点在哪里？\n就是当 $p_m(x; \\hat{\\theta}, \\hat{c})$ 在分布上与 $p_d(x)$ 完全一致时。此时，$\\frac{p_m(x; \\hat{\\theta}, \\hat{c})}{p_n(x)} = \\frac{p_d(x)}{p_n(x)}$，分类器达到了贝叶斯最优分类器的形态，总损失达到最小。而当 $p_m = p_d$ 时，必然意味着模型已经正确归一化，即 $e^{\\hat{c}} \\int f(x, \\hat{\\theta}) dx = \\int p_d(x) dx = 1$。 由此可得 $e^{\\hat{c}} Z(\\hat{\\theta}) = 1$，即 $\\hat{c} = -\\ln Z(\\hat{\\theta})$。\n总结一下，这个保证归一化的数学结构是一个以“区分真实与噪声”为目标的对抗性/对比性框架，其中归一化常数（或其对数）作为分类器的一个可学习偏置项。优化该分类器的损失函数，会内在地、非直接地迫使该偏置项收敛到能使模型概率分布有效的正确值。\nReference @article{10.5555/2188385.2188396, author = {Gutmann, Michael U. and Hyv\u0026quot;{a}rinen, Aapo}, title = {Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics}, year = {2012}, issue_date = {3/1/2012}, publisher = {JMLR.org}, volume = {13}, number = {null}, issn = {1532-4435}, journal = {J. Mach. Learn. Res.}, month = feb, pages = {307–361}, numpages = {55}, keywords = {computation, estimation, natural image statistics, partition function, unnormalized models} }\n","permalink":"https://wangjv0812.cn/2025/08/noise-contrastive-estimation/","summary":"\u003ch2 id=\"1-动机\"\u003e1. 动机\u003c/h2\u003e\n\u003cp\u003e在无监督学习时，我们往往都需要处理维度非常大的数据。例如无监督学习的一个经典案例：图像生成。对于一个尺寸为 $1920 \\times 1080$ 的图片，其像素总量为 $2073600$，生成一张这样的照片时，我们需要对一个维度为 $2073600$ 的随机分布建模和采样。这些维度之间不可能是完全独立的，这个原因很显然，如果所有的维度完全独立，生成的数据就是完全随机的，不会包含任何信息。相邻像素的颜色、纹理高度相关，真正需要被建模的自由度远小于像素个数\u003c/p\u003e\n\u003cp\u003e可以说，我们希望通过无监督学习学习到的数据之间的规律，就建模在数据维度之间的约束中，或者换一个更常用的说法，数据之间的规律。由于数据维度之间约束的存在，数据的维度一定是小于（甚至可以说远远小于）其随机向量的维度。我们希望建模的数据事实上存在于一个高维空间上的流形上，而无监督学习实质上是通过神经网络，建模这个高位空间中数据分布的流形。\u003c/p\u003e\n\u003cp\u003e有了上面的理解，原本的如何从数据中挖掘关系这个问题就被转换为如何对数据所在的流形建模。这个问题并不容易，流形的复杂性和高维数据的稀疏性都给建模带来了挑战。目前一个主流的思路是通过概率分布对流形建模。显然，概率分布可以很方便的表达流形上的几何结构；对于一个维度为 $d$ 的概率分布，可以理解为一个从 $R^d \\to R$ 的映射，当然这个映射需要满足非负和归一化。那么对于不属于流形上的点，映射到 $0$ 就好了。当然，概率分布带给我们的好处远不止于此：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e概率分布本身可以很方便的处理噪声，可以很方便的用于处理真实的，带噪声的真实数据。\u003c/li\u003e\n\u003cli\u003e概率分布的采样和优化十分方便，有很多现成的研究成果\u003c/li\u003e\n\u003cli\u003e概率分布本身赋予流形一个 “软边界”。这让模型的泛化能力有保障。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e但是概率模型依然不是完美的。一般而言，我们假设存在一个理想分布 $p_d(x)$，它表达了完美的，真实的数据的分布。这是一个 “可望而不可达” 的理想分布，我们所用的数据集 $x_1, x_2, \\cdots x_n$ 可以认为从分布 $p_d(x)$ 中做的采样。（一种柏拉图式的哲学）。我们希望可以通过一个受到参数 $\\theta$ 控制的模型分布 $p(x, \\theta)$ 来逼近和代替真实分布 $p_d(x)$。\u003c/p\u003e\n\u003cp\u003e我们的神经网络不可能直接建模非归一化模型，模型本身几乎一定是非归一化的。假设我们只能建模一个非归一化模型 $q(x\\mid \\theta)$，则需要通过归一化系数将其转换为归一化的。\u003c/p\u003e\n$$\n\\begin{aligned}\np(x\\mid \\theta) = \\frac{1}{Z(\\theta)}q(x\\mid \\theta)\\\\\n\\text{where: } Z(\\theta) = \\int q(x\\mid \\theta) dx\n\\end{aligned}\n$$\u003cp\u003e但是归一化系数 $Z(\\theta) = \\int q(x\\mid \\theta) dx$ 对于高维分布几乎是不可能直接计算的。我们希望能找到一些办法，避免对归一化系数的直接计算，\u003ccode\u003eNoise Contrastive Estimation\u003c/code\u003e 就是为此而提出的一种方法。（当然，其他方法还可以参考 \u003ca href=\"https://wangjv0812.github.io/WangJV-Blog-Pages/2025/08/scorematching/\"\u003eScore Matching\u003c/a\u003e 和 \u003ca href=\"https://wangjv0812.github.io/WangJV-Blog-Pages/2024/12/dreamfusion/#1-%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90\"\u003e使用神经网路进行数据生成\u003c/a\u003e）\u003c/p\u003e\n\u003ch2 id=\"2-通过比较来估计密度density-estimation-by-comparison\"\u003e2. 通过比较来估计密度（Density Estimation by Comparison）\u003c/h2\u003e\n\u003cp\u003e当我们希望从一系列离散的，从一个未知分布的采样 $\\{X\\}_n$ 估计出具体的分布时，我们几乎一定会去提取分布的中特征。或者换句话说，我们是通过刻画分布的性质来估计未知分布的密度的。甚至可以说刻画其性质是第一性的。因此一个显然的思路是，如果我们提供一个已知的、与目标分布显著不同的噪声分布 $p_n(x)$。之后训练一个神经网络，用于区分数据来自于数据分布 $p_d(x)$ 还是噪声分布 $p_n(x)$，神经网络自然而然的就学习到了数据分布的特征。这个过程实质上就实现了数据生成的目标，即学习数据的潜在分布。而这个过程则将原本的无监督学习转换为一个简单的，有负样本的，简单的而分类问题。换句话说，\u003cstrong\u003eNCE不直接估计概率密度，而是通过学习区分“真实数据”和“人工产生的噪声”来间接地学习模型参数。\u003c/strong\u003e\u003c/p\u003e","title":"Noise Contrastive Estimation"},{"content":"1. 目的和动机 在之前的关于 Score Matching 的文章中，介绍了 Score Matching 的基本概念和方法。Score Matching 巧妙的引入了 Score Function，避免了直接计算高维随机向量的归一化系数，让估计一个高维分布成为了可能。其 Loss Function 可以写作：\n$$ \\begin{aligned} J(\\theta) \u0026= \\text{E}_{\\xi \\sim p_X(\\xi)}\\left[ \\text{tr} \\left(\\nabla^2_\\xi \\log p(\\xi, \\theta)\\right)+ \\frac 1 2\\left\\| \\nabla_\\xi \\log p(\\xi, \\theta)\\right\\|^2 \\right] \\\\ \u0026= \\text{E}_{\\xi \\sim p_X(\\xi)}\\left[ \\text{tr} \\left(\\nabla_\\xi \\psi(\\xi, \\theta)\\right)+ \\frac 1 2\\left\\| \\psi(\\xi, \\theta)\\right \\|^2 \\right] \\\\ \\end{aligned} $$但是成为可能不代表它好算。Score Matching 引入了对原始分布的 Hessian Matrix 的迹 $\\nabla^2_\\xi \\log p(\\xi, \\theta)$ 的计算。显然，这比直接计算归一化系数简单了不少，但是对于一个维度为 $d$ 的随机向量的估计，需要进行 $d$ 次反向传播，者仍然十分困难。更可怕的是，在反向传播的过程中需要计算：\n$$ \\frac{\\partial}{\\partial \\theta} \\big[ \\text{tr} \\left(\\nabla_\\xi \\psi(\\xi, \\theta)\\right)\\big] = \\text{tr} \\left(\\frac{\\partial^2}{\\partial \\theta \\xi} \\psi(\\xi, \\theta)\\right) $$这一项对于数值计算而言就是灾难。在实践中，需要找到一个真实可行的简化方法。人们常常使用的方法有：\nSliced score matching Denoising score matching (DSM) 本文要讨论的则就是 Sliced Score Matching。\n2. Sliced Score Matching 受到 Sliced Wasserstein distance 的启发，宋飏博士提出 Sliced Score Matching 的方法。显然，处理一个 $1$ 维向量而非一个 $d$ 维向量容易非常多。那么如果我们将模型分布和真实分布 $p(\\theta, \\xi), p_X(\\xi)$ 的 score function. $\\psi(\\theta, \\xi), \\psi_X(\\xi)$ 随机的投影到一个随机变量 $v\\sim p_V(v)$ 上，去计算在投影向量上的平均 difference。那么，传统的 Score Matching 用的 fisher divergence 就变成了：\n$$ \\begin{aligned} J(\\theta, p(v)) = \\frac 1 2 \\mathbb{E}_{v\\sim p(v)} \\text{E}_{\\xi \\sim p_X(\\xi)}\\bigg[ \\left\\| \\psi(\\xi_i, \\theta) - \\psi_X(\\xi_i) \\right\\| ^2 \\bigg] \\end{aligned} $$之后的推导和 Score Matching 一直，只不过由于引入了随机向量 $v$，我们对 $v$ 增加了一些约束，这要求：\n$$ \\begin{array}{l} \\mathbb{E}_{v\\sim p(v)}\\left[vv^T\\right] \u003e 0\\\\ \\mathbb{E}_{v\\sim p(v)}\\left[\\| v \\|_2^2\\right] \u003c \\infty\\\\ \\end{array} $$事实上，这也是一个很弱的约束，大部分分布都满足。一般而言，$v$ 取标准高斯分布或者 Rademacher distribution 就好了。我们隐去具体的推导步骤（和 Score Matching 是一样的），此处给出结果\n$$ \\begin{aligned} J(\\theta, p(v)) \u0026= \\frac 1 2 \\mathbb{E}_{v\\sim p(v)}\\text{E}_{\\xi \\sim p_X(\\xi)}\\bigg[\\left\\| \\psi(\\xi_i, \\theta) - \\psi_X(\\xi_i) \\right\\| ^2\\bigg]\\\\ \u0026= \\mathbb{E}_{v\\sim p(v)}\\text{E}_{\\xi \\sim p_X(\\xi)}\\bigg[ v^T \\nabla_\\xi \\psi(\\xi, \\theta) v + \\frac 1 2\\left\\| v^T\\psi(\\xi, \\theta)\\right \\|^2 \\bigg]\\\\ \\end{aligned} $$对于数据集中的一个 $\\xi_i^N$，或者说对于真实分布 $p_x(\\xi)$ 的 $N$ 个采样中的一个。我们随机的在分布 $p(v)$ 中进行 $M$ 次抽样，其中任一个用于投影的随机向量 $v_{ij}^{NM}$。那么对应的估计可以写作：\n$$ \\hat{J}(\\theta, x_i^N, v_{ij}^{NM}) = \\frac 1 N \\frac 1 M \\sum_{i=1}^{N} \\sum_{j=1}^{M} v_{ij}^T \\nabla_\\xi \\psi(\\xi_i, \\theta) v_{ij} + \\frac 1 2\\left\\| v_{ij}^T\\psi(\\xi_i, \\theta) \\right \\|^2 $$注意力集中！不难注意到，由于 $v$ 服从高斯分布或者 Rademacher distribution，有（推导见附录）：\n$$ \\mathbb{E}_{p(v)} \\left[\\left(v^T \\psi(\\xi_i, \\theta)\\right)^2\\right] = \\left\\|\\psi(\\xi_i, \\theta)\\right\\|^2_2 $$那么，上式可以简化为：\n$$ \\hat{J}_{vr}(\\theta, x_i^N, v_{ij}^{NM}) = \\frac 1 N \\frac 1 M \\sum_{i=1}^{N} \\sum_{j=1}^{M} v_{ij}^T \\nabla_\\xi \\psi(\\xi_i, \\theta) v_{ij} + \\frac 1 2 \\left\\|\\psi(\\xi_i, \\theta)\\right\\|^2_2 $$事实上，我们在 Score Matching 中，已经 “剧透” 了这部分内容。$v_{ij}^T \\nabla_\\xi \\psi(\\xi_i, \\theta) v_{ij}$ 就是上一篇文章中介绍的 Hessain Trace Estimation。显然的，$\\hat{J}_{vr}$ 的表现是优于 $\\hat{J}$（不只是在计算复杂度上，还在估计的效果上）。由于我们通过解析的方法事实上消除了 $\\left\\| \\psi(\\xi, \\theta)\\right \\|^2$ 的随机性，这在事实上移除了一个对结果方差的 “贡献源”，有效的降低了结果的方差。\n关于 $v_{ij}^T \\nabla_\\xi \\psi(\\xi_i, \\theta) v_{ij}$ 的估计，我们在上一篇文章中已经介绍了。具体参考 Score Matching 4.1. 哈钦森迹估计。之前在计算 $\\text{tr} \\left(\\nabla_\\xi \\psi(\\xi, \\theta)\\right)$ 时，需要进行 $d$ 次反向传播，而对于一次 $v_i$，$v_{ij}^T \\nabla_\\xi \\psi(\\xi_i, \\theta) v_{ij}$ 只需要进行一次反向传播。而一般而言，$M \u003c\u003c d$，极大的降低了算法的复杂度。\n在实践中，分布 $p(v)$ 的采样次数 $M$ 可以用于平衡计算复杂度和估计的方差。有趣的是 YangSong 的论文中指出，甚至 $M = 1$ 也是一个不错的选择。\nAppendix ","permalink":"https://wangjv0812.cn/2025/08/sliced-score-matching/","summary":"\u003ch2 id=\"1-目的和动机\"\u003e1. 目的和动机\u003c/h2\u003e\n\u003cp\u003e在之前的关于 \u003ca href=\"https://wangjv0812.github.io/WangJV-Blog-Pages/2025/08/scorematching/\"\u003eScore Matching\u003c/a\u003e 的文章中，介绍了 Score Matching 的基本概念和方法。Score Matching 巧妙的引入了 Score Function，避免了直接计算高维随机向量的归一化系数，让估计一个高维分布成为了可能。其 Loss Function 可以写作：\u003c/p\u003e\n$$\n\\begin{aligned}  \nJ(\\theta)\n\u0026= \\text{E}_{\\xi \\sim p_X(\\xi)}\\left[\n\\text{tr} \\left(\\nabla^2_\\xi \\log p(\\xi, \\theta)\\right)+ \\frac 1 2\\left\\| \\nabla_\\xi  \\log p(\\xi, \\theta)\\right\\|^2\n\\right] \\\\\n\u0026= \\text{E}_{\\xi \\sim p_X(\\xi)}\\left[\n\\text{tr} \\left(\\nabla_\\xi \\psi(\\xi, \\theta)\\right)+ \\frac 1 2\\left\\| \\psi(\\xi, \\theta)\\right \\|^2\n\\right] \\\\\n\\end{aligned}\n$$\u003cp\u003e但是成为可能不代表它好算。Score Matching 引入了对原始分布的 Hessian Matrix 的迹 $\\nabla^2_\\xi \\log p(\\xi, \\theta)$ 的计算。显然，这比直接计算归一化系数简单了不少，但是对于一个维度为 $d$ 的随机向量的估计，需要进行 $d$ 次反向传播，者仍然十分困难。更可怕的是，在反向传播的过程中需要计算：\u003c/p\u003e\n$$\n\\frac{\\partial}{\\partial \\theta} \\big[ \\text{tr} \\left(\\nabla_\\xi \\psi(\\xi, \\theta)\\right)\\big] = \\text{tr} \\left(\\frac{\\partial^2}{\\partial \\theta \\xi}  \\psi(\\xi, \\theta)\\right)\n$$\u003cp\u003e这一项对于数值计算而言就是灾难。在实践中，需要找到一个真实可行的简化方法。人们常常使用的方法有：\u003c/p\u003e","title":"Sliced Score Matching"},{"content":"1. 为什么要用 Score Matching 很多是否，我们希望从大量的数据 $x_1, x_2, \\cdots x_n$（或者换句话说，从一个随机变量 $X$ 的大量抽象）还原回分布 $p(x)$ 本身。一个很显然的想法是通过一个带有可优化参数 $\\theta$ 的函数 $q(x \\mid \\theta)$ 来还原/近似真实的数据分布。但是优化过程中，想要保证分布的归一化性质并不容易。一个很显然思路时优化完成后通过归一化系数来保证归一化性质：\n$$ \\begin{array}{c} p(x\\mid \\theta) = \\frac{1}{Z(\\theta)}q(x\\mid \\theta)\\\\ \\text{where: } Z(\\theta) = \\int q(x\\mid \\theta) dx \\end{array} $$但是在很多情况下，生成模型需要处理一个极高维度随机向量的概率分布的积分。此时归一化系数 $Z(\\theta)$ 的计算几乎是不可能的。（如果实在希望直接计算，可以用数值方法或者 MCMC，但是这类方法同样很难直接计算。）\n要解决归一化问题的办法其实很多，事实上这在随机分布估计中是一个很常见的问题。我们不妨举一些显然的方案，例如 Flow Module、Bolzemann Machine、Variational Autoencoder 等等。那么如果归一化的分布不好处理，我们是否可以找到一个与归一化的概率分布等价的，不需要归一换的形式？答案是肯定的，就是我们后面要介绍的 Score Function 和对应的估计的方法 Score Matching。\n2. Score Function 对于一个受到参数 $\\boldsymbol{\\theta}$ 控制的，关于随机向量 $\\boldsymbol{\\xi}$ 的随机分布 $p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})$。我们定义其对数梯度为其 Score Function。形式化的，可以写作：\n$$ \\psi (\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) = \\begin{pmatrix} \\frac{\\partial p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\xi}_1}\\\\ \\frac{\\partial p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\xi}_2}\\\\ \\vdots\\\\ \\frac{\\partial p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\xi}_n} \\end{pmatrix} = \\begin{pmatrix} \\psi_1(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})\\\\ \\psi_2(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})\\\\ \\vdots\\\\ \\psi_n(\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) \\end{pmatrix} = \\nabla_{\\boldsymbol{\\xi}} \\log p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) $$我们不难发现：\n$$ \\begin{aligned} \\psi (\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) \u0026= \\nabla_{\\boldsymbol{\\xi}} \\log \\left( \\frac{q(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{Z(\\boldsymbol{\\theta})} \\right)\\\\ \u0026= \\nabla_{\\boldsymbol{\\xi}} \\log q(\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) - \\nabla_{\\boldsymbol{\\xi}} \\log Z(\\boldsymbol{\\theta})\\\\ \u0026= \\nabla_{\\boldsymbol{\\xi}} \\log q(\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) \\end{aligned} $$不难发现，在使用 Score Function 表达一个概率分布时，归一化系数被梯度吸收了。换句话说，使用 Score Function 描述分布时，我们不需要关心归一化系数 $Z(\\boldsymbol{\\theta})$ 了，我们用了一个巧妙的数学结构解决了困难的归一化问题。\n你可能会问：“是的，这是一个很巧妙的数学结构，但是如何估计这个分布，如何采样呢？”我们先解决采样的问题，答案是 Langevin Dynamics。\n预告一下，第二个问题的答案就是我们这篇文章的主题，Score Matching。\n2.1. Langevin Dynamics 在学习了得分函数 $s_\\theta(\\boldsymbol{x}) \\approx \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x})$ 后，我们还需要将其转换回原本的概率分布，这实际上是要解决一个随机微分方程。这同样并不容易，但是我们可以直接跳过求原本的分布这一步，直接从 Score Function 采样，这个方法被称为郎之万动力学 (Langevin Dynamics)。郎之万动力学提供了一种马尔可夫的方法；具体而言，从任意先验分布 $x_0 \\sim \\pi(x)$ 初始化，然后进行如下迭代：\n$$ \\begin{array}{ll} x_{i+1} = x_i + \\epsilon \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x}) + \\sqrt{2\\epsilon} \\boldsymbol{z}_i \u0026 i = 0, 1, \\cdots K \\end{array} $$其中 $\\boldsymbol{z_i} \\sim \\mathcal N(0, I)$，$\\epsilon \\to 0$, $K\\to \\infty$。实际上这个行为类似与数值微分方程的求解，后面的高斯分布项可以有效的避免陷入局部最优。事实上只要这个过程足够，误差几乎可以忽略。\n3. Score Matching 一个很直觉的想法是，对于一个原始分布的 Score Function $\\psi_X(\\xi)$，我们希望用一个收参数 $\\theta$ 控制的模型分布 $\\psi(\\xi, \\theta)$ 来近似。一个很直接的方法是用一个均方期望来衡量二者之间的距离：\n$$ J(\\theta) = \\frac 1 2 \\int_{\\xi\\in R^n} p_X(\\xi)\\left\\| \\psi(\\xi, \\theta) - \\psi_X(\\xi) \\right\\| ^2 d\\xi $$这里需要注意到一个细节，此处我们是对一个按照原始分布 $p_X(\\xi)$ 的期望，这对我们的编写程序是有很大帮助的。对于一个有 $m$ 个数据的原始分布（数据集），我们均匀随机的选择出其中 $k$ 个数据点 $\\xi_1, \\xi_2, \\cdots, \\xi_k$，那么我们可以将上面的依赖于原始分布的期望可以似为：\n$$ J(\\theta) \\approx \\frac{1}{2k} \\sum_{i=1}^k \\left\\| \\psi(\\xi_i, \\theta) - \\psi_X(\\xi_i) \\right\\| ^2 $$那么，对模型分布 $\\psi(\\xi_i, \\theta)$ 的参数 $\\theta$ 的一个最优估计可以通过最小化上面的形式来获得。事实上在这个最优估计就是 Score Matching：\n$$ \\hat \\theta = \\arg\\min_\\theta J(\\theta) $$上面这个形式的估计确实解决了需要计算归一化参数的积分的问题，但是想直接使用这个形式依旧困难。因为我们在事实上无法计算原始分布的 Score Function $\\psi_X(\\xi)$。Score Function 的精妙之处就在于通过一个数学变换避免了直接对原始分布的对数梯度的依赖。\n我们不妨假设模型分布的 Score Function $\\psi(\\xi_i, \\theta)$ 是可导。展开均方形式，有：\n$$ \\begin{aligned} J(\\theta) \u0026= \\frac 1 2 \\int_{\\xi\\in R^n} p_X(\\xi)\\left\\| \\psi(\\xi, \\theta) - \\psi_X(\\xi) \\right\\| ^2 d\\xi\\\\ \u0026= \\frac 1 2 \\int_{\\xi\\in R^n} p_X(\\xi) \\left( \\psi^2(\\xi, \\theta) - 2 \\psi(\\xi, \\theta) \\psi_X(\\xi) + \\psi_X^2(\\xi) \\right) d\\xi\\\\ \u0026= \\frac 1 2\\int_{\\xi\\in R^n} p_X(\\xi)\\psi^2(\\xi, \\theta) d\\xi - \\int_{\\xi\\in R^n} p_X(\\xi)\\psi(\\xi, \\theta) \\psi_X(\\xi) d\\xi + \\frac 1 2 \\int_{\\xi\\in R^n} p_X(\\xi)\\psi_X^2(\\xi) d\\xi\\\\ \\end{aligned} $$显然，其中 $\\int_{\\xi\\in R^n} p_X(\\xi)\\psi_X^2(\\xi) d\\xi$ 不包含待优化参数 $\\theta$，可以忽略，上式简化为：\n$$ J(\\theta) = \\frac 1 2\\int_{\\xi\\in R^n} p_X(\\xi)\\psi^2(\\xi, \\theta) d\\xi - \\int_{\\xi\\in R^n} p_X(\\xi)\\psi(\\xi, \\theta) \\psi_X(\\xi) d\\xi $$其中 $\\frac 1 2\\int_{\\xi\\in R^n} p_X(\\xi)\\psi^2(\\xi, \\theta) d\\xi$ 可以直接计算，我们并不关心。需要处理的是 $\\int_{\\xi\\in R^n} p_X(\\xi)\\psi(\\xi, \\theta)^T \\psi_X(\\xi) d\\xi$。对于这个形式，我们可以做如下变换：\n$$ \\begin{aligned} \\int_{\\xi\\in R^n} p_X(\\xi)\\psi(\\xi, \\theta) \\psi_X(\\xi) d\\xi \u0026= \\int_{\\xi\\in R^n} p_X(\\xi) \\frac{\\partial \\log p_X(\\xi)}{\\partial \\xi_i} \\psi(\\xi, \\theta) d\\xi\\\\ \u0026= \\int_{\\xi\\in R^n} \\frac{p_X(\\xi)}{p_X(\\xi)} \\frac{\\partial p_X(\\xi)}{\\partial \\xi_i} \\psi(\\xi, \\theta) d\\xi\\\\ \u0026= \\int_{\\xi\\in R^n} \\frac{\\partial p_X(\\xi)}{\\partial \\xi_i} \\psi(\\xi, \\theta) d\\xi \\end{aligned} $$$\\frac{\\partial p_X(\\xi)}{\\partial \\xi_i}$ 依然不好计算，但是可以用分部积分来处理：\n$$ \\begin{aligned} \\int d \\left(\\frac{\\partial \\log p(\\xi, \\theta)}{\\partial \\xi_i} p_X(\\xi)\\right) \u0026= \\int \\frac{\\partial \\log p(\\xi, \\theta)}{\\partial \\xi_i} \\frac{\\partial p_X(\\xi)}{\\partial \\xi_i} d \\xi + \\int \\frac{\\partial^2 \\log p(\\xi, \\theta)}{\\partial \\xi_i^2} p_X(\\xi) d \\xi \\end{aligned} $$那么，原始分布可以变形为：\n$$ \\begin{aligned} \\int_{\\xi\\in R^n} p_X(\\xi)\\psi(\\xi, \\theta) \\psi_X(\\xi) d\\xi \u0026= \\frac{\\partial \\log p(\\xi, \\theta)}{\\partial \\xi_i} p_X(\\xi) \\bigg|^\\infty_{-\\infty} - \\int \\frac{\\partial^2 \\log p(\\xi, \\theta)}{\\partial \\xi_i^2} p_X(\\xi) d \\xi \\end{aligned} $$我们不妨做一个比较一般的假设（确实是一个很弱的假设），假设 $p(\\xi, \\theta)$ 和 $p_X(\\xi)$ 是短尾的，乘积 $\\frac{\\partial \\log p(\\xi, \\theta)}{\\partial \\xi_i} p_X(\\xi)$ 在 $\\infty, -\\infty$ 处收敛为 $0$。这个假设弱到几乎找不到一个反例（事实上有反例，例如帕累托分布）。我们几乎总是可以认为这个假设是成立的。\n$$ \\int_{\\xi\\in R^n} p_X(\\xi)\\psi(\\xi, \\theta) \\psi_X(\\xi) d\\xi = - \\int \\frac{\\partial^2 \\log p(\\xi, \\theta)}{\\partial \\xi_i^2} p_X(\\xi) d \\xi $$原始的均方期望可以被变换为：\n$$ \\begin{aligned} J(\\theta) \u0026= \\frac 1 2\\int_{\\xi\\in R^n} p_X(\\xi)\\psi^2(\\xi, \\theta) d\\xi + \\sum_{i=1}^n\\int_{\\xi\\in R^n} \\frac{\\partial^2 \\log p(\\xi, \\theta)}{\\partial \\xi_i^2} p_X(\\xi) d \\xi \\\\ \u0026= \\int_{\\xi\\in R^n} p_X(\\xi) \\sum_{i=1}^{n} \\left[ \\frac 1 2 \\psi_i(\\xi, \\theta)^2 + \\frac{\\partial \\psi(\\xi, \\theta)}{\\partial \\xi_i} \\right] d \\xi \\end{aligned} $$问题已经解决了，但是结论还不够好看。不妨注意力集中一点，不难注意到：\n$$ \\sum_{i=1}^{n} \\frac{\\partial^2 \\log p(\\xi, \\theta)}{\\partial \\xi_i^2} = \\text{tr} \\left(\\nabla^2_\\xi \\log p(\\xi, \\theta)\\right) $$结论可以写成一个更简洁优雅的形式：\n$$ \\begin{aligned} J(\\theta) \u0026= \\text{E}_{\\xi \\sim p_X(\\xi)}\\left[ \\text{tr} \\left(\\nabla^2_\\xi \\log p(\\xi, \\theta)\\right) + \\frac 1 2\\left\\| \\nabla_\\xi \\log p(\\xi, \\theta)\\right\\|^2\\right] \\\\ \u0026= \\text{E}_{\\xi \\sim p_X(\\xi)}\\left[ \\text{tr} \\left(\\nabla_\\xi \\psi(\\xi, \\theta)\\right) + \\frac 1 2\\left\\| \\psi(\\xi, \\theta)\\right \\|^2\\right] \\\\ \\end{aligned} $$4. 讨论 我们在这里只希望讨论一下在编程实践中可能遇到的问题。对于一个有 $m$ 个数据的原始分布，我嘛可以认为是对原始分布所在的流形上采样出了 $m$ 个数据点。为了描述方便，不妨将这些采样记作 $X_1, X_2, \\cdots, X_{m}$。那么 Score Matching 的目标函数可以写作：\n$$ \\check{J}(\\theta) = \\frac{1}{m} \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\left[ \\frac 1 2 \\psi_i(\\xi, \\theta)^2 + \\frac{\\partial \\psi(\\xi, \\theta)}{\\partial \\xi_i} \\right] $$虽然 Score Matching 避免了计算复杂的归一化系数，但是却引入了另一个计算上的麻烦，我们需要计算海森矩阵的迹。对于一个维度为 $d$ 的随机分布的估计 $\\psi(\\xi, \\theta)$，其对于 $\\xi$ 的梯度是一个 $d \\times d$ 维度的矩阵，在事实上我们进行了 $$d(d+1)/2$$ 次的计算。事实上我们不需要计算这么多元素，只需要计算对角线上的元素就可以了。全部计算是十分不经济的。一般而言，会用哈钦森迹估计（Hutchinson\u0026rsquo;s Trace Estimation）来近似。\n4.1. 哈钦森迹估计 (Hutchinson\u0026rsquo;s Trace Estimation) 对于任意一个方阵 $H\\in R^{d\\times d}$ 和一个满足均值为 $0$，方差为 $I$的随机分布（一般可以取 Rademacher 分布 或者 标准高斯分布） $v\\in R^d$，有：\n$$ \\begin{aligned} \\mathbb{E}\\left[\\text{tr}(v^\\top H v)\\right] \u0026= \\mathbb{E} \\left[\\sum_{i=1}^d \\sum_{j=1} v_i H_{ij} v_j\\right]\\\\ \u0026= \\sum_{i=1}^d \\sum_{j=1} H_{ij}\\mathbb{E}\\left[v_i v_j^T\\right] \\end{aligned} $$因为 $v$ 满足标准高斯分布，因此方差 $\\mathbb{E}\\left[v_i v_j^T\\right] = I$。所以：\n$$ \\mathbb{E}\\left[\\text{tr}(v^\\top H v)\\right] = \\sum_{i=1}^d H_{ii} = \\text{tr}(H) $$此时，我们只需要从标准高斯分布中采样出一定 $k$ 个随机变量 $v_1, v_2, \\cdots, v_k$（一般 $k$ 是远小于随机变量维度 $d$ 的），然后计算 $v^\\top Hv$。因为 $Hv$ 是一个标准的 海森向量积 (Hessian-Vector Product, HVP)。\nHVP 在计算上是经济的，是因为：\n$$ \\nabla_\\xi^2 p(\\xi; \\theta) v = \\nabla_\\xi \\left(\\nabla_\\xi p(\\xi; \\theta) v\\right) $$不难发现:\n$$ \\begin{array}{lc} \\text{Hessian-Vector Product} \u0026 \\text{Shape}\\\\ \\nabla_\\xi^2 p(\\xi; \\theta) \u0026 d \\times d\\\\ \\nabla_\\xi p(\\xi; \\theta) \u0026 d \\times 1\\\\ \\nabla_\\xi p(\\xi; \\theta) \\cdot v \u0026 1 \\times 1\\\\ \\nabla_\\xi[\\nabla_\\xi p(\\xi; \\theta)v] \u0026 d \\times 1\\\\ \\end{array} $$这个计算过程中避免了直接存储和计算高维的 Hessian Matrix。因此具有高效性。\n4.2. 隐分布估计 (Score Estimation for Implicit Distributions) 一些情况下，我们会面对一些隐式分布，这些分布常常容易采样，但是难移直接计算概率密度。例如对抗生成网络 (GAN) 的生成器。GAN 的生成器可以看作一个专门用来做采样的函数，但是如果我们希望得知某一个采样结果在分布中的概率，这就很难了。在这些情况下，Score Function $\\psi_q(x) = \\nabla_x \\log q_\\theta(x)$ 就很有用了。\n下面不妨用一个例子，来具体讲讲 Score Matching 在隐分布估计中的应用。在 Variational Free Energy 中，常常使用 熵正则化 （Entropy Regularization）来帮助优化。一般而言，对于一个隐分布 $q(x)$，其熵可以写作：\n$$ H\\left(q_\\theta(x)\\right) = \\mathbb{E}_{q_\\theta(x)}\\big[\\log q_\\theta(x)\\big] $$熵一般用来衡量一个分布的不确定程度，我们可以认为熵正则化是一种 “模型推动力”，它会促使模型去探索更广大的分布空间，提高模型的多样性。但是对于一个 GAN 生成器，我们只能很方便的实现采样，熵正杂化并不容易。\n例如对于一个生成器，我们可以认为存在一个训练的、确定性的变换 $g_\\theta(\\epsilon)$，这个变换将一个简单的分布 $\\epsilon$（一般是正态分布） 变换到一个复杂分布上（就是我们的目标分布），因此在采样中，只需要在正态分布 $\\epsilon$ 上采样，并通过变换 $g_\\theta(\\epsilon)$ 获得一个采样 $x = g_\\theta(\\epsilon)$。但是关于采样 $x$ 的概率密度，我们并不清楚，这导致我们无法直接计算熵正则化。但是熵正则化关于可学习参数的梯度可以写作：\n$$ \\begin{aligned} \\nabla_\\theta H\\left(q_\\theta(x)\\right) \u0026= \\nabla_\\theta \\mathbb{E}_{q_\\theta(x)}\\big[\\log q_\\theta(x)\\big]\\\\ \u0026= \\nabla_\\theta \\mathbb{E}_{p(\\epsilon)}\\big[ \\log q_\\theta(g_\\theta(\\epsilon)) \\big]\\\\ \u0026= \\mathbb{E}_{p(\\epsilon)}\\big[ \\nabla_x \\log q_\\theta(x)\\bigg|_{x=g_\\theta(\\epsilon)} \\nabla_\\theta g_\\theta(\\epsilon)\\big]\\\\ \\end{aligned} $$其中 $\\nabla_\\theta g_\\theta(\\epsilon)$，直接反向传播即可，比较容易计算。而 $\\nabla_x \\log q_\\theta(x)\\bigg|_{x=g_\\theta(\\epsilon)}$ 则是一个 Score Function。但是这项依然没法直接算，可以通过 Score Matching 来估计。\n此时，如果我们希望通过熵正则化来优化我们的模型，则需要先通过一个 Score Matching 来估计 Score Function $\\nabla_x \\log q_\\theta(x)$，然后再通过上面的形式来计算梯度。\nreference @article{JMLR:v6:hyvarinen05a, author = {Aapo Hyv{{\u0026quot;a}}rinen}, title = {Estimation of Non-Normalized Statistical Models by Score Matching}, journal = {Journal of Machine Learning Research}, year = {2005}, volume = {6}, number = {24}, pages = {695\u0026ndash;709}, url = {http://jmlr.org/papers/v6/hyvarinen05a.html} }\n","permalink":"https://wangjv0812.cn/2025/08/scorematching/","summary":"\u003ch2 id=\"1-为什么要用-score-matching\"\u003e1. 为什么要用 Score Matching\u003c/h2\u003e\n\u003cp\u003e很多是否，我们希望从大量的数据 $x_1, x_2, \\cdots x_n$（或者换句话说，从一个随机变量 $X$ 的大量抽象）还原回分布 $p(x)$ 本身。一个很显然的想法是通过一个带有可优化参数 $\\theta$ 的函数 $q(x \\mid \\theta)$ 来还原/近似真实的数据分布。但是优化过程中，想要保证分布的归一化性质并不容易。一个很显然思路时优化完成后通过归一化系数来保证归一化性质：\u003c/p\u003e\n$$\n\\begin{array}{c}\np(x\\mid \\theta) = \\frac{1}{Z(\\theta)}q(x\\mid \\theta)\\\\\n\\text{where: } Z(\\theta) = \\int q(x\\mid \\theta) dx\n\\end{array}\n$$\u003cp\u003e但是在很多情况下，生成模型需要处理一个极高维度随机向量的概率分布的积分。此时归一化系数 $Z(\\theta)$ 的计算几乎是不可能的。（如果实在希望直接计算，可以用数值方法或者 MCMC，但是这类方法同样很难直接计算。）\u003c/p\u003e\n\u003cp\u003e要解决归一化问题的办法其实很多，事实上这在随机分布估计中是一个很常见的问题。我们不妨举一些显然的方案，例如 Flow Module、Bolzemann Machine、Variational Autoencoder 等等。那么如果归一化的分布不好处理，我们是否可以找到一个与归一化的概率分布等价的，不需要归一换的形式？答案是肯定的，就是我们后面要介绍的 Score Function 和对应的估计的方法 Score Matching。\u003c/p\u003e\n\u003ch2 id=\"2-score-function\"\u003e2. Score Function\u003c/h2\u003e\n\u003cp\u003e对于一个受到参数 $\\boldsymbol{\\theta}$ 控制的，关于随机向量 $\\boldsymbol{\\xi}$ 的随机分布 $p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})$。我们定义其对数梯度为其 Score Function。形式化的，可以写作：\u003c/p\u003e\n$$\n\\psi (\\boldsymbol{\\xi}, \\boldsymbol{\\theta}) =\n\\begin{pmatrix}\n  \\frac{\\partial p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\xi}_1}\\\\\n  \\frac{\\partial p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\xi}_2}\\\\\n  \\vdots\\\\\n  \\frac{\\partial p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\xi}_n}\n\\end{pmatrix} =\n\\begin{pmatrix}\n  \\psi_1(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})\\\\\n  \\psi_2(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})\\\\\n  \\vdots\\\\\n  \\psi_n(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})\n\\end{pmatrix} =\n\\nabla_{\\boldsymbol{\\xi}} \\log p(\\boldsymbol{\\xi}, \\boldsymbol{\\theta})\n$$\u003cp\u003e我们不难发现：\u003c/p\u003e","title":"ScoreMatching"},{"content":"我们知道，李群实质上是在一个微分流形性质的群。可以看到，李群实质上是 群 和 微分流形 的交集。想要搞明白微分流形是什么并不容易，这需要学习关于微分几何的知识。但是幸运的是，李群研究研究并没有那么依赖于微分流形的知识（事实上这样说并不准确，但是我们尽量不涉及）。和微分几何相比，群的知识就简单的多了。只要捋清概念，即便是中学生也可以明白。\n事实上群被描述为一个带有一个运算（或者说二元关系）的集合，这个集合和其上定义的二元运算需要满足四个基本性质。我们将集合标记为 $C$，二元运算为 $[\\cdot\\ ,\\ \\cdot]$。需要满足的性质为：\n封闭性：$\\forall c_1, c_2 \\in C, [c_1, c_2] \\in C$ 结合律：$\\forall c_1, c_2, c_3 \\in C, [[c_1, c_2], c_3 ] = [c_1, [c_2, c_3]]$ 单位元：$\\forall c \\in C, \\exists e \\in C, \\text{ s.t. } ce = ec = c$ 逆元：$\\forall c_1 \\in C, \\exists c_2 \\in C, c_1c_2 = c_2c_1 = e$ 在研究群的性质时，我们需要清晰的认识到 集合 和 定义在集合上的 二元运算 是同样重要的。最初提出群这个概念是诶了解决对称性问题，这种对称性关系实质上是研究一种数学结构上的 “操作不变形”。即在一个元素操作前后的结果是完全相同的，我们就称这两个元素在操作上是 “对称” 的。例如对于一个球，在任意元素在球心上做“旋转” 操作，球本身是完全不变的，我们可以称“球”构成的集合 在 “过圆心旋转” 这样操作下，是对称的。\n另一个很经典的例子是用群来描述等边三角形的旋转不变形。但是这个例子我们后面再补充\n1. 群的结构和基本操作 1.1. 子群 对于集合 $G$ 的一个子集 $H \\subset G$，在群 $G$ 定义的运算律 $\\cdot$ 上满足群的性质，就称 $H$ 为 $G$ 的子群。不难察觉到，单位元 $e$ 一定在 $H$ 上。例如任意通过坐标原点的直线都可以看作定义在加法上的对 $R(2)$ 的子群。\n1.2. 陪群 对于一个群 $G$ 和其子群 $N$，$N$ 对 $g \\in G$ 的陪集定义为\n$$ \\begin{aligned} \u0026\\text{rightcoset: } \u0026 Ng = \\{n \\cdot g \\mid \\forall n \\in N\\} \\\\ \u0026\\text{leftcoset: } \u0026 gN = \\{g \\cdot n \\mid \\forall n \\in N\\} \\\\ \\end{aligned} $$对于陪集，有很多信息可以研究。不妨先看看左右陪集的关系，也就是著名的 陪集定理\n1.2.2. 陪集定理 对于一个子集 $N$，其左右陪集合要么没有相同元素，要么完全相等\n证明：对于子集 $N$ 的两个陪集 $g_1 N$ 和 $g_2 N$，假设有一个相同元素:\n$$ g_1 n_1 = g_2 n_2 $$那么应有：\n$$ \\begin{aligned} g_1 N \u0026= g_1 n_1 N \u0026 \\text{because } N \\text{ is closed}\\\\ \u0026= g_2 n_2 N \u0026 \\text{because } g_1 n_1 = g_2 n_2\\\\ \u0026= g_2 N \u0026 \\text{because } N \\text{ is closed} \\end{aligned} $$可以看到，如果 $g_1 N$ 和 $g_2 N$ 有一个相同元素，那么它们就完全相等。也就是说，$g_1 N = g_2 N$。\n从陪集定理可以看出来，因为配集之间绝对不重复，可以用来分割群 $G = \\cup_{i} g_i N$，缺那个元素就用该元素作为生成元搞一个陪集就好了。因为该元素一定可以在生成的陪集中找到。事实上这个分割的思路就是后面要讲到的商群和核的基础。\n1.2.3. 陪集的重复性 对于 $aN$，容易找到 $\\forall a' \\in aN$，满足 $a' N = aN$。证明很容易，不妨假设 a\u0026rsquo; = an\u0026rsquo;。那么有 $a'N = an'N = aN$\n1.3. 共轭子群 / 正规子群（normal subgroup） 刚才提到了，两个陪集要么完全不同，要么完全相同。那么不妨思考，对于群 $G$ 如果可以找到一个子群 $N$，满足对于 $\\forall g \\in G$，满足其左右陪集都相等，我们称之为 $G$ 的正规子群。\n$$ \\begin{aligned} gN = Ng \u0026 \u0026 \\forall g \\in G \\end{aligned} $$或者变形一下，变为更常见的形式：\n$$ N = \\{gNg^{-1} \\mid \\forall g \\in G\\} $$事实上这个形式也很重要。一般而言，我们称 $n_\\alpha = g n_\\beta g^{-1} \\exists g \\in G$ 为 $n_\\alpha$ 和 $n_\\beta$ 共轭。\n1.4. 群中心 群中心是一个特殊的正规子群，定义为群中对其他元素交换的部分，即：\n$$ C(G) = \\{g_c \\mid g_c \\in G, g_cg = gg_c \\forall g \\in G\\} $$不难证明群的中心一定是正规子群。事实上只需要证明 $\\forall g \\in G, z \\in C(G), gzg^{-1} \\in C(G)$。即 $gzg^{-1} = gg^{-1}z = z$。群的中心位正规子群。反过来不难发现，正规子群不一定是群的中心。群的中心要求对任意元素交换，即 $gz = zg$，但是正规子群之要求 $gz_1 = z_2 g,\\ \\{z_1, z_2\\} \\in Z$ 即可。\n1.5. 商群 不难发现，正规子群的左配集本身可以构成一个群。我们便称之为商群。定义群 $G$ 和正规子群 $N$，有：\n$$ G/N = \\{gN \\mid \\forall g \\in G\\} $$这个定义看起来很难理解，用比较简答的话说就是：对于不同的 $g \\in G$，$gN$ 组成的集合可以在 $G$ 定义的运算上构成一个群。好吧我知道这个解释只能让你知道商群张什么样，但是还是不知道为什么要定义它。但是不妨先接受它，我们先证明它是一个群，之后再来讨论起实质上的含义。\n幺元 我们不妨直接定义 $eN$ 为商群的幺元。不难发现其满足：\n$$ gN \\cdot eN = gN \\cdot N = gN $$ 封闭性 对于商群中任意两个元素 $g_1 N, g_2 N$，有：\n$$ \\begin{aligned} g_1 N g_2 N = g_1 g_2 N g_2^{-1} g_2 N = g_1 g_2 N \\end{aligned} $$这个证明不难看出，只有 $N$ 是正规子群时才能保证\n结合律 对于商群中的三个元素 $g_1N, g_2N, g_3N$，有：\n$$ \\begin{aligned} (g_1N g_2N) g_3N \u0026= g_1g_2 N g_3 N = g_1g_2g_3 N\\\\ \u0026= g_1N g_2g_3N = g_1N (g_2N g_3N)\\\\ \\end{aligned} $$ 逆元 对于任意元素 $gN$，我们总可以找到 $g^{-1}N$，满足:\n$$ gN g^{-1}N = eN $$ 运算的良定性 最后，还需要再证明最后一个重要性。事实上这个性质是前面所有证明的前提，但是为了该证明更好理解，我们放在后面讨论。我们前面在讨论商群时提到了，对于 $g_1' \\neq g_1, g_1'N = g_1N, g_2' \\neq g_2, g_2'N = g_2N$。是否有 $g_1N g_2N = g_1'N g_2'N$。即陪群上的运算是否会与陪集的选择有关。不妨看：\n$$ \\begin{aligned} g_1'N g_2'N \u0026= g_1' g_2' N\\\\ \u0026= g_1' N g_2'\\\\ \u0026= g_1 N g_2'\\\\ \u0026= g_1 N g_2 N \\end{aligned} $$可以看到，只要商群上的元素相同（即配集相同），即便配集的生成元不同，运算的结果也相同。\n结合前面介绍的陪集定理，可以知道群 $G$ 商的每个元素都可以在商群的元素中找到。事实上，商群是使用不变子群对原始群做的一个 \u0026ldquo;分割\u0026rdquo;。即通过不变子群和群中元素的 “作用” 产生陪集来讲\n这个理解可以在后面的核同构中得到更深刻的诠释。\n2. 同态 (Homomorphism) 和同构 (isomorphism) 我们可以将同态和同构这两个概念一同讨论，不妨先讨论同态：\n同态指的是可以保持群的结构（运算运算）的映射。即对于两个群 $G_1$ 和 $G_2$，如果存在一个映射 $\\phi: G_1 \\to G_2$，使得：\n$$ \\phi(g_1 g_2) = \\phi(g_1) \\cdot \\phi(g_2) \\quad \\forall g_1, g_2 \\in G_1 $$这个映射有一种给人一种群运算穿透映射的印象。直观的讲，映射 $\\phi$ 将 $G_1$ 翻译到了 $G_2$ 中，同时不改变运算关系。有趣的一点是，这个映射 $\\phi$ 会将 $G_1$ 中的单位元映射到 $G_2$ 的单位元上，类似的，逆元也有类似的对应关系。\n可以看到，同态既不要求单射也不要求满射, 而同构是既要求单射又要求满射的同态.\n2.1. 映射的核和核同态定理 2.1.1. 映射的核 对于一个群同态 (Homomorphism) $f: G_1 \\to G_2$，映射 $f$ 的核 $\\text{Ker}(f)$ 定位为 $G_1$ 中所有的映射到 $G_2$ 中的单位元上的元素：\n$$ \\text{Ker}(f) = \\{g\\in G_1 \\mid f(g) = e_H\\} $$一个直观的理解是，映射的核实质上衡量了映射 $f$ 中，$G_1$ 中有多少信息消失了，或者说被压缩了，这个理解在后面介绍的 映射的核与商群之间的关系时由根深刻的认识。\n2.1.2. 同态核定理（第一同态定理） 同态核定理可以描述为：\n同态核 $\\text{Ker}(f)$ 是 $G_1$ 的正规子群 商群 $G_1/\\text{Ker}(f)$ 与 $G_2$ 同构 事实上上面第二条描述是不完备的，上面的描述需要 $\\phi$ 是一个满射，如果不是满射的话，需要将 $G_2$ 替换为 $G_2$ 的子群 $\\text{Im}(f)$；不过这个简化无伤大雅。完整版本用符号可以表示为：\n$$ G/\\text{Ker}(f) \\simeq \\text{Im}(f) $$结合商群和陪集定理，可以对核同态定理有一个直观且清晰的认识：同态核 $\\text{Ker}(f)$ 可以将 $G_1$ 分割为 $G_1 = \\cup_{i}\\text{Ker}(f)g_{1i}$。核同态定理的证明过程可以总结为：\n证明 $\\text{Ker}(f)$ 是 $G_1$ 上的正规子群 对于任意 $k_1, k_2 \\in \\text{Ker}(f)$，根据核的定义我们知道一定有 $k_1, k_2 \\in G_1$，只需要证明 $\\text{Ker}(f)$ 满足群的四个性质。对于幺元性和结合性我们无需证明，这在核的定义中是显然的，只需证明封闭性和逆元：\n$$ \\begin{array}{cl} \u0026\\text{封闭性}： \u0026 \\phi(k_1k_2) = \\phi(k_1)\\phi(k_2) = h_e \\in \\text{Ker}(f)\\\\ \u0026\\text{逆元}： \u0026 \\phi(k_1k_1^{-1}) = \\phi(k_1)\\phi(k_1^{-1}) = h_e \\end{array} $$之后需要证明其是正规子群。对于 $\\forall g \\in G_1$，有：\n$$ \\begin{aligned} \\phi(g k_1 g^{-1}) \u0026= \\phi(g)\\phi(k_1)\\phi(g)^{-1}\\\\ \u0026= h_g h_e h_g^{-1}\\\\ \u0026= h_e \\in G_2 \\end{aligned} $$ 下面我们证明使用核 $\\text{Ker}(f)$ 分割出的 $G_1$ 中的元素全部被映射到了 $G_2$ 的同一个元素上： $$ \\phi(g\\text{Ker}(f)) = \\phi(g)\\phi(\\text{Ker}(f)) = \\phi(g) \\in G_2 $$这就可以看到，通过 $\\text{Ker}(f)$ 对原本的操作产生了一个分割。核 $\\text{Ker}(f)$ 的作用就是将映射 $\\phi$ 映射到同一个元素的所有 “源元素” 打包。\n下面我们不妨定义一个商群 $G/\\text{Ker}(f)$ 到 $G_2$ 到映射 $f$： $$ f(g) = \\phi(g/\\text{Ker}(f)) $$这一步事实上证明了映射 $\\phi(g/\\text{Ker}(f))$ 是满射。只要再这个映射是单射的，就证明了商群 $g/\\text{Ker}(f)$ 和 $\\phi(G_1)$ 是完全等价的，是同构的。\n证明映射 $\\phi(g/\\text{Ker}(f))$ 是单射，不妨用反证法证明 先假设存在 $g_1 \\text{Ker}(f) \\neq g_2\\text{Ker}(f)$，满足像相同\n现在对 $\\phi(g_1)$ = $\\phi(g_2)$ 这个等式两边同时左乘 $\\phi(g_1)^{-1}$。我们知道 $\\phi(g_1)^{-1} = \\phi(g_1^{-1})$，有：\n$$ \\begin{aligned} \\phi(g_1^{-1})\\phi(g_1) \u0026= \\phi(g_1^{-1})\\phi(g_2)\\\\ h_e \u0026= \\phi(g_1^{-1}g_2)\\\\ \\end{aligned} $$根据核的定义，应有 $g_1^{-1}g_2 \\in \\text{Ker}(f)$。在介绍陪集时我们知道，两个陪集合要么完全相同，要么完全不同。那么由 $g_1^{-1}g_2 \\in \\text{Ker}(f)$ 恰恰可以说明 $g_1 \\text{Ker}(f) = g_2\\text{Ker}(f)$。\n与假设不符，证明完毕。\n2.2. 同构映射 同构（Isomorphism）就是一个双射（或者说一一映射）版本的同态（Homomorphism）。同构的好处是如果两个群之间存在一个同构映射，在事实上说明这两个群是完全等价的（元素之间可以一一映射，还保留群结构，或者说群元素之间的作用关系）。\n一个群上的任意性质都可以无损的迁移到和其同构的群上。\n3. 群作用和变换群 3.1. 左右作用和伴随作用 左作用：对于群 $G$，$\\forall g, g' \\in G$，定义左作用为一个映射 $L_g: G \\to G$, $L_g g' = gg'$ 右作用：对于群 $G$，$\\forall g, g' \\in G$，定义右作用为一个映射 $R_g: G \\to G$, $R_g g' = g'g^{-1}$ 伴随作用（非常重要滴）：对于群 $G$，$\\forall g, g' \\in G$，定义伴随作用为 $\\text{Ad}_g g' = gg'g^{-1}$ 伴随作用是非常非常重要，可以预告一下，后面的李群和李代数操作中，伴随作用可以用来“挪动李代数的展开原点“。显然，伴随作用是一个同构映射，容易证明其可以保持群的运算（结构），且是一一映射。还需要题的一点是，虽然这里叫伴随，其实 $Ad$，Adjacend 也可以被翻译成共轭，就是前面 \u0026ldquo;共轭子群\u0026rdquo;。\n3.2. 变换群与自同构映射 之前我们接触的群所描述的对象都是一些比较自然的对象，下面我们将看到群这个数学工具所具有的巨大的承载能力。不妨思考，对于一个集合 $X$，双射 $\\phi: X \\to X$ 称为定义在 $X$ 上的置换作用。对于置换作用，我们可以定义映射的 “乘法”（事实上称之为一个 “二元运算” 更加准确）:\n对于任意定义在 $X$ 上的两个一一映射 $\\phi,\\psi$，其乘法 $\\phi\\psi = \\phi(\\psi())$。容易证明，在这个二元运算的定义下，$X$ 的置换群构成一个群。定义在 $X$ 上的全部二元运算称为 $X$ 上的 完全置换群 $S_X$。如果集合 $X$ 有 $n$ 个元素，对应的完全置换群称之为 $n$阶置换群，有 $n!$ 个元素（因为这是一个一一映射，一共有 $n!$ 中可能的映射关系 ）。还需要提一小点，完全置换群还可以作用在无穷集合上（甚至希尔伯特空间）。\n形式化的讲，完全置换群 $S_X$ 可以写作:\n$$ S_X = \\{\\text{all map from } X \\to X\\} $$从左变换群或者右变换群可以看出，群 $G$ 的完全变换群中一定存在一个子群同构于 $G$。证明也很容易，左变换群或者右变换群就与 $G$ 同构。\n但是显然的，完全置换去中很多运算本身并无法在群 $G$ 上形成一个同构。我们将在 $G$ 上同构的映射称为 自同构映射，自同构映射构成的群称为 自同构映射群 $AutG$。\n自同构群实质上表示的是保持群 $G$ 的结构（运算规则）的映射，实质上是对 $G$ 上的元素的 “重排”。但是很多时候这种 “重排作用” 并不是来自于群 $G$ 内部的作用（可以不由群内元素作用，或者不由群内定义的运算）。因此我们需要明确，完全由群内元素作用的同构映射，我们称之为 内自同构映射 $InnG$。可以证明，自同构映射一定是 群内 伴随作用 诱导出来的。\n","permalink":"https://wangjv0812.cn/2025/06/naive-group-theory/","summary":"\u003cp\u003e我们知道，李群实质上是在一个微分流形性质的群。可以看到，李群实质上是 \u003cstrong\u003e群\u003c/strong\u003e 和 \u003cstrong\u003e微分流形\u003c/strong\u003e 的交集。想要搞明白微分流形是什么并不容易，这需要学习关于微分几何的知识。但是幸运的是，李群研究研究并没有那么依赖于微分流形的知识（事实上这样说并不准确，但是我们尽量不涉及）。和微分几何相比，群的知识就简单的多了。只要捋清概念，即便是中学生也可以明白。\u003c/p\u003e\n\u003cp\u003e事实上群被描述为一个带有一个运算（或者说二元关系）的集合，这个集合和其上定义的二元运算需要满足四个基本性质。我们将集合标记为 $C$，二元运算为 $[\\cdot\\ ,\\  \\cdot]$。需要满足的性质为：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e封闭性：$\\forall c_1, c_2 \\in C, [c_1, c_2] \\in C$\u003c/li\u003e\n\u003cli\u003e结合律：$\\forall c_1, c_2, c_3 \\in C, [[c_1, c_2], c_3 ] = [c_1, [c_2, c_3]]$\u003c/li\u003e\n\u003cli\u003e单位元：$\\forall c \\in C, \\exists e \\in C, \\text{  s.t.  } ce = ec = c$\u003c/li\u003e\n\u003cli\u003e逆元：$\\forall c_1 \\in C, \\exists c_2 \\in C, c_1c_2 = c_2c_1 = e$\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在研究群的性质时，我们需要清晰的认识到 \u003cstrong\u003e集合\u003c/strong\u003e 和 定义在集合上的 \u003cstrong\u003e二元运算\u003c/strong\u003e 是同样重要的。最初提出群这个概念是诶了解决对称性问题，这种对称性关系实质上是研究一种数学结构上的 “\u003cstrong\u003e操作不变形\u003c/strong\u003e”。即在一个元素操作前后的结果是完全相同的，我们就称这两个元素在操作上是 “对称” 的。例如对于一个球，在任意元素在球心上做“旋转” 操作，球本身是完全不变的，我们可以称“球”构成的集合 在 “过圆心旋转” 这样操作下，是对称的。\u003c/p\u003e\n\u003cp\u003e另一个很经典的例子是用群来描述等边三角形的旋转不变形。但是这个例子我们后面再补充\u003c/p\u003e\n\u003ch2 id=\"1-群的结构和基本操作\"\u003e1. 群的结构和基本操作\u003c/h2\u003e\n\u003ch3 id=\"11-子群\"\u003e1.1. 子群\u003c/h3\u003e\n\u003cp\u003e对于集合 $G$ 的一个子集 $H \\subset G$，在群 $G$ 定义的运算律 $\\cdot$ 上满足群的性质，就称 $H$ 为 $G$ 的子群。不难察觉到，单位元 $e$ 一定在 $H$ 上。例如任意通过坐标原点的直线都可以看作定义在加法上的对 $R(2)$ 的子群。\u003c/p\u003e","title":"Naive Group Theory"},{"content":"1. 旋转矩阵 对于空间中的一个向量 $\\boldsymbol{r}$，在坐标系 $\\mathcal F_1, \\mathcal F_2$ 下，用坐标描述有：\n$$ \\boldsymbol{r} = \\mathcal F_1^T \\boldsymbol{r}_1 = \\mathcal F_2^T \\boldsymbol{r}_2 $$应有：\n$$ \\boldsymbol{r}_1 = \\mathcal F_1 \\mathcal F_2^T \\boldsymbol{r}_2 $$我们令：\n$$ \\boldsymbol R_{12} = \\mathcal F_1 \\mathcal F_2^T $$各种文献中旋转矩阵的定义十分混乱，我们遵从这样一个定义：$\\boldsymbol R_{12}$，即将同一个向量丛坐标系 $\\mathcal F_2$ 变换到坐标系 $\\mathcal F_1$ 下。坐标系变换有：\n$$ \\boldsymbol R_{12} \\mathcal F_2= \\mathcal F_1 \\mathcal F_2^T \\mathcal F_2 = \\mathcal F_1 $$类似的，我们还可以用向量的坐标和基之间的关系来推导坐标变换：\n$$ \\begin{aligned} \\boldsymbol{r}_2 \u0026= \\boldsymbol{R}_{21} \\boldsymbol{r}_1\\\\ \\mathcal F_1^T \\boldsymbol{r}_1 \u0026= \\mathcal F_2^T \\boldsymbol{R}_{21} \\boldsymbol{r}_1\\\\ \\mathcal F_1^T \\boldsymbol{r}_1 \u0026= \\left(\\boldsymbol{R}_{12} \\mathcal F_2\\right)^T \\boldsymbol{r}_2\\\\ \\end{aligned} $$那么应该有：\n$$ \\mathcal F_1 = \\boldsymbol{R}_{12} \\mathcal F_2 $$1.1. 罗德里格斯公式和角轴 任意一个向量 $\\boldsymbol{r}$ 三维旋转总能通过一个单位的旋转轴 $\\boldsymbol{n}$ 和绕着该轴的旋转角度 $\\theta$ 来描述。我们不妨将向量 $\\boldsymbol{r}$ 分解为垂直旋转轴的部分和水平旋转轴的部分：\n$$ \\boldsymbol{r} = \\boldsymbol{r}_\\parallel + \\boldsymbol{r}_\\perp $$根据点乘（投影）和叉乘（公共法线）的性质，不难写出：\n$$ \\boldsymbol{r}_\\parallel = \\frac{\\boldsymbol{r} \\cdot \\boldsymbol{n}}{\\boldsymbol{n} \\cdot \\boldsymbol{n}} \\boldsymbol{n} = (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n} $$此时 $\\boldsymbol{r}_\\parallel$ 与旋转轴平行，旋转本身不影响其大小。对于垂直的部分 $\\boldsymbol{r}_\\perp$，有：\n$$ \\boldsymbol{r}_\\perp = \\boldsymbol{r} - \\boldsymbol{r}_\\parallel = \\boldsymbol{r} - (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n} $$我们需要在旋转平面（与旋转轴垂直的平面）上找到一个与 $\\boldsymbol{r}_\\perp$ 垂直的向量。这样可以构建出一个平面上的正交基，这样就可以通过平面的旋转矩阵来实现 $\\boldsymbol{r}_\\perp$ 的旋转了。这个向量 $\\boldsymbol{w}$ 可以通过叉乘得到：\n$$ \\begin{aligned} \\boldsymbol{w} \u0026= \\boldsymbol{n} \\times \\boldsymbol{r}_\\perp\\\\ \u0026= \\boldsymbol{n} \\times \\left(\\boldsymbol{r} - (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n}\\right)\\\\ \u0026= \\boldsymbol{n} \\times \\boldsymbol{r} - \\boldsymbol{n} \\times (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n}\\\\ \u0026= \\boldsymbol{n} \\times \\boldsymbol{r} \\end{aligned} $$那么旋转结果可以写出：\n$$ \\begin{aligned} \\boldsymbol{r}_\\perp' \u0026= \\cos \\theta \\boldsymbol{r}_\\perp + \\sin \\theta \\boldsymbol{w}\\\\ \u0026= \\cos \\theta \\left(\\boldsymbol{r} - (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n}\\right) + \\sin \\theta \\boldsymbol{n} \\times \\boldsymbol{r}\\\\ \u0026= \\cos \\theta \\boldsymbol{r} - \\cos \\theta (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n} + \\sin \\theta \\boldsymbol{n} \\times \\boldsymbol{r}\\\\ \\end{aligned} $$垂直分量和平行分量结合，有：\n$$ \\begin{aligned} \\boldsymbol{r}' \u0026= \\boldsymbol{r}_\\perp' + \\boldsymbol{r}_\\parallel'\\\\ \u0026= \\cos \\theta \\boldsymbol{r} - \\cos \\theta (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n} + \\sin \\theta \\boldsymbol{n} \\times \\boldsymbol{r} + (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n}\\\\ \u0026= cos \\theta \\boldsymbol{r} + (1 - \\cos \\theta) (\\boldsymbol{r} \\cdot \\boldsymbol{n}) \\boldsymbol{n} + \\sin \\theta \\boldsymbol{n} \\times \\boldsymbol{r}\\\\ \u0026= \\cos \\theta \\boldsymbol{r} + (1 - \\cos \\theta) \\boldsymbol{n}\\boldsymbol{n}^T \\boldsymbol{r} + \\sin \\theta \\boldsymbol{n} \\times \\boldsymbol{r}\\\\ \\end{aligned} $$可以总结为：\n$$ \\boldsymbol{r}' \\boldsymbol{r}^{-1} = \\boldsymbol{R}_{21} = \\cos \\theta + (1 - \\cos \\theta) \\boldsymbol{n}\\boldsymbol{n}^T + \\sin \\theta \\boldsymbol{n}^\\times $$1.2. 欧拉角 欧拉角将三维旋转分解为绕三个坐标轴的旋转，最终的旋转可以表达为三个绕坐标轴旋转的合成：\n我们可以写出绕着三个旋转轴的旋转矩阵为：\n$$ \\begin{array}{c} \\boldsymbol{R}_1 = \\begin{bmatrix} \\cos \\theta_1 \u0026 -\\sin \\theta_1 \u0026 0\\\\ \\sin \\theta_1 \u0026 \\cos \\theta_1 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \\end{bmatrix}\\\\ \\boldsymbol{R}_2 = \\begin{bmatrix} \\cos \\theta_2 \u0026 0 \u0026 -\\sin \\theta_2\\\\ 0 \u0026 1 \u0026 0\\\\ \\sin \\theta_2 \u0026 0 \u0026 \\cos \\theta_2\\\\ \\end{bmatrix}\\\\ \\boldsymbol{R}_3 = \\begin{bmatrix} 1 \u0026 0 \u0026 0\\\\ 0 \u0026 \\cos \\theta_3 \u0026 \\sin \\theta_3\\\\ 0 \u0026 -\\sin \\theta_3 \u0026 \\cos \\theta_3 \\end{bmatrix} \\end{array} $$任意一个旋转矩阵总可以表达为上面三个旋转矩阵的合成。麻烦的点在于不同的合成顺序，最终的旋转矩阵并不相同。我们给出 $1-2-3$ 顺序下的欧拉角旋转矩阵：\n$$ \\begin{align*} \\boldsymbol{R}_{21}(\\theta_{3},\\theta_{2},\\theta_{1}) \u0026= \\boldsymbol{R}_{3}(\\theta_{3})\\boldsymbol{R}_{2}(\\theta_{2})\\boldsymbol{R}_{1}(\\theta_{1})\\\\ \u0026= \\begin{bmatrix} c_{2}c_{3} \u0026 c_{1}s_{3}+s_{1}s_{2}c_{3} \u0026 s_{1}s_{3}-c_{1}s_{2}c_{3}\\\\ -c_{2}s_{3} \u0026 c_{1}c_{3}-s_{1}s_{2}s_{3} \u0026 s_{1}c_{3}+c_{1}s_{2}s_{3}\\\\ s_{2} \u0026 -s_{1}c_{2} \u0026 c_{1}c_{2} \\end{bmatrix} \\end{align*} $$不难发现旋转矩阵的奇异点：\n$$ \\boldsymbol{R}_{21}(\\theta_3, \\frac \\pi 2, \\theta_1) = \\begin{bmatrix} 0\u0026 s_(\\theta_1 + \\theta_3)\u0026 -c(\\theta_1 + \\theta_3)\\\\ 0\u0026 c(\\theta_1 + \\theta_3)\u0026 s(\\theta_1 + \\theta_3)\\\\ 1\u0026 0\u0026 0 \\end{bmatrix} $$可以看到，此时 $\\theta_1$ 和 $\\theta_3$ 表达了同一旋转。\n此外，当旋转非常小时，有 $c_\\theta \\sim 1, s_\\theta \\sim \\theta$，有：\n$$ \\boldsymbol{R}_{21} = \\begin{bmatrix} 1 \u0026 \\theta_3 \u0026 -\\theta_2 \\\\ -\\theta_3 \u0026 1 \u0026 \\theta_1 \\\\ \\theta_2 \u0026 -\\theta_1 \u0026 1 \\end{bmatrix} = 1 - \\boldsymbol{\\theta}^\\times $$2. 旋转的运动学 2.1. 角速度 假设参考系 $\\mathcal{F}_1$ 与 $\\mathcal F_2$ 之间存在着相对旋转。我们定义 $\\mathcal F_1$ 相对于 $\\mathcal F_2$ 的角速度为 $\\omega_{12} = -\\omega_{21}$。其方向为旋转的轴的方向，大小为旋转的标量速度。由于旋转速度的存在，$\\mathcal F_1$ 与 $\\mathcal F_2$ 之间观察到的运动是不同的，不妨定义在 $\\mathcal F_1$ 下的向量的时间导数为 $(·)^\\bullet$，$\\mathcal F_2$ 下向量的时间导数定义为 $(\\cdot)^\\circ$。应有在各自坐标系下观察坐标系的基向量的导数为0:\n$$ \\begin{array}{c} \\mathcal F_1^\\bullet = 0\u0026 \\mathcal F_2^\\circ = 0 \\end{array} $$且根据叉乘的几何性质，有在坐标系 $\\mathcal F_1$ 观察下，$\\mathcal F_2$ 的基向量的导数为：\n$$ \\mathcal F_2^\\bullet = \\omega_{21} \\times \\mathcal F_2 $$类似的，有：\n$$ \\mathcal F_1^\\circ = \\omega_{12} \\times \\mathcal F_1 $$对于空间中的一个向量 $\\boldsymbol{r}$，通过坐标描述，有：\n$$ \\boldsymbol{r} = \\mathcal F_1^T \\boldsymbol{r}_1 = \\mathcal F_2^T \\boldsymbol{r}_2 $$那么 $\\mathcal F_1$ 坐标下观察的 $\\boldsymbol{r}$ 的时间导数为：\n$$ \\begin{aligned} \\boldsymbol{r}^\\bullet \u0026= \\mathcal {F_1^{\\bullet}}^T \\boldsymbol{r}_1 + \\mathcal F_1^T \\boldsymbol{r}_1^\\bullet\\\\ \u0026= \\mathcal F_1^T \\boldsymbol{r}_1^\\bullet \\end{aligned} $$类似的有：\n$$ \\begin{aligned} \\boldsymbol{r}^\\circ \u0026= \\mathcal {F_2^{\\circ}}^T \\boldsymbol{r}_2 + \\mathcal F_2^T \\boldsymbol{r}_2^\\circ\\\\ \u0026= \\mathcal F_2^T \\boldsymbol{r}_2^\\circ \\end{aligned} $$需要注意，对于非向量的量（如标量、坐标等），可以理解其时间导数不受基向量的影响，满足 $\\boldsymbol{r}^\\bullet = \\boldsymbol{r}^\\circ$。那么有：\n$$ \\boldsymbol{r}^\\circ = \\mathcal F_2^T \\boldsymbol{r}_2^\\bullet $$如果我们观察到向量 $\\boldsymbol{r}$ 在 $\\mathcal F_2$ 下的速度 $\\boldsymbol{r}^\\circ$（事实上这是很常见的。我们在一个运动坐标系下观察了一个物体的运动，希望知道其相对于禁止的坐标系的运动），想知道在坐标系 $\\mathcal F_1$ 下向量 $\\boldsymbol{r}^\\bullet$ 的速度，那么应有：\n$$ \\begin{aligned} \\boldsymbol{r}^\\bullet \u0026= \\left(\\mathcal F_2^T \\boldsymbol{r}_2\\right)^\\bullet = \\mathcal {F_2^T}^\\bullet \\boldsymbol{r}_2 + \\mathcal F_2^T \\boldsymbol{r}_2^\\bullet\\\\ \u0026= \\boldsymbol{r}^\\circ + \\omega_{21} \\times \\mathcal F_2^T \\boldsymbol{r}_2\\\\ \u0026= \\boldsymbol{r}^\\circ + \\omega_{21} \\times \\boldsymbol{r} \\end{aligned} $$我们可以标记 $\\omega_{21} = \\mathcal F_2^T \\omega_{21}^2$，那么有：\n$$ \\begin{array}{cccl} \u0026\\mathcal F_1^T \\boldsymbol{r_1}^\\bullet \u0026=\u0026 \\mathcal F_2^T \\boldsymbol{r}_2^\\circ + \\mathcal F_2^T \\omega_{21}^2 \\times \\boldsymbol{r}_2\\\\ \u0026\\boldsymbol{r}_1^\\bullet \u0026=\u0026 \\boldsymbol{R}_{12} \\left(\\boldsymbol{r}_2^\\circ + \\omega_{21}^2 \\times \\boldsymbol{r}_2\\right) \\end{array} $$可以总结出；\n$$ \\boldsymbol{r}_1^\\bullet = \\boldsymbol{R}_{12} \\left(\\boldsymbol{r}_2^\\circ + \\omega_{21}^2 \\times \\boldsymbol{r}_2\\right) $$有趣的是，我们并没有对向量 $\\boldsymbol{r}$ 的性质有任何假设。希望这句话可以给读者留下来一点点印象，该性质在后面的推导中十分重要。\n2.2. 加速度 令速度 $\\boldsymbol{v} = \\boldsymbol{r}^\\circ + \\omega_{21} \\times \\boldsymbol{r}$，使用前面的性质，有：\n$$ \\begin{aligned} \\boldsymbol{r}^{\\bullet\\bullet} = \\boldsymbol{v}^\\bullet \u0026= \\boldsymbol{v}^\\circ + \\omega_{21} \\times \\boldsymbol{v}\\\\ \u0026= \\left(\\boldsymbol{r}^\\circ + \\omega_{21} \\times \\boldsymbol{r}\\right) ^\\circ + \\omega_{21} \\times \\left(\\boldsymbol{r}^\\circ + \\omega_{21} \\times \\boldsymbol{r}\\right)\\\\ \u0026= \\boldsymbol{r}^{\\circ\\circ} + \\omega_{21}^\\circ \\times \\boldsymbol{r} + \\omega_{21} \\times \\boldsymbol{r}^\\circ + \\omega_{21} \\times \\boldsymbol{r}^\\circ + \\omega_{21} \\times \\left(\\omega_{21} \\times \\boldsymbol{r}\\right)\\\\ \u0026= \\boldsymbol{r}^{\\circ\\circ} + \\omega_{21}^\\circ \\times \\boldsymbol{r} + 2\\omega_{21} \\times \\boldsymbol{r}^\\circ + \\omega_{21} \\times \\left(\\omega_{21} \\times \\boldsymbol{r}\\right) \\end{aligned} $$同样的，我们引入旋转矩阵，建立两个坐标系下的向量的导数之间的关系：\n$$ \\begin{array}{cccc} \\boldsymbol{r}^{\\bullet\\bullet} = \\mathcal F_1^T \\boldsymbol{r}_1^{\\bullet\\bullet} \u0026 \\boldsymbol{r}^{\\circ\\circ} = \\mathcal F_2^T \\boldsymbol{r}_2^{\\circ\\circ} \u0026 \\omega_{21} = \\mathcal F_2^T \\omega_{21}^2 \u0026 \\boldsymbol{r} = \\mathcal F_2^T \\boldsymbol{r}_2 \\end{array} $$有：\n$$ \\boldsymbol{r}_1^{\\bullet\\bullet} = \\boldsymbol{R}_{12}\\left( \\boldsymbol{r}_2^{\\circ\\circ}+ \\omega_{21}^2 \\times \\boldsymbol{r}_2+ 2\\omega_{21} \\times \\boldsymbol{r}_2^\\circ+ \\omega_{21} \\times \\left(\\omega_{21} \\times \\boldsymbol{r}_2\\right) \\right) $$事实上上面的形式中的每一项都有名字：\n$$ \\begin{array}{rl} \\boldsymbol{r}_2^{\\circ\\circ} \u0026 \\text{在} \\mathcal F_2 \\text{下的加速度}\\\\ 2\\omega_{21} \\times \\boldsymbol{r}_2^\\circ \u0026 \\text{科里奥利加速度}\\\\ \\omega_{21}^2 \\times \\boldsymbol{r}_2 \u0026 \\text{角加速度}\\\\ \\omega_{21} \\times \\left(\\omega_{21} \\times \\boldsymbol{r}_2\\right) \u0026 \\text{向心加速度}\\\\ \\end{array} $$2.3. 旋转矩阵的导数 对于坐标系 $\\mathcal F_1$ 与 $\\mathcal F_2$ 之间，有：\n$$ \\mathcal F_1^T = \\mathcal F_2^T \\boldsymbol{R}_{21} $$两边在 $\\mathcal F_1$ 下求时间导数：\n$$ \\begin{array}{c} 0 = {\\mathcal F_2^T}^\\bullet \\boldsymbol{R}_{21} + \\mathcal F_2^T \\boldsymbol{R}_{21}^\\bullet\\\\ 0 = \\mathcal F_2^T {\\omega_{21}^2}^\\times \\boldsymbol{R}_{21} + \\mathcal F_2^T \\boldsymbol{R}_{21}^\\bullet\\\\ 0 = \\mathcal F_2^T \\left({\\omega_{21}^2}^\\times \\boldsymbol{R}_{21} + \\boldsymbol{R}_{21}^\\bullet \\right) \\end{array} $$那么有：\n$$ \\boldsymbol{R}_{21}^\\bullet = -{\\omega_{21}^2}^\\times \\boldsymbol{R}_{21} $$这就是著名的柏松公式。柏松公式还有下面一个有用的变形：\n$$ {\\omega_{21}^2}^\\times = -\\boldsymbol{R}_{21}^\\bullet \\boldsymbol{R}_{21}^T $$通过这个形式我们可以通过计算旋转矩阵的数值微分来计算角速度。\n","permalink":"https://wangjv0812.cn/2025/06/3d-kinematics-and-dynamics/","summary":"\u003ch2 id=\"1-旋转矩阵\"\u003e1. 旋转矩阵\u003c/h2\u003e\n\u003cp\u003e对于空间中的一个向量 $\\boldsymbol{r}$，在坐标系 $\\mathcal F_1, \\mathcal F_2$ 下，用坐标描述有：\u003c/p\u003e\n$$\n\\boldsymbol{r} = \\mathcal F_1^T \\boldsymbol{r}_1 = \\mathcal F_2^T \\boldsymbol{r}_2\n$$\u003cp\u003e应有：\u003c/p\u003e\n$$\n\\boldsymbol{r}_1 = \\mathcal F_1 \\mathcal F_2^T \\boldsymbol{r}_2\n$$\u003cp\u003e我们令：\u003c/p\u003e\n$$\n\\boldsymbol R_{12} = \\mathcal F_1 \\mathcal F_2^T\n$$\u003cp\u003e各种文献中旋转矩阵的定义十分混乱，我们遵从这样一个定义：$\\boldsymbol R_{12}$，即将同一个向量丛坐标系 $\\mathcal F_2$ 变换到坐标系 $\\mathcal F_1$ 下。坐标系变换有：\u003c/p\u003e\n$$\n\\boldsymbol R_{12} \\mathcal F_2= \\mathcal F_1 \\mathcal F_2^T \\mathcal F_2 = \\mathcal F_1\n$$\u003cp\u003e类似的，我们还可以用向量的坐标和基之间的关系来推导坐标变换：\u003c/p\u003e\n$$\n\\begin{aligned}\n    \\boldsymbol{r}_2 \u0026= \\boldsymbol{R}_{21} \\boldsymbol{r}_1\\\\\n    \\mathcal F_1^T \\boldsymbol{r}_1 \u0026= \\mathcal F_2^T \\boldsymbol{R}_{21} \\boldsymbol{r}_1\\\\\n    \\mathcal F_1^T \\boldsymbol{r}_1 \u0026= \\left(\\boldsymbol{R}_{12} \\mathcal F_2\\right)^T \\boldsymbol{r}_2\\\\\n\\end{aligned}\n$$\u003cp\u003e那么应该有：\u003c/p\u003e","title":"3D Kinematics and Dynamics"},{"content":"1. Preliminary: Attention and ViT 我们先来回顾一下经典的 Transformer 结构，之后从 Transformer 的角度来理解 ViT，这样大家能更好的理解 VGGT 和 MASt3R、DUSt3R 之类工作的苦恼之处。\n1.1. Encoder and Decoder 深度学习名著 《Attention is all you need》 提出的古典派 Attention（这么说是因为由于 Transformer 的大火，Attention 机制的变种已经太多了，我们只关注最经典的架构就好，其他都大同小异）。最经典的 Transformer 致力于解决翻译问题，是一个十分经典的 nlp 问题，采用了最经典的 Encoder-Decoder 结构。\nEncoder 由 6 个完全相同的层堆叠而成，每个层由两个子层组成。第一个层负责实现 multi-head self-attention 机制；第二个层是一个简单的全连接前馈网络。为了避免在训练中出现梯度消失的问题，Transformer 在子层间采用了残差链接，之后对子层的输出做归一化（即 Add\u0026amp;Norm）那个块。因此，每个子层的输出可以表示为：\n$$ \\text{LayerNorm}(x+\\text{Sublayer}(x)) $$其中 $\\text{Sublayer}(x)$ 是每个子层具体的实现。为了方便残差链接，每层的输出和输入（包括 embedding layers）都被约定为 $d_{module} = 512$。（至少 Attention is all you need 是这样的）。\nDecoder 也是由完全相同的层堆叠而成，和 Encoder 不同的是 Decoder 的每个层也有三个子层。Decoder 的第一个子层是 masked multi-head self-attention，负责对输入的 embedding 做自注意力机制，之后将输入的 embedding 和 encoder 编码的结合结合起来。后面的层和 Encoder 一样，都是 multi-head self-attention 和前馈网络的组合。\n1.2. Attention 注意力可以被看作一个函数，将 Querry、Key、Value 映射到一个输出上。Querry、Key、Value 和输出都是向量。事实上，输出可以看作一个对 Value 的加权输出。而具体如何用 Querry 和 Key 来加权 Value，就是注意力机制了，事实上这个权描述了对应的 Value 在这个任务上的重要程度。具体而言，此处使用的是 Scaled Dot-Product Attention。\n上图左侧就是 Scaled Dot-Product Attention。我们假设输入 querry 和 key 的维度是 $d_k$，Value 的维度是 $d_v$，我们计算 querry 和所有 key 的点积并数乘以 $\\frac{1}{\\sqrt{d_k}}$，之后使用一个 softmax 来得到 value 的权重。事实上我们一堆 querry，key，value （batchsize）打包成矩阵 $Q, K, V$ 来加速计算。最终，矩阵形式的 Attention 可以写作：\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$事实上，Scaled Dot-Product Attention 并不是唯一的注意力即使（加权方法）。但是更多的 Attention mechanism，例如 点积注意力（dot-product attention）、$QKV$ 注意力（query-key-value attention）、强注意力（hard attention）、软注意力（soft attention）、自注意力（self attention）、交叉注意力（cross attention）、Luong 注意力、Bahdanau 注意力等。我们就不赘述。\n1.3. Multi-head Attention 相比一个单头注意力机制直接解决，Multi-head Attention 机制将注意力机制分成了 $h$ 个单独的头，每个头通过三个对应的科学系的先行变换层将输入的 $QKV$ 变换到这个注意力头专用的 $QKV$ 空间中，分别投影到 $d_k, d_k, d_v$ 维度上。每个头上都进行经典的 Scaled Dot-Product Attention 计算。之后将这些头的输出结果（对于每个头而言，单个querry 而言，输出结果是一个标量）堆叠起来组成一个向量（对于 batch 的角度而言就是矩阵了），再通过一个可学习的线形映射层综合这些信息。\n这些输出值会被拼接起来，然后再次进行投影，从而得到最终的值，如图2所示。\n多头注意力机制使模型能够在不同位置同时关注来自不同表示子空间的信息。如果仅使用单个注意力头，这种信息的综合利用就会受到限制。\n$$ \\begin{aligned} \\text{MultiHead}(Q, K, V) \u0026= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\\\\ \\text{where: } \\text{head}_i \u0026= \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i) \\end{aligned} $$这里的投影矩阵分别为：$W^Q_i \\in R^{d_{model} \\times d_k}$，$W^K_i \\in R^{d_{model} \\times d_k}$，$W^V_i \\in R^{d_{model} \\times d_v}$，以及 $W^O \\in R^{hd_v \\times d_{model}}$。\n1.4. Attention 的一些使用细节 需要注意的一点是，为了保证残差链接，encoder 中每一层的输出均为和输入相同维度的向量。因此事实上这张 pipline 是有些误导性的。其 $QKV$ 均为前一层的输出，事实上是通过多头注意力机制中的线形变换矩阵 $W^Q_i, W^K_i, W^V_i$ 变换到对应的 $QKV$ 的维度的。一般而言，我们称之为自注意力机制，其 $QKV$ 都是来源于同一个地方的 在 decoder 中的注意力处理与 encoder 有所不同，其 querry 来源于上一个 decoder 层的输出，keys 和 values 来源于 encoder 的输出。别忘了 Transformer 提出时是为了解决翻译问题，这样设计的好处是在自回归预测下一个词时，可以关注被翻译的句子的全部信息，同时为了保证句子的连贯性，decoder 中的注意力时 masked 的，即只能看到已经计算完成的信息。 同样地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直至并包含该位置的所有位置。为了保持自回归特性，我们需要阻止解码器中的信息向左流动。我们通过在缩放点积注意力机制中，将 softmax 输入里所有对应于非法连接的值屏蔽掉（设为负无穷）来实现这一点。 1.5. 逐位置（position-wise）的前馈网络 在每个 encoder 和 decoder 层中，除了注意力机制之外，还附加了一个全连接前馈网络。这个前馈网络会对输入的每个词操作。事实上这是一个非常简单的网络，只有两层，两层之间包含一个Relu。可以写作：\n$$ \\text{FNN}(x) = max(0, xW_1 + b_1)W_2 + b_2 $$尽管不同位置上的线性变换是相同的，但不同层之间使用的参数是不同的。我们可以将其视为两次核大小为 $1$ 的卷积操作。输入和输出的维度为 $d_{model}=512$，而中间层的维度为 $d_{ff}=2048$。\n需要简单说明的是，这里的逐位置指的就是针对每个位置计算的 Value，都有一个对应的前馈网络。\n1.6. 位置编码 因为 Transformer 中不包含任何循环或者卷积结构，任意顺序的输入对于 Transformer 而言都是平等的。其网络结构天然的不擅长捕捉序列间的相对位置关系。事实上也就因如此，Transformer 对并行化训练十分友好（以为各个位置的输入都是平等的，或者说相同的，先算谁后算谁都是一个道理）。我们必须给序列中引入一些标记，让其体现出输入的相对或绝对位置信息。为此，我们在编码器和解码器堆叠层底部的输入嵌入中添加了“位置编码”。位置编码与嵌入的维度相同，均为 $d_{model}$，这样二者就可以直接相加。\n在这项工作中，我们使用不同频率的正弦和余弦函数：\n$$ PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)\\\\ PE_{(pos, 2i + 1)} = \\cos\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right) $$其中 $pos$ 表示位置，$i$ 表示维度。也就是说，位置编码的每个维度都对应一个正弦曲线。波长形成一个从 $2\\pi$ 到 $10000\\cdot 2\\pi$ 的等比数列。我们选择这个函数，是因为我们推测它能让模型轻松地通过相对位置来学习注意力机制，这是因为对于任何固定的偏移量 $k$，$PE_{pos + k}$ 都可以表示为 $PE_{pos}$ 的线性函数。\n1.7. 复杂度分析 Layer Type Complexity per Layer Sequential Operations Maximum Path Length Self - Attention $O(n^2 \\cdot d)$ $O(1)$ $O(1)$ Recurrent $O(n \\cdot d^2)$ $O(n)$ $O(n)$ Convolutional $O(k \\cdot n \\cdot d^2)$ $O(1)$ $O(\\log_k(n))$ Self - Attention (restricted) $O(r \\cdot n \\cdot d)$ $O(1)$ $O(\\frac{n}{r})$ 事实上，Transformer 的改进可太多了，例如应对巨量文本输入的稀疏注意力、对注意力计算结果缓存（KV Cache、Flash Attention）、针对位置编码的改进（RoPE）等等等等。要全部讨论这些可能需要不止一次 Seminar 我们后面找机会再讨论吧。\n1.8. Vision Transformer ViT将输入图片分为多个patch（16x16），再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。但是因为对图片分类，因此在输入序列中加入一个特殊的token，该token对应的输出即为最后的类别预测。按照上面的流程，ViT 可以分为下面几个步骤：\nPath embedding：例如输入照片的尺寸为 $244 \\times 244$，之后将图片分为大小为 $16 \\times 16$ 的 Patch。因此每个图片会产生 $\\frac{244}{16} \\times \\frac{244}{16} = 196$ patch。按照传统 Transformer 的话来说，就是输入序列的长度为 196。每个 patch 或者说 embedding 的维度为 $16\\times 16\\times 3 = 768$，线形投影层的维度为 $768 \\times N$。那么通过线形投影层变换后的维度为 $196\\times N$。即 $196$ 个 token，每个 token 的维度为 $N$。但是还没完，最终还需要添加一个特殊的token cls 标记序列结束了。所以此时 attention mecanism 输入的维度为 $197\\times N$。不难发现，此时已将一个图像处理问题转换为一个 seq2seq 问题。 Transformer 的对序列相对/绝对关系的缺点（或者说是优点）依然存在，我们需要给每个图片的 patch 编码。此处的方法和 Transformer 完全相同。对于每个 patch 生成一个只与其位置有关，维度为 $N$ 的编码，直接加到 patch embedding 上。这样就能保留 patch 的位置信息了。 multihead attention。对于多个头的情况（例如 12 个头），ViT 会通过一个线形映射将输入的 embedding 降维映射到维度为 $N // 12$ 的 QKV 上，每个头处理后再将输出拼接起来，还原为原始的维度 $N$。之后通过一个先变大后变小的 MLP。最终每层输出的维度依然为 $197\\times N$ $$ \\begin{array}{ll} z_{0}=\\left[x_{class } ; x_{p}^{1} E ; x_{p}^{2} E ; \\cdots ; x_{p}^{N} E\\right]+E_{pos}, \u0026E \\in \\mathbb{R}^{(P^{2} \\cdot C) \\times D}, E_{pos} \\in \\mathbb{R}^{(N + 1) \\times D}\\\\ z_{\\ell}'=\\text{MSA}(\\text{LN}(z_{\\ell - 1})) + z_{\\ell - 1}, \u0026\\quad \\ell = 1 \\cdots L\\\\ z_{\\ell}=\\text{MLP}(\\text{LN}(z_{\\ell}')) + z_{\\ell}', \u0026\\quad \\ell = 1 \\cdots L\\\\ y=\\text{LN}(z_{L}^{0}) \\end{array} $$读到这里你可能会想，为什么 ViT 这样一个显然的思路之前没人做呢？事实上当网络的规模不够大时，ViT 的效果是不不过 ResNet 或者 CNN 的。但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。\nCNN具有两种归纳偏置：\n一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征 一种是平移不变性（translation equivariance），$f(g(x)) = g(f(x))$ ，其中 $g$ 代表卷积操作，$f$ 代表平移操作。 当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。而当数据量和网络规模变大时，Transformer 的优势才体现出来。ViT 则相信 Transformer 的表达能力，即使没有归纳偏置，也能从数据中学习到足够的先验知识。ViT 通过大规模数据集的训练，获得了比 CNN 更好的性能。\n2. VGGT Introduction VGGT 是一个大型的前馈网络，通过输入一张照片到多张照片（甚至几百张）作为输入；直接推理出一个场景几乎所有的三维特征，包括相机矩阵、点图（point map）、深度图和三维点跟踪（3D Point Tracks）。传统的三维重建方法通常基于视觉几何，使用例如 BA 之类迭代优化的方法。机器学习常常扮演着重要的补充角色，用于处理仅靠几何方法无法解决的任务，比如特征匹配和单目出深度。二者的融合越来越紧密，如今像 VGGSfM 这样 SOTA 的 SfM 方法，通过可微光束法平差将机器学习和视觉几何进行端到端的结合。即便如此，视觉几何在三维重建中仍起着主要作用，这增加了问题的复杂性和计算成本。\n本文作者发现，没有必要为三维重建任务设计一个专用的网络结构，VGGT 直接使用了标注你的大型 Transformer，没有使用特殊的三维或其他归纳偏置，而是用一种通用的网络结构在极大量的有三维标注的公开图像数据集上训练。和其他针对三维任务的大型神经网络，例如 DepthAnything、MoGe、LRM 等只针对一个特定的三维任务（例如单目深度估计）。VGGT 使用一个共享的骨干网络来预测所有感兴趣的三维特征，在推理过程中可以从预测出的深度图和相机矩阵计算出 pointmap，与直接使用专门的 pointmap 预测头相比，能获得更高的精度。\n总而言之，这篇文章的贡献如下\n引入了视觉几何基础变换器（VGGT），这是一个大型的前馈 Transformer。给定一个场景的单张、几张甚至数百张图像，它能够在数秒内预测出该场景的所有关键三维属性，包括相机的内参和外参、点云图、深度图以及三维点轨迹。 VGGT 的预测结果可直接使用，其竞争力很强，并且通常优于那些使用缓慢的后处理优化技术的最先进方法的预测结果。 当进一步结合光束法平差（BA）后处理时，即使与那些专门针对某一类三维任务的方法相比，VGGT 也全面取得了领先的成果，并且常常能大幅提升任务处理的质量。 3. Method 3.1. 问题定义和符号约定 对于一个输入网络侧图片序列 $(I_i)^N_{i = 1}$，表示 $N$ 张 RGB 图片 $I_i \\in \\mathbb R^{3\\times H \\times W}$，我们默认这些图片是对同一个场景的观测。VGGT 使用的 Transformer 认为也是一个映射，将这个图片序列映射到这个三维场景的描述，对于每一帧，这个映射可以写作：\n$$ f\\left(\\left(I_{i}\\right)_{i = 1}^{N}\\right)=\\left(g_{i}, D_{i}, P_{i}, T_{i}\\right)_{i = 1}^{N} $$这个 Transformer 将每张图像 $I_i$ 映射为其相机参数 $g_i \\in \\mathbb{R}^9$，包含内参和外参；深度图 $D_i \\in \\mathbb{R}^{H×W}$；其点图 $P_i \\in \\mathbb{R}^{3×H×W}$；以及一个用于点匹配的的 $C$ 维特征 $T_i \\in \\mathbb{R}^{C×H×W}$。接下来我们将解释这些量是如何定义的。\n相机参数 $g_i$ 定义为：$g = [\\bf q, \\bf t, \\bf f]$，是将描述旋转的四元数 $\\bf q \\in \\mathbb R^4$，描述平移的向量 $\\bf t \\in \\mathbb R^3$ 和焦距 $\\bf f \\in \\mathbb R^2$ 组合在一起的。此处我们假设了相机为针孔模型。\n我们定义图片 $I_i$ 的定义域为 $\\mathcal I(I_i) = \\{1, 2, \\dots, H\\}\\times \\{1, 2, \\dots, W\\}$，是所有可能的像素的集合。深度图 $D_{i}$ 将每个像素位置 $y \\in I(I_{i})$ 与其对应的深度值 $D_{i}(y) \\in \\mathbb{R}^{+}$ 相关联，该深度值是从第 $i$ 个相机观测到的。类似的，pointmap $P_{i}$ 将每个像素与其对应的三维场景点 $P_{i}(y) \\in \\mathbb{R}^{3}$ 相关联。与 DUSt3R 一致，三维点 $P_{i}(y)$ 的坐标系为第一个相机 $g_{1}$ 的坐标系中。换句话说，我们认为第一个相机 $g_{1}$ 的坐标系为世界坐标系\n最后，关于关键点跟踪（point tracking）。对于图像 $I_q$ 中的一个固定点 $y_q$，网络将输出一个跟踪 $\\mathcal T^\\star (y_q) = (y_i)^N_{i = 1}$，即其他所有图片上的对应点。需要注意的是，网络并不会直接输出轨迹，而是输出用于跟踪的特征 $T_i \\in \\mathbb{R}^{C \\times H \\times W}$。具体的点的跟踪将会由一个独立的网络实现：\n$$ T \\left( (y_j)_{j=1}^M, (T_i)_{i=1}^N \\right) = \\left( (\\hat{y}_{j,i})_{i=1}^N \\right)_{j=1}^M $$这个网络 $\\mathcal T$ 输入跟踪点 $y_q$ 和稠密的跟踪 feather $T_i$，计算对应的其他照片中的对应点。跟踪网络 $\\mathcal T$ 和主网络 $f$ 是一起端到端训练的。\n在推理和训练时，除了第一张照片的输入需要特别注意意外，其他照片可以以任意顺序输入。因为第一张照片将作为参考坐标系。VGGT 的网络结构在设计时特意考虑了这一点，让除了第一张照片以外的任意输入都是等价的。在后面网络结构的介绍中我们将看到这一点。\n事实上，VGGT 的网络具有 \u0026ldquo;over-complete prediction\u0026rdquo; 的性质。换句话说，它的有些预测是相关的，例如预测的相机参数 $\\bf g$ 可以从不变的 pointmap $P$ 计算出。此外，深度图还可以从 pointmap 和相机矩阵推出。事实上正是这种预测结果之间的闭式关系让我们实现预测量之间的相互监督，实现了显著的心梗提升。事实上与直接专门预测 pointmap 的网络相比，结合相机参数和深度图的相互监督可以提升 pointmap 的预测精度。\n3.2. 特征提取骨干网络 事实上，VGGT 通过一个巨大的 Transformer （1.2B）实现了映射 $f$ 的功能：\n$$ f\\left(\\left(I_{i}\\right)_{i = 1}^{N}\\right)=\\left(g_{i}, D_{i}, P_{i}, T_{i}\\right)_{i = 1}^{N} $$首先，VGGT 先通过 DINO 将图片 $I$ 分割为一组 $K$ 个 token $t^I \\in \\mathbb R^{K\\times C}$。之后，所有帧的 token 组的并 $t^I = \\cup^N_{i = 1}\\{t_i^I\\}$ 通过骨干网络来处理，进行全局自注意力。\n3.3. 交替注意力机制 VGGT 引入了交替注意力机制对经典的 Transformer 的注意力机制做了调整，使 Transformer 能够以交替的方式聚焦于每个帧内和全局的信息。具体而言，帧级的注意力机制单独处理每个帧内的 token $t_k^I$，而全局注意力则将所有帧的 token 拼接在一起 $t_I$ 处理。这种设计在整合不同图像间信息与归一化每个图像内令牌的激活值之间取得了平衡。本文中使用 $L=24$ 层的全局-局部注意力机制。\n具体的 Alternative Attention Transformer 的机制论文中没有说的非常明白，我们不妨回到论文，看看具体是怎么做的。首先是 global attention 和 frame attention 的定义，我们此处不贴具体代码了，只说结论，global attention 和 frame attention 的 block 是完全相同的。在具体处理时：\nself.aa_order=[\u0026#34;frame\u0026#34;, \u0026#34;global\u0026#34;] ······· for _ in range(self.aa_block_num): for attn_type in self.aa_order: if attn_type == \u0026#34;frame\u0026#34;: tokens, frame_idx, frame_intermediates = self._process_frame_attention( tokens, B, S, P, C, frame_idx, pos=pos ) elif attn_type == \u0026#34;global\u0026#34;: tokens, global_idx, global_intermediates = self._process_global_attention( tokens, B, S, P, C, global_idx, pos=pos ) else: raise ValueError(f\u0026#34;Unknown attention type: {attn_type}\u0026#34;) for i in range(len(frame_intermediates)): # concat frame and global intermediates, [B x S x P x 2C] concat_inter = torch.cat([frame_intermediates[i], global_intermediates[i]], dim=-1) output_list.append(concat_inter) 其中 B 表示 batch size，即批量大小，S 表示 sequence length，即图片序列长度。P 表示每张照片的 patch 数量，C 表示每个 patch 的维度。具体的 frame attention 和 global attention 的实现细节如下：\ndef _process_frame_attention(self, tokens, B, S, P, C, frame_idx, pos=None): \u0026#34;\u0026#34;\u0026#34; Process frame attention blocks. We keep tokens in shape (B*S, P, C). \u0026#34;\u0026#34;\u0026#34; # If needed, reshape tokens or positions: if tokens.shape != (B * S, P, C): tokens = tokens.view(B, S, P, C).view(B * S, P, C) if pos is not None and pos.shape != (B * S, P, 2): pos = pos.view(B, S, P, 2).view(B * S, P, 2) intermediates = [] # by default, self.aa_block_size=1, which processes one block at a time for _ in range(self.aa_block_size): tokens = self.frame_blocks[frame_idx](tokens, pos=pos) frame_idx += 1 intermediates.append(tokens.view(B, S, P, C)) return tokens, frame_idx, intermediates def _process_global_attention(self, tokens, B, S, P, C, global_idx, pos=None): \u0026#34;\u0026#34;\u0026#34; Process global attention blocks. We keep tokens in shape (B, S*P, C). \u0026#34;\u0026#34;\u0026#34; if tokens.shape != (B, S * P, C): tokens = tokens.view(B, S, P, C).view(B, S * P, C) if pos is not None and pos.shape != (B, S * P, 2): pos = pos.view(B, S, P, 2).view(B, S * P, 2) ········ 和上一个函数完全相同，就不赘述了 return tokens, global_idx, intermediates 这样，我们可以缕出 alternative attention 的完整逻辑。实际上不论是 frame attention 还是 global attention，都是一个完整的 transformer block。对于 VGGT 的每一层，输入的 token 先通过 frame attention 处理，之后再通过 global attention 处理。（此处和论文有所不同，论文 pipline 是先经过 global attention 后 frame attention），以此往复 24 次。\n事实上 global attention 和 frame attention的实现也十分简单。global attention 实际上是将所有的帧的 tokne 堆叠在一起处理做自注意力，而 frame attention 则是将帧当作 batch 处理，也就是代码中的：\ntokens = tokens.view(B, S, P, C).view(B * S, P, C) tokens = tokens.view(B, S, P, C).view(B, S * P, C) 3.4. 预测头 下面我们要讨论 VGGT 是如何通过 Transformer 和预测头实现这神乎其神的功能。\n首先，VGGT 在每张图片用 DINO pachify 的 token $t_i^I$ 的基础上，给每个图片都添加了一个额外的相机 token $t_i^g\\in \\mathbb R^{1\\times C^\\prime}$ 和 4 个寄存器 token $t_i^R\\in \\mathbb R^{4\\times C^\\prime}$，由此，每个图片的 token 可以表达为：$\\{t_i^g, t_i^R, t_i^I\\}$。这些添加的 5个 token 是网络的可学习参数。\n这里原论文没有说明白的一点是，事实上，我们只添加了两组 camera token 和 register token。第一组 camera token 是第一帧专项的，记做：\n$$ \\begin{array}{l} t_1^g = \\overline{t}^g\u0026 t_1^R = \\overline{t}^R \\end{array} $$其他所有帧公用另外一组 camera token 和 register token，记做：\n$$ \\begin{array}{cc} \\begin{aligned} t_i^g \u0026= \\overline{\\overline{t}}^g\\\\ t_i^R \u0026= \\overline{\\overline{t}}^R \\end{aligned} \u0026\\forall i \\in \\{2,3,\\dots,N\\} \\\\ \\end{array} $$camera token 和 register token 经过网络预测记做：\n$$ \\begin{aligned} f(t_i^g) \u0026= \\hat{t_i^g}\\\\ f(t_i^R) \u0026= \\hat{t_i^R}\\\\ f(t_i^I) \u0026= \\hat{t_i^I}\\\\ \\end{aligned} $$在预测相机参数时，我们只使 $\\hat{t_i^g}$，register token 被抛弃了。\n3.4.1 坐标系 如前所述，我们在第一台相机 $g_{1}$ 的坐标系中预测相机参数、点图和深度图。因此，第一台相机的输出相机外参被设置为单位矩阵，即第一个旋转四元数为 $q_{1}=[0,0,0,1]$，第一个平移向量为 $t_{1}=[0,0,0]$。回顾可知，特殊的相机令牌和寄存器令牌 $t_{1}^{g}:=\\overline{t}^{g}$、$t_{1}^{R}:=\\overline{t}^{R}$ 使 Transformer 能够识别第一台相机。\n3.4.2 相机参数预测头 VGGT 通过刚才添加的相机 token 预测出的 token $(\\hat t_u^g)_{i=1}^N$ 来预测相机参数 $(\\hat g^i)_{i=1}^N$。$(\\hat t_u^g)_{i=1}^N$ 通过连接 4 个额外的连接了线形层的自注意力层进行预测，这构成了相机内参的预测头。\n事实上使用的网络比论文描述的稍微复杂不少。相机参数预测头会多轮优化预测的结果。大致的 pipline 可以被描述为：\n当一个新的迭代开始时，先会使用一个全为 0 的位置编码，通过一个线形映射投影到 token 的维度 如果这不是第一轮，之前已经有了迭代结构，则会用上一轮的优化结果映射到 token 的维度（当然这里需要detach，避免梯度回传），叫做 module_input。 之后会通过一个 SiLU 网络和一个很浅的全连接调整维度，将 module_input映射到 shift, scale和 gate。 之后用 shift，scale 对归一化的 $(\\hat g^i)_{i=1}^N$ 做仿射变换，用于体现之前优化的结构 $$ \\text{pose\\_tokens\\_modulated} = (1 + scale) * (\\hat g^i)_{i=1}^N + shift $$ 对 pose_tokens_modulated 和 $(\\hat g^i)_{i=1}^N$ 残差连接，之后进行 transformer 最后进行全连接调整维度和预测 最终将调整的结果加在上一轮迭代结果上 事实上，这样做是因为 Transformer 更擅长与处理上下文语意，而相机参数恰好是一个尺度敏感的参数。因此此处用了一个 学习的放射变换来调整。\n3.4.3 稠密预测头 输出的图像 token $\\hat t^I_i$ 用于预测稠密输出的深度图 $D_i$、点图 $P_i$ 和用于跟踪的特征 $T_i$。更具体的说，VGGT 首先将 $\\hat t_i^I$ 通过 DPT 层转换为较为稠密的特征图 $F_i \\in \\mathbb R^{C^{\\prime \\prime} \\times H\\times W}$，之后 $F_i$ 通过一个 $3 \\times 3$ 的卷积层映射到对应的深度图 $D_i$ 和 $P_i$。此外，DPT 还输出了稠密预测特征 $T_i \\in \\mathbb R^{C\\times H \\times W}$，将输入后面的跟踪头。\n此外，VGGT 还预测出了每个深度图和 pointmap 的不确定性图 $\\Sigma_i^D, \\Sigma_i^P \\in \\mathbb R_+^{H\\times W}$。这个不确定性图和 MASt3R 相同，用于加权的计算损失。\n3.4.4 跟踪头 跟踪网络 $\\mathcal T$ 采用了 CoTracker2 架构。对于一个等待 track 的三维点集 $y_j$ 和这个点对应的图片提取出的特征 $T_i$，跟踪网络 $\\mathcal T$ 将会计算出所有的照片中 $y_j$ 对应的二维点。\n$$ T\\big((y_j)_{j = 1}^M, (T_i)_{i = 1}^N\\big)=((\\hat{y}_{j,i})_{i = 1}^N)_{j = 1}^M $$通过上面的形式可以看出相对于三维点而言，骨干网络提取出的特征 $T_i$ 可能是不对齐的，因此首先我们要在每个查询点 $y_j$ 上对 $T_i$ 双线形差值。然后将该特征与其他特征图 $T_i$ 进行关联，得到一组关联图。接着通过自注意力机制对关联图处理，以预测出最终对应的二维点 $\\hat y_i$。\n4. Training 训练时，VGGT 使用了如下的 lossfunction：\n$$ \\mathcal L = \\mathcal L_{\\text{camera}} + \\mathcal L_{\\text{pmap}} + \\lambda\\mathcal L_{\\text{track}} $$有趣的是，VGGT 训练时发现 $\\mathcal L_{\\text{camera}}$ 和 $\\mathcal L_{\\text{pmap}}$ 的尺度是相似的，因此不需要通过缩放因子来调整。但是 $\\mathcal L_{\\text{track}}$ 不同，需要调整。一般而言有 $\\lambda = 0.05$。\n4.1. 相机损失 相机损失 $\\mathcal L_{\\text{camera}}$ 用于监督相机参数 $\\hat{\\bf g}$:\n$$ \\mathcal L_{\\text{camera}} = \\sum_{i=1}^N \\|\\hat{\\bf g}_i - \\bf g_i \\|_\\epsilon $$其中 $\\| \\cdot \\|_\\epsilon$ 是 huber 损失。\n4.2. 深度和点图损失 深度损失 $\\mathcal L_{\\text{depth}}$ 和 DUSt3R 相同，使用不确定性图加权了预测深度图 $\\hat D_i$ 与 真值 $D_i$ 之间的残差。与 DUSt3R 不同的是，VGGT 还引入了单目深度估计中的图像梯度项，损失可以被写作：\n$$ L_{\\text{depth}} = \\sum_{i=1}^{N} \\|\\Sigma_i^D \\odot (\\hat{D}_i - D_i)\\| + \\|\\Sigma_i^D \\odot (\\nabla \\hat{D}_i - \\nabla D_i)\\| - \\alpha \\log \\Sigma_i^D $$其中 $\\odot$ 表示通道广播逐元素乘积（Channel-broadcast element-wise product）（看着这个名字很吓人，实际上就是数乘，因为 $\\Sigma_i^D\\in \\mathbb R^{H\\times W}$，而 $\\nabla \\hat{D}_i\\in \\mathbb R^{H\\times W\\times 2}，需要乘到向量上$ ）。$\\alpha$ 是一个超参数。\npoint map 的损失和深度损失很类似，可以写作：\n$$ L_{\\text{pmap}} = \\sum_{i=1}^{N} \\|\\Sigma_i^P \\odot (\\hat{P}_i - P_i)\\| + \\|\\Sigma_i^P \\odot (\\nabla \\hat{P}_i - \\nabla P_i)\\| - \\alpha \\log \\Sigma_i^P $$4.3. 跟踪损失 跟踪损失可以写作：\n$$ L_{\\text{track}} = \\sum_{j=1}^{M} \\sum_{i=1}^{N} \\|y_{j,i} - \\hat{y}_{j,i}\\| $$其中外层求和是遍历查询图像 $I_q$ 所有的查询点 $y_j$，内层循环遍历所有可能的照片。$y_{j,i}$ 为在图像 $i$ 中的真实点。$\\hat{y}_{j,i}$ 为跟踪模块的预测结果 $T((y_j)_{j=1}^{M}, (T_i)_{i=1}^{N})$。\n4.4. 真值坐标系归一化 如果我们缩放一个场景或者改变其参考系，这个场景的照片事实上不会有任何变换。这意味着改变参考系和缩放在三维重建上是正常的，或者说等价的，换句话说就是三维重建的尺度不一致性。这种不同图片预测出的结果的尺度不一致性是无法自然的通过损失来监督的，因此要对真值做归一化。这个归一化事实上分为两步：\n将所有照片的坐标系转换到第一个照片的坐标系中 计算该场景中所有三维点在异地个照片的坐标系中，距离坐标原点的平均欧几里得距离。用这个平均距离来归一化相机外参 $t$ 深度图 $D$ 和点图 $P$。 一个很有趣的细节，或者说和 M/DUSt3R 的不同之处是，VGGT 不会对预测结果归一化，而是迫使模型从数据集学习这种归一化的方式\n4.5. 训练细节 实现细节。默认情况下，我们分别使用 $L = 24$ 层的全局注意力和帧级注意力。该模型总共约有 1.2B 参数。我们通过 AdamW 优化器对训练损失进行 16 万轮优化来训练模型。我们使用余弦学习率调度器，峰值学习率为 0.0002，预热 8000 轮。对于每个批次，我们从随机选取的训练场景中随机采样 2 - 24 帧。输入的帧、深度图和点图被调整为最大尺寸为 518 像素。宽高比在 0.33 到 1.0 之间随机设定。我们还对帧随机应用颜色抖动、高斯模糊和灰度增强等操作。训练在 64 块 A100 GPU 上进行，持续九天。我们采用梯度范数裁剪，阈值设为 1.0，以确保训练的稳定性。我们利用 bfloat16 精度和梯度检查点技术来提高 GPU 内存和计算效率。\n5. Experiments References Attention is all you need\nViT（Vision Transformer）解析\nAN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nwiki vision transformer\nVGGT: Visual Geometry Grounded Transformer\nVGGT opensource coed in github\nVision Transformer for Dense Prediction\nAppendix A. huber loss $$ L_\\delta(a) = \\begin{cases} \\frac{1}{2} a^2, \u0026 \\text{if } |a| \\leq \\delta \\\\ \\delta(|a| - \\frac{1}{2} \\delta), \u0026 \\text{if } |a| \u003e \\delta \\end{cases} $$其中：\n$a = y - \\hat{y}$ 表示预测值和真实值的差； $\\delta$ 是一个超参数，控制“哪些差值被当作离群点”。 huber loss 在误差比较小的时候 $|a| \\le \\delta$ 表现的像是 L2 Loss，当误差比较大的时候 $|a| \u003e \\delta$ 表现的像是 L1 Loss，而对异常值具有更强的鲁棒性。\n","permalink":"https://wangjv0812.cn/2025/04/from-transformer-to-vggt/","summary":"\u003ch2 id=\"1-preliminary-attention-and-vit\"\u003e1. Preliminary: Attention and ViT\u003c/h2\u003e\n\u003cp\u003e我们先来回顾一下经典的 Transformer 结构，之后从 Transformer 的角度来理解 ViT，这样大家能更好的理解 VGGT 和 MASt3R、DUSt3R 之类工作的苦恼之处。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"VGGT Attention pipline\" loading=\"lazy\" src=\"/2025/04/from-transformer-to-vggt/Images/VGGT%20Attention%20pipline.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"11-encoder-and-decoder\"\u003e1.1. Encoder and Decoder\u003c/h3\u003e\n\u003cp\u003e深度学习名著 《\u003ca href=\"https://arxiv.org/pdf/1706.03762\"\u003eAttention is all you need\u003c/a\u003e》 提出的古典派 Attention（这么说是因为由于 Transformer 的大火，Attention 机制的变种已经太多了，我们只关注最经典的架构就好，其他都大同小异）。最经典的 Transformer 致力于解决翻译问题，是一个十分经典的 nlp 问题，采用了最经典的 Encoder-Decoder 结构。\u003c/p\u003e\n\u003cp\u003eEncoder 由 6 个完全相同的层堆叠而成，每个层由两个子层组成。第一个层负责实现 multi-head self-attention 机制；第二个层是一个简单的全连接前馈网络。为了避免在训练中出现梯度消失的问题，Transformer 在子层间采用了残差链接，之后对子层的输出做归一化（即 Add\u0026amp;Norm）那个块。因此，每个子层的输出可以表示为：\u003c/p\u003e\n$$\n\\text{LayerNorm}(x+\\text{Sublayer}(x))\n$$\u003cp\u003e其中 $\\text{Sublayer}(x)$ 是每个子层具体的实现。为了方便残差链接，每层的输出和输入（包括 embedding layers）都被约定为 $d_{module} = 512$。（至少 Attention is all you need 是这样的）。\u003c/p\u003e\n\u003cp\u003eDecoder 也是由完全相同的层堆叠而成，和 Encoder 不同的是 Decoder 的每个层也有三个子层。Decoder 的第一个子层是 masked multi-head self-attention，负责对输入的 embedding 做自注意力机制，之后将输入的 embedding 和 encoder 编码的结合结合起来。后面的层和 Encoder 一样，都是 multi-head self-attention 和前馈网络的组合。\u003c/p\u003e","title":"From Transformer to VGGT"},{"content":"1. DUSt3R 1.1. Introduction 一般而言，现代的 MVS 和 SFM 的流程总是可以总结为以下几个子问题\n特征点匹配 寻找本质矩阵 对点进行三角测量 对场景进行稀疏重建 估计相机参数， 密集重建 但是在这个复杂的过程中，每个子问题都对原始问题做了简化，无法完美解决，为后面的步骤引入了噪声，从而导致整个系统显的“精致而脆弱”。在这方面，每个子问题之间缺乏沟通就很能说明问题：如果能将这些缓解紧耦合到一起，将噪声统一的，全局的考虑，可以很大程度上解决应为过度简化和解耦导致的种种问题。此外，这个流程中的关键步骤很脆弱，在很多情况下容易出错。例如，很多 SFM 方法都依赖于相机参数的估计，但是如果遇到观察比较少、非漫反射表面或者相机姿态运动较为单一时，相机参数估计可能失效，导致整个 SFM 过程都会失效。归根结底：一个多视图立体视觉（MVS）算法的性能仅取决于输入图像和相机参数的质量\n事实上，单张图或者多张图哦通过深度学习的方式提取深度并不罕有。但是在不引入额外的先验信息时，这个问题往往是不适定的，所以这些方法利用神经网络从大量数据中学习巨量的三维先验知识来解决模糊性问题。这些方法可以分为两类。第一类利用类别级别的物体先验知识，事实上 DreamFusion 就属于这类工作，可以从单张照片或者一句自然语言描述生成三纬结构。另一种与 DUSt3R 较为类似，系统的学习一般的场景来实现单目深度估计。但是一般而言，例如 SuperGlue 之类，在训练和推理过程中，都没有显然的引入三维结构的信息，也没有扔掉相机矩阵的桎梏。可以说，DUSt3R 是一种基于深度学习的 ALL in One 的深度估计方法，入了点图（Point Map）表示，使网络能够在规范框架中预测三维形状。\n1.2. Method and forward 1.2.1. Ponit Map 接下来，我们将图片中每个像素对应的三维点构成的集合称为 点图（point map） $X \\in R^{W×H×3}$。与分辨率为 $W×H$的对应RGB图像 $I$相关联，点图 $X$ 在图像像素与三维点之间形成一一映射，即对于所有像素坐标 $(i, j) \\in \\{1...W\\}×\\{1...H\\}$，都有 $I_{i,j} \\leftrightarrow X_{i,j}$。此处每个像素点对应于一个三维点实事丧引入了一个简化假设，即假设观测的场景全部是不透明且漫反射的，不存在透过某个物体并观察到另一个物体的情况。\n1.2.2. 相机和场景 相机与场景。给定相机内参 $K \\in \\mathbb{R}^{3 ×3}$ ，所观测场景的点图 $X$ 可以直接从真实深度图：\n$$ D \\in \\mathbb{R}^{W ×H} $$中获取，其公式为\n$$ \\begin{aligned} X_{i, j} \u0026= K^{-1}D_{i,j}[i , j , 1]^{\\top}\\\\ \\end{aligned} $$其中， $X$ 是在相机坐标系中表示的。接下来，我们将在相机 $m$ 的坐标系中表示的、来自相机 $\\pi$ 的点图 $X^{n}$ 记为 $X^{n, m}$，表示第 $n$ 帧在 第 $m$ 帧的坐标系下的点图为 ：\n$$ X^{n, m}=P_{m} P_{n}^{-1} h\\left(X^{n}\\right) $$其中 $P_{m}$、$P_{n} \\in \\mathbb{R}^{3 ×4}$分别是图像$n$和图像$m$从世界坐标系到相机坐标系的位姿矩阵，$h:(x, y, z) \\to (x, y, z, 1)$ 是从笛卡尔坐标到齐次坐标的映射。\n1.2.3. 网络结构 DUSt3R 训练了一个用于预测三维场景点的回归网络 $\\bm F$。该神经网络的输入为两张 RGB 图像 $I_1, I_2 \\in \\mathbb{R}^{W×H×3}$，输出为两张图片对应的点图（point map）$X_{1,1}, X_{2,1} \\in \\mathbb{R}^{W×H×3}$。 这两张点图的坐标原点固定在 $I_1$ 的坐标原点上，与 $I_1$ 的相机坐标系重合。此外还会预测出对每个像素点的三维坐标预测的置信度 $C_{1,1}, C_{2,1} \\in \\mathbb{R}^{W×H}$。\n如上所示，它由两个相同的分支组成（每个图像对应一个分支），每个分支都包含一个图像编码器、一个解码器和一个回归头。首先，两张输入图像由同一个权重共享的Vision Transformer（ViT）编码器以孪生网络的方式进行编码，得到两个标记表示 $F^{1}$ 和 $F^{2}$：\n$$ \\begin{aligned} F^{1}=\\text{Encoder}(I^{1})\\\\ F^{2}=\\text{Encoder}(I^{2}) \\end{aligned} $$因此，每个解码器模块会按顺序执行自注意力机制，然后执行交叉注意力机制，最后将标记输入到 MLP 中。在 Decoder 中的交叉注意力机制是网络能实现点三维点重建的关键。\n$$ \\begin{aligned} G_{i}^{1} \u0026= \\text{DecoderBlock}_{i}^{1}(G_{i - 1}^{1}, G_{i - 1}^{2})\\\\ G_{i}^{2} \u0026= \\text{DecoderBlock}_{i}^{2}(G_{i - 1}^{2}, G_{i - 1}^{1}) \\end{aligned} $$对于有 $B$ 个模块的解码器， $i = 1, ..., B$ ，并使用编码器标记 $G_{0}^{1}:=F^{1}$ 和 $G_{0}^{2}:=F^{2}$ 进行初始化。这里， $\\text{DecoderBlock}_{i}^{v}(G^{1}, G^{2})$ 表示分支 $v \\in \\{1, 2\\}$ 中的第 $i$ 个模块， $G^{1}$ 和 $G^{2}$ 是输入标记，其中 $G^{2}$ 是来自另一个分支的标记。最后，在每个分支中，一个独立的回归头会获取解码器的标记集，并输出一个点图和一个相关的置信度图：\n$$ \\begin{aligned} X^{1,1}, C^{1,1} \u0026= \\text{Head}^{1}(G_{0}^{1}, ..., G_{B}^{1})\\\\ X^{2,1}, C^{2,1} \u0026= \\text{Head}^{2}(G_{0}^{2}, ..., G_{B}^{2}) \\end{aligned} $$1.3. Training 1.3.1. 三维回归损失 我们唯一的训练目标是基于三维空间中的回归。我们将真实点图记为 $\\overline{X}_{1,1}$ 和 $\\overline{X}_{2,1}$，它们是根据公式\n$$ X^{n, m}=P_{m} P_{n}^{-1} h\\left(X^{n}\\right) $$以及两组对应的有效像素点 $D_1, D_2 \\subseteq \\{1 \\ldots W\\} \\times \\{1 \\ldots H\\}$ 得到的，真实值是在这些有效像素点上定义的。对于视图 $v \\in \\{1, 2\\}$ 中有效像素点 $i \\in D_v$ 的回归损失，简单定义为欧几里得距离：\n$$ \\ell_{regr}(v, i) = \\left\\| \\frac{1}{z} X^{v,1}_i - \\frac{1}{\\overline{z}} \\overline{X}^{v,1}_i \\right\\| $$需要着重强调一下 $X^{v,1}$ 两个上标的含义是什么。如前式所示，点图中的点 $X$ 的第一个上标表示图像 $v$ 在图像 $1$ (实质上就是网络预测输入的第一张照片，就是 DUSt3R 正常的坐标系和输出顺序) 上的坐标。因此此处需要做的坐标变换主要是将图像 2 的 GT 变换到 图像 1 上，之后和图像 2 预测出的点图求欧式距离。\n为了解决预测值和真实值之间的尺度模糊问题，我们分别通过缩放因子 $z = \\text{norm}(X_{1,1}, X_{2,1})$ 和 $\\overline{z} = \\text{norm}(\\overline{X}_{1,1}, \\overline{X}_{2,1})$ 对预测点图和真实点图进行归一化处理，这些缩放因子简单表示为所有有效点到原点的平均距离：\n$$ \\text{norm}(X_1, X_2) = \\frac{1}{|D_1| + |D_2|} \\sum_{v \\in \\{1,2\\}} \\sum_{i \\in D_v} \\|X_v^i\\| $$这个归一化系数实质上是分别对预测的点图和Ground Truth 分别求了点到坐标原点的欧式距离的平均值。以此解决了预测值和真实值之间的尺度模糊问题。也就是说，DUSt3R 通过对点图进行归一化来解决尺度模糊的问题\n1.3.2. 置信损失 置信度感知损失。实际上，与我们的假设相反，存在定义不明确的三维点，例如在天空中或半透明物体上的点。更普遍地说，图像中的某些部分通常比其他部分更难预测。因此，我们联合学习为每个像素预测一个分数，该分数表示网络对该特定像素的置信度。最终的训练目标是对所有有效像素根据公式\n$$ \\ell_{regr}(v, i) = \\left\\| \\frac{1}{z} X_{v,1}^i - \\frac{1}{\\overline{z}} \\overline{X}_{v,1}^i \\right\\| $$计算的置信度加权回归损失：\n$$ L_{conf} = \\sum_{v \\in \\{1,2\\}} \\sum_{i \\in D_v} C_{v,1}^i \\ell_{regr}(v, i) - \\alpha \\log C_{v,1}^i $$其中 $C_{v,1}^i$ 是像素 $i$ 的置信度分数， $\\alpha$ 是一个控制正则化项的超参数。\n为了确保置信度严格为正，我们通常定义 $C_{v,1}^i = \\frac{1}{1 + \\exp(-g_{C_{v,1}^i})} \u003e 0$ （。这会迫使网络在更难处理的区域进行推断，例如那些仅由单个视图覆盖的区域。使用这个目标来训练网络 $F$ 可以在没有明确监督的情况下估计置信度分数。\n1.4. 应用 1.4.1. 三维点匹配 在三维点图空间中，通过最近邻搜索可以很容易地在两张图像的像素之间建立对应关系。为了将误差降至最低，我们通常会保留图像 $I^{1}$ 和 $I^{2}$ 之间的相互（双向）对应关系 $M_{1,2}$，也就是说，我们有：\n$$ \\begin{array}{c} \\mathcal{M}_{1,2}=\\left\\{(i, j) | i=NN_{1}^{1,2}(j)\\right. and \\left.j=NN_{1}^{2,1}(i)\\right\\}\\\\ \\text{where: } NN_{k}^{n, m}(i)=\\underset{j \\in\\{0, ..., W H\\}}{arg min }\\left\\| X_{j}^{n, k}-X_{i}^{m, k}\\right\\| \\end{array} $$1.4.2. 恢复相机内参 恢复相机内参。根据定义，点图 $X^{1,1}$ 是在图像 $I^{1}$ 的坐标系中表示的。因此，通过求解一个简单的优化问题来估计相机内参是可行的。在这项工作中，我们假设主点大致位于中心位置，且像素为正方形，因此仅需估计焦距 $f_{1}^{*}$ ：\n$$ f_{1}^{*}=\\underset{f_{1}}{\\arg\\min} \\sum_{i = 0}^{W} \\sum_{j = 0}^{H} C_{i, j}^{1,1}\\left\\| \\left(i', j'\\right)-f_{1} \\frac{\\left(X_{i, j, 0}^{1,1}, X_{i, j, 1}^{1,1}\\right)}{X_{i, j, 2}^{1,1}}\\right\\| $$其中：\n$$ X_{i, j}^{1,1} = \\left[\\begin{array}{ccc} X_{i, j, 0}^{1,1} \u0026 X_{i, j, 1}^{1,1} \u0026 X_{i, j, 2}^{1,1} \\end{array}\\right] ^\\top $$且有 $i' = i - \\frac{W}{2}$ 且 $j' = j - \\frac{H}{2}$。\n1.4.3. 相对位置姿估计 相对位姿估计可以通过多种方式实现。一种方法是如上述进行二维匹配并恢复内参，然后估计基础矩阵并恢复相对位姿。或者可以直接比较点图 $X^{1,1} \\leftrightarrow X^{1,2}$ 以得到相对位姿 $P^{*}=[R^{*} | t^{*}]$ ：\n$$ R^{*}, t^{*}=\\underset{\\sigma, R, t}{\\arg\\min} \\sum_{i} C_{i}^{1,1} C_{i}^{1,2}\\left\\| \\sigma\\left(R X_{i}^{1,1}+t\\right)-X_{i}^{1,2}\\right\\| ^{2}, $$这可以通过解析解得到。但是解析解对对噪声和外点十分敏感。可以考虑使用 RANSAC 求解。\n2. MASt3R MASt3R 和 DUSt3R 其实没有特别大的区别，事实上，MASt3R 和 DUSt3R 甚至没有太多新的代码。和 DUSt3R 相比，其主要增强了输入照片之间的特征匹配能力。具体而言，MASt3R 的 Encoder 和 Decoder 与 DUSt3R 是完全相同的，主要区别在于增加了一个预测头，用于预测出稠密的，逐像素的特征描述子 $F \\in R^{W \\times H \\times d}$，其中 $d$ 是特征描述子的维度。\n应为网络结构和三维结构预测头和 DUSt3R 是完全相同的，我们暂且不论，主要关注于 Local Feather Predicting Head 的训结构和训练。MASt3R 论文中提到，直接通过 DUSt3R 到的对应关系相当不精确，导致精度不够理想。其原因如下：\n回归本质上会受到噪声的影响 因为 DUSt3R 从未经过专门针对匹配的显式训练。 2.1. 匹配头 匹配头。出于这些原因，我们提议添加第二个头，它输出两个维度为 $d$的密集特征图 $D^{1}$和 $D^{2}$， $D^{1},D^{2}\\in \\mathbb{R}^{H ×W ×d}$：\n$$ \\begin{aligned} D^{1} \u0026= Head_{desc }^{1}\\left(\\left[H^{1}, H^{\\prime 1}\\right]\\right)\\\\ D^{2} \u0026= Head_{desc }^{2}\\left(\\left[H^{2}, H^{\\prime 2}\\right]\\right) \\end{aligned} $$ （其中 $Head_{desc }^{1}$和 $Head_{desc }^{2}$ 为特征描述子预测头， $[H^{1}, H^{\\prime 1}]$和 $[H^{2}, H^{\\prime 2}]$为相应的输入组合，上式用于计算两个特征图 $D^{1}$和 $D^{2}$。我们将这个头实现为一个简单的两层多层感知器（MLP），其中穿插了非线性 GELU 激活函数。最后，我们将每个局部特征归一化为单位范数。\n2.2. 匹配 loss 我们希望一张照片中的局部描述子最多与另一张照片中最多一个描述子相匹配，并且相互匹配的描述子描述了两张照片中同一个三维点。不妨假设我们有真值的匹配关系：\n$$ \\hat {\\mathcal M} = \\{(i, j) \\mid \\hat X_{i}^{1,1} = \\hat X_{i}^{1,2}\\} $$可以通过 infoNCE loss 来寻找描述子之间的对应关系：\n$$ \\mathcal{L}_{match }=-\\sum_{(i, j) \\in \\hat{\\mathcal{M}}} log \\frac{s_{\\tau}(i, j)}{\\sum_{k \\in \\mathcal{P}^{1}} s_{\\tau}(k, j)}+log \\frac{s_{\\tau}(i, j)}{\\sum_{k \\in \\mathcal{P}^{2}} s_{\\tau}(i, k)}, $$其中：\n$$ s_{\\tau}(i, j)=exp \\left[-\\tau D_{i}^{1 \\top} D_{j}^{2}\\right] $$在这里， $\\mathcal P^{1}=\\{i |(i, j) \\in \\hat{M}\\}$和 $\\mathcal P^{2}=\\{j |(i, j) \\in \\hat{M}\\}$分别表示每张图像中所考虑像素的子集，而 $\\tau$是一个温度超参数。请注意，这个匹配目标本质上是一种交叉熵分类损失：与 DUSt3R 的 3D 点回归损失不同，只有当网络正确匹配到正确的像素时，它才会得到奖励。这极大地促使网络实现高精度匹配。最终的损失为：\n$$ \\mathcal{L}_{total }=\\mathcal{L}_{conf }+\\beta \\mathcal{L}_{match } $$2.3. 快速相互匹配 (Fast reciprocal matching) 给定两个预测的特征图 $D^{1}, D^{2} \\in \\mathbb{R}^{H×W×d}$，我们希望可以在两张图片之间提取出对应的像素匹配关系：\n$$ \\mathcal{M}=\\{(i, j) | j = NN_{2}(D_{i}^{1}) \\text{ 且 } i = NN_{1}(D_{j}^{2})\\} $$其中：\n$$ NN_{A}(D_{j}^{B}) = \\underset{i}{\\arg\\min} \\| D_{i}^{A} - D_{j}^{B}\\| $$但是问题在于，我们目前面对的是一个稠密匹配问题。如果直接使用简单的暴力匹配，其时间复杂度将达到 $O(W^2H^2)$，着显然是无法接受的。当然，还可以用 k-d tree 一类的方法。但是事实上这类方法在特征的纬度非常高时时间复杂度也同样无法接受。因此，本文提出了一种名为 fast match 的方法，用于解决由于稠密匹配且特征维度过高导致的问题。\n这个方法的核心思路是通过采样，降低每一次匹配过程的时间复杂度。这个过程是一个迭代过程，每一次迭代可以分为下面几步：\n我们现在特征图 $D^1$ 中进行随机采样，稀疏的得到 $k$ 个像素 $U^{0}=\\{U_{n}^{0}\\}_{n = 1}^{k}$。 通过 NN，在 $D^2$ 上找到 $U^{0}=\\{U_{n}^{0}\\}_{n = 1}^{k}$ 对应的 $k$ 个像素 $V^{1}=\\{V_{n}^{1}\\}_{n = 1}^{k}$。 将 $D^2$ 上找到的特征 $V^{1}=\\{V_{n}^{1}\\}_{n = 1}^{k}$ 重新通过 NN 在 $D^1$ 上寻找匹配点 $U^{1}=\\{U_{n}^{1}\\}_{n = 1}^{k}$。 $$ \\begin{array}{c} U^{t} \\stackrel{}{\\longrightarrow}\\left[NN_{2}\\left(D_{u}^{1}\\right)\\right]_{u \\in U^{t}} = V^{t}\\\\ V^{t} \\stackrel{}{\\longrightarrow}\\left[NN_{1}\\left(D_{v}^{2}\\right)\\right]_{v \\in V^{t}} = U^{t + 1} \\end{array} $$ 对比 $U^{0}=\\{U_{n}^{0}\\}_{n = 1}^{k}$ 和 $U^{1}=\\{U_{n}^{1}\\}_{n = 1}^{k}$。我们认为满足 $M_{k}^{t}=\\{(U_{n}^{t}, V_{n}^{t}) | U_{n}^{t}=U_{n}^{t + 1}\\}$ 的点已经完成匹配，会将这些点过滤掉。再进行下一轮循环。 如上图所示，实质上，对于初试选择的 $k$ 个点，很快的会辗转的匹配，直到完全匹配。\n2.4. 从粗到细的匹配 事实上，自从 Transformer 提出的那一天开始，就因其长序列输入时的时间复杂度而被诟病。例如对于一个分辨率为 $(H, W)$ 的图像输入而言，如果将其划分为 $m$ 个 ViT 的图片序列，其时间复杂度为 $m^2$。因此 MASt3R 目前仅能处理最大为 512 像素的图像。事实上，高分辨率的图片往往首先需要降采样到 MASt3R 所能接受的最大水平，并将推理的结果上采样到原始分辨率。但是往往会导致性能损失和重建质量大幅下降。MASt3R 使用了从粗到细的匹配方法来解决这个问题。具体思路如下：\n对两张照片的降采样版本进行匹配。我们将子采样 $k$ 得到的粗匹配的对应关系记作 $M_{k}^{0}$ 接下来在每幅全分辨率图像上独立生成一组有重叠的裁剪窗口 $W \\in \\mathbb{R}^{w×4}$。每个窗口都有 $512$ 像素，且重叠率为 $50\\%$ 枚举所有可能的窗口对 $(w_1, w_2) \\in (W \\times W)$。之后选择一个覆盖粗匹配 $M_{k}^{0}$ 的子匹配 具体来说，我们以贪心的方式逐个添加窗口对，直到覆盖90%的对应关系。最后，我们对每个窗口对独立进行匹配： $$ \\begin{aligned} D^{\\omega_{1}}, D^{\\omega_{2}} \u0026= \\text{MASt3R}\\left(I_{w_{1}}^{1}, I_{w_{2}}^{2}\\right)\\\\ \\mathcal{M}_{k}^{w_{1}, w_{2}} \u0026= \\text{Fast\\_Reciprocal\\_Match} \\left(D^{w_{1}}, D^{w_{2}}\\right) \\end{aligned} $$\nReference @inproceedings{dust3r_cvpr24, title={DUSt3R: Geometric 3D Vision Made Easy}, author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud}, booktitle = {CVPR}, year = {2024} }\n@misc{mast3r_arxiv24, title={Grounding Image Matching in 3D with MASt3R}, author={Vincent Leroy and Yohann Cabon and Jerome Revaud}, year={2024}, eprint={2406.09756}, archivePrefix={arXiv}, primaryClass={cs.CV} }\n@inproceedings{croco, title={{CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion}}, author={{Weinzaepfel, Philippe and Leroy, Vincent and Lucas, Thomas and Br'egier, Romain and Cabon, Yohann and Arora, Vaibhav and Antsfeld, Leonid and Chidlovskii, Boris and Csurka, Gabriela and Revaud J'er^ome}}, booktitle={{NeurIPS}}, year={2022} }\n从DUSt3R 到 MASt3R\n","permalink":"https://wangjv0812.cn/2025/03/dust3r-and-must3r/","summary":"\u003ch2 id=\"1-dust3r\"\u003e1. DUSt3R\u003c/h2\u003e\n\u003ch3 id=\"11-introduction\"\u003e1.1. Introduction\u003c/h3\u003e\n\u003cp\u003e一般而言，现代的 MVS 和 SFM 的流程总是可以总结为以下几个子问题\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征点匹配\u003c/li\u003e\n\u003cli\u003e寻找本质矩阵\u003c/li\u003e\n\u003cli\u003e对点进行三角测量\u003c/li\u003e\n\u003cli\u003e对场景进行稀疏重建\u003c/li\u003e\n\u003cli\u003e估计相机参数，\u003c/li\u003e\n\u003cli\u003e密集重建\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e但是在这个复杂的过程中，每个子问题都对原始问题做了简化，无法完美解决，为后面的步骤引入了噪声，从而导致整个系统显的“精致而脆弱”。在这方面，每个子问题之间缺乏沟通就很能说明问题：如果能将这些缓解紧耦合到一起，将噪声统一的，全局的考虑，可以很大程度上解决应为过度简化和解耦导致的种种问题。此外，这个流程中的关键步骤很脆弱，在很多情况下容易出错。例如，很多 SFM 方法都依赖于相机参数的估计，但是如果遇到观察比较少、非漫反射表面或者相机姿态运动较为单一时，相机参数估计可能失效，导致整个 SFM 过程都会失效。归根结底：\u003cstrong\u003e一个多视图立体视觉（MVS）算法的性能仅取决于输入图像和相机参数的质量\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e事实上，单张图或者多张图哦通过深度学习的方式提取深度并不罕有。但是在不引入额外的先验信息时，这个问题往往是\u003cstrong\u003e不适定的\u003c/strong\u003e，所以这些方法利用神经网络从大量数据中学习巨量的三维先验知识来解决模糊性问题。这些方法可以分为两类。第一类利用类别级别的物体先验知识，事实上 DreamFusion 就属于这类工作，可以从单张照片或者一句自然语言描述生成三纬结构。另一种与 DUSt3R 较为类似，系统的学习一般的场景来实现单目深度估计。但是一般而言，例如 SuperGlue 之类，在训练和推理过程中，都没有显然的引入三维结构的信息，也没有扔掉相机矩阵的桎梏。可以说，DUSt3R 是一种基于深度学习的 ALL in One 的深度估计方法，入了点图（Point Map）表示，使网络能够在规范框架中预测三维形状。\u003c/p\u003e\n\u003ch3 id=\"12-method-and-forward\"\u003e1.2. Method and forward\u003c/h3\u003e\n\u003ch4 id=\"121-ponit-map\"\u003e1.2.1. Ponit Map\u003c/h4\u003e\n\u003cp\u003e接下来，我们将图片中每个像素对应的三维点构成的集合称为 \u003cstrong\u003e点图\u003c/strong\u003e（point map） $X \\in R^{W×H×3}$。与分辨率为 $W×H$的对应RGB图像 $I$相关联，\u003cstrong\u003e点图 $X$ 在图像像素与三维点之间形成一一映射\u003c/strong\u003e，即对于所有像素坐标 $(i, j) \\in \\{1...W\\}×\\{1...H\\}$，都有 $I_{i,j} \\leftrightarrow X_{i,j}$。此处每个像素点对应于一个三维点实事丧引入了一个简化假设，即假设观测的场景全部是不透明且漫反射的，不存在透过某个物体并观察到另一个物体的情况。\u003c/p\u003e\n\u003ch4 id=\"122-相机和场景\"\u003e1.2.2. 相机和场景\u003c/h4\u003e\n\u003cp\u003e相机与场景。给定相机内参 $K \\in \\mathbb{R}^{3 ×3}$  ，所观测场景的点图 $X$ 可以直接从真实深度图：\u003c/p\u003e\n$$\nD \\in \\mathbb{R}^{W ×H}\n$$\u003cp\u003e中获取，其公式为\u003c/p\u003e\n$$\n\\begin{aligned}\nX_{i, j}\n\u0026= K^{-1}D_{i,j}[i , j , 1]^{\\top}\\\\\n\\end{aligned}\n$$\u003cp\u003e其中， $X$ 是在相机坐标系中表示的。接下来，我们将在相机 $m$ 的坐标系中表示的、来自相机 $\\pi$ 的点图 $X^{n}$ 记为 $X^{n, m}$，表示第 $n$ 帧在 第 $m$ 帧的坐标系下的点图为 ：\u003c/p\u003e","title":"DUSt3R and MUSt3R"},{"content":"假设我们有两个坐标系 $\\mathcal F_a, \\ \\mathcal F_b$，有一个点 $P$ 在一个平面上。在两个坐标系下，这个点可以描述为 $\\rho_a, \\rho_b$；对应的平面可以通过法向量和截距来描述：$\\{n_a. d_a\\}, \\{n_b. d_b\\}$。\n此外，该点有在图像坐标系下的描述 $p_a, p_b$ 和对应的相机矩阵 $K_a, K_b$。那么可以写出：\n$$ \\begin{aligned} p_a = \\frac{1}{z_a} K_a \\rho_a \\\\ p_b = \\frac{1}{z_b} K_b \\rho_b \\end{aligned} $$此外， 由于该点在对应的平面上，有平面约束：\n$$ \\begin{aligned} n^T_a \\rho_a + d_a = 0 \\\\ n^T_b \\rho_b + d_b = 0 \\end{aligned} $$那么，将平面约束中的 $\\rho$ 通过投影矩阵转换为像素坐标，有：\n$$ \\begin{aligned} \u0026z_a n^T_a K_a^{-1} p_a + d_a = 0\\\\ \u0026z_a = -\\frac{d_a}{n^T_a K_a^{-1} p_a}\\\\ \u0026z_b n^T_b K_b^{-1} p_b + d_b = 0\\\\ \u0026z_b = -\\frac{d_b}{n^T_b K_b^{-1} p_b}\\\\ \\end{aligned} $$带入到 $\\rho_a, \\rho_b$ 的表达式中，有：\n$$ \\begin{aligned} \\rho_a = -\\frac{d_a}{n^T_a K_a^{-1} p_a} K_a^{-1} p_a \\\\ \\rho_b = -\\frac{d_b}{n^T_b K_b^{-1} p_b} K_b^{-1} p_b \\end{aligned} $$我们知道，$\\rho_a, \\rho_b$ 之间有一个变换矩阵 $T_{ab}$，有：\n$$ \\rho_b = C_ba \\rho_a + r_b^{ba} $$将 $\\rho_a, \\rho_b$ 变换为像素坐标，有：\n$$ \\begin{aligned} z_b K_b^{-1} p_b \u0026= C_{ba} z_a K_a^{-1} p_a + r_b^{ba}\\\\ p_b \u0026= \\frac{z_a}{z_b} K_b C_{ba} K_a^{-1} p_a + \\frac{1}{z_b}K_b r_b^{ba} \\end{aligned} $$将 $\\frac{1}{z_b}$ 变换为 $\\frac{z_a}{z_b}\\frac{1}{z_a}$ ，且带入 $r_b^{ab} = -C_{ba} r_a^{ba}$，有：\n$$ \\begin{aligned} p_b \u0026= \\frac{z_a}{z_b} K_b C_{ba} K_a^{-1} p_a + \\frac{z_a}{z_b} K_b C_{ba} r_a^{ba}\\frac{n^T_a K_a^{-1} p_a}{d_a}\\\\ \u0026= \\frac{z_a}{z_b} K_b C_{ba} K_a^{-1} p_a + \\frac{z_a}{z_b} K_b C_{ba} r_a^{ba}\\frac{n^T_a}{d_a} K_a^{-1} p_a\\\\ \u0026= \\frac{z_a}{z_b} K_b C_{ba} \\left( 1 + \\frac{r_a^{ba}n^T_a}{d_a} \\right)K_a^{-1}p_a \\end{aligned} $$其中 $\\frac{z_a}{z_b}$ 为一个系数，可以被齐次坐标吸收。有：\n$$ p_b = K_b C_{ba} \\left( 1 + \\frac{r_a^{ba}n^T_a}{d_a} \\right)K_a^{-1}p_a $$其中：\n$$ H_{ba} = C_{ba} \\left( 1 + \\frac{r_a^{ba} n_a^T}{d_a} \\right) $$我们可以写出：\n$$ K_b^{-1} p_b = H_{ba} K_a^{-1} p_a $$我们可以这样理解：\n$K_b^{-1} p_b$ 和 $K_a^{-1} p_a$ 为归一化坐标系下的点 $H_{ba}$ 为一个 $3 \\times 3$ 的矩阵，描述了两个归一化坐标系之间的变换 ","permalink":"https://wangjv0812.cn/2025/02/homography/","summary":"\u003cp\u003e假设我们有两个坐标系 $\\mathcal F_a, \\ \\mathcal F_b$，有一个点 $P$ 在一个平面上。在两个坐标系下，这个点可以描述为 $\\rho_a, \\rho_b$；对应的平面可以通过法向量和截距来描述：$\\{n_a. d_a\\}, \\{n_b. d_b\\}$。\u003c/p\u003e\n\u003cp\u003e此外，该点有在图像坐标系下的描述 $p_a, p_b$ 和对应的相机矩阵 $K_a, K_b$。那么可以写出：\u003c/p\u003e\n$$\n\\begin{aligned}\np_a = \\frac{1}{z_a} K_a \\rho_a \\\\\np_b = \\frac{1}{z_b} K_b \\rho_b\n\\end{aligned}\n$$\u003cp\u003e此外， 由于该点在对应的平面上，有平面约束：\u003c/p\u003e\n$$\n\\begin{aligned}\nn^T_a \\rho_a + d_a = 0 \\\\\nn^T_b \\rho_b + d_b = 0\n\\end{aligned}\n$$\u003cp\u003e那么，将平面约束中的 $\\rho$ 通过投影矩阵转换为像素坐标，有：\u003c/p\u003e\n$$\n\\begin{aligned}\n\u0026z_a n^T_a K_a^{-1} p_a + d_a = 0\\\\\n\u0026z_a = -\\frac{d_a}{n^T_a K_a^{-1} p_a}\\\\\n\u0026z_b n^T_b K_b^{-1} p_b + d_b = 0\\\\\n\u0026z_b = -\\frac{d_b}{n^T_b K_b^{-1} p_b}\\\\\n\\end{aligned}\n$$\u003cp\u003e带入到 $\\rho_a, \\rho_b$ 的表达式中，有：\u003c/p\u003e","title":"homography"},{"content":"1. 使用神经网路进行数据生成 使用神经网络生成一个高维度数据是机器学习中非常重要的一个工作。我们假设数据集 $\\left\\{\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n}\\right\\}$ 为一个大小为$n$的数据集，该数据集统一的服从一个概率分布 $p_{data}(\\boldsymbol{x})$ 。我们假设对数据集的抽样都是独立同分布的，即：\n$$ \\left\\{\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n}\\right\\} \\sim p_{data}(\\boldsymbol{x}) $$那么丛现有数据生成新的数据的核心就是使用神经网络学习这个概率分布。不妨假设学习的概率分布为 $\\hat p_\\theta(\\boldsymbol x)$。我们会希望 $\\hat p_\\theta(\\boldsymbol x)$ 尽可能的接近 $p_{data}(\\boldsymbol(x))$ 。为了衡量真是分布和我们学习的分布之间的差距，我们需要定义一个距离函数 $D(\\cdot \\mid \\cdot)$ 我们可以定义优化目标：\n$$ \\hat \\theta = \\arg \\min_{\\theta} D\\left(p_{data}(\\boldsymbol{x}) \\mid \\hat p_\\theta(\\boldsymbol x) \\right) $$关于距离函数，我们可以定义 $D(\\cdot \\mid \\cdot)$ 为 f-divergence 定义为：\n$$ D_f(p_{data}(\\boldsymbol(x)) \\mid \\hat p_\\theta(\\boldsymbol x)) = \\int p_\\theta(\\boldsymbol x) f \\left(\\frac{p_{data}(\\boldsymbol x)}{p_\\theta(\\boldsymbol x)}\\right) d\\boldsymbol x $$不妨取 $f(x) = x\\log x$ ，我们可以得到 KL 散度：\n$$ \\begin{aligned} D_{KL}(p_{data}(\\boldsymbol(x)) \\mid \\hat p_\\theta(\\boldsymbol x)) \u0026= \\int p_{data}(\\boldsymbol x) \\log \\frac{p_{data}(\\boldsymbol x)}{p_\\theta(\\boldsymbol x)} d\\boldsymbol x\\\\ \u0026= \\mathbb E_{p_{data}(\\boldsymbol x)}\\left[ \\log \\frac{p_{data}(\\boldsymbol x)}{p_\\theta(\\boldsymbol x)} \\right] \\end{aligned} $$我们可以用抽样的均值来代替期望，有：\n$$ \\begin{aligned} D_{KL}(p_{data}(\\boldsymbol(x)) \\mid \\hat p_\\theta(\\boldsymbol x)) \u0026\\approx \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{p_{data}(\\boldsymbol x_i)}{p_\\theta(\\boldsymbol x_i)}\\\\ \u0026= \\frac{1}{n} \\sum_{i=1}^n \\log p_{data}(\\boldsymbol x_i) - \\frac{1}{n} \\sum_{i=1}^n\\log p_\\theta(\\boldsymbol x_i)\\\\ \u0026\\stackrel{i}{=} -\\frac{1}{n} \\sum_{i=1}^n\\log p_\\theta(\\boldsymbol x_i) + \\text{const} \\end{aligned} $$其中，因为 $\\frac{1}{n} \\sum_{i=1}^n \\log p_{data}(\\boldsymbol x_i)$ 是一个与优化参数无关的分布，我们可以直接抛弃。那么显然的，我们可以通过下面的方式训练我们的生成式模型的概率分布，即在所有的数据点上计算最大似然：\n$$ \\hat \\theta = \\arg \\max_{\\theta} \\frac{1}{n} \\sum_{i=1}^n\\log p_\\theta(\\boldsymbol x_i) $$但是问题没有解决。在优化过程中，我们需要保证训练的生成式神经网络为一个概率分布，那么需要满足概率分布的基本条件：\n非负性： $\\forall \\boldsymbol x , p_\\theta(\\boldsymbol x) \u003e 0$ 归一化性质： $\\int p_\\theta(\\boldsymbol x) d\\boldsymbol x = 1$ 这两个条件都不容易满足，但是和归一化性质需要计算全概率的积分相比，非负性都显得不那么困难了。因此我们需要思考一些数学技巧来摆脱归一化条件的桎梏。\n1.1. 归一化常数近似 第一个方法收到了玻尔兹曼函数的启发。我们使用神经网络学习一个标量的能量函数 $E_\\theta(\\boldsymbol x)$ ，有概率分布：\n$$ \\begin{array}{c} p_\\theta(\\boldsymbol x) = \\frac{\\exp \\left(-E_\\theta(\\boldsymbol x)\\right)}{Z_\\theta}\\\\ \\text{where: }Z_\\theta = \\int \\exp \\left(-E_\\theta(\\boldsymbol x)\\right) d\\boldsymbol x \\end{array} $$显然其中能量函数 $E_\\theta(\\boldsymbol x)$ 不需要受到概率分布的性质约束。对于前面的最大自然函数，带入玻尔兹曼函数的概率分布，有：\n$$ \\begin{aligned} \\hat \\theta \u0026= \\arg \\max_{\\theta} \\frac{1}{n} \\sum_{i=1}^n\\log p_\\theta(\\boldsymbol x_i)\\\\ \u0026= \\arg \\max_{\\theta} -\\frac{1}{n} \\sum_{i=1}^n E_\\theta(\\boldsymbol x) - \\log Z_\\theta\\\\ \u0026= \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n E_\\theta(\\boldsymbol x) + \\log Z_\\theta \\end{aligned} $$不妨定义损失函数为：\n$$ \\mathcal L(\\theta) = \\frac{1}{n} \\sum_{i=1}^n E_\\theta(\\boldsymbol x) + \\log Z_\\theta $$我们有：\n$$ \\nabla_\\theta \\mathcal L = \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta E_\\theta(\\boldsymbol x) + \\nabla_\\theta\\log Z_\\theta $$其中 $\\nabla_\\theta E_\\theta(\\boldsymbol x)$ 的计算是简单的，问题是后一项。\n$$ \\begin{aligned} \\nabla_\\theta\\log Z_\\theta \u0026= \\nabla_\\theta \\log \\int \\exp \\left(-E_\\theta(\\boldsymbol x)\\right) d\\boldsymbol x\\\\ \u0026= -\\frac{\\int \\exp \\left(-E_\\theta(\\boldsymbol x)\\right) \\nabla E_\\theta (\\boldsymbol x)d \\boldsymbol x}{\\int \\exp \\left(-E_\\theta(\\boldsymbol x)\\right) d\\boldsymbol x}\\\\ \u0026= -\\int \\frac{\\exp \\left(-E_\\theta(\\boldsymbol x)\\right)}{\\int \\exp \\left(-E_\\theta(\\boldsymbol x)\\right) d\\boldsymbol x} \\nabla E_\\theta (\\boldsymbol x)d \\boldsymbol x\\\\ \u0026= -\\int p_\\theta(\\boldsymbol x) \\nabla E_\\theta (\\boldsymbol x)d \\boldsymbol x\\\\ \u0026= E_{p_\\theta (\\boldsymbol x)}\\left[\\nabla_\\theta E_\\theta (\\boldsymbol x)\\right] \\end{aligned} $$在训练中，我们解决了归一化的问题，但是在推理中并没能避免。此时一般会用蒙特卡洛方法计算。\n1.2. Autoregress Module (自回归模型) 通过概率的链式法则，我们可以将一个高维度的概率分布分解为多个低维度的概率分布的乘积。这种方法被称为自回归模型。例如：\n$$ \\begin{aligned} p_\\theta\\left(x_1, x_2, \\cdots, x_n\\right) \u0026= p_\\theta(x_1) p_\\theta(x_2\\mid x_1) p_\\theta(x_3\\mid x_1, x_2) \\cdots p_\\theta(x_n\\mid x_1, x_2, \\cdots, x_{n-1})\\\\ \u0026= \\prod_{i=1}^n p_\\theta(x_i\\mid x_{i-1}, x_{i-2}, \\cdots, x_1) \\end{aligned} $$那么归一化性质应有：\n$$ \\begin{aligned} \\int p_\\theta\\left(x_1, x_2, \\cdots, x_n\\right) d\\boldsymbol x \u0026= \\int \\prod_{i=1}^n p_\\theta(x_i\\mid x_{i-1}, x_{i-2}, \\cdots, x_1) d\\boldsymbol x\\\\ \u0026= \\int p_\\theta(x_1) d\\boldsymbol x \\int p_\\theta(x_2\\mid x_1) d\\boldsymbol x \\cdots \\int p_\\theta(x_n\\mid x_1, x_2, \\cdots, x_{n-1}) d\\boldsymbol x\\\\ \\end{aligned} $$此时，我们不需要计算一个很高维度的概率分布的积分，只需要对一些简单的低维概率分布归一化就好了。事实上自回归模型的训练更加巧妙，对于 $n$ 维数据生成而言，我们不需要同时训练 $n$ 个神经网络，而是训练一个神经网络，结合一个编码位置或者无效数据掩码来实现。\n1.3. Normalizing Flows (归一化流模型) 假设存在一个平凡的概率分布 $\\pi(\\boldsymbol z)$，例如我们可以取 $\\pi(\\boldsymbol z) \\sim \\mathcal G(\\boldsymbol z)$。我们希望存在一个可逆函数 $\\boldsymbol x = f_\\theta(\\boldsymbol z)$，实现潜变量 $\\boldsymbol z$ 与数据空间 $\\boldsymbol x$ 之间的转换。那么根据概率的链式法则，我们有：\n$$ p_\\theta(\\boldsymbol x) = \\pi(\\boldsymbol z) = \\pi\\left(f_\\theta^{-1}(\\boldsymbol x)\\right) | \\det \\boldsymbol J_{f_\\theta^{-1}}(\\boldsymbol x)| $$其中 $\\boldsymbol J_{f_\\theta^{-1}}(\\boldsymbol x)$ 为 $f_\\theta^{-1}(\\boldsymbol x)$ 的雅可比矩阵。\n因为 $\\pi(\\boldsymbol z)$ 是一个归一化的概率函数，那么 $p_\\theta(\\boldsymbol x)$ 也是一个归一化的概率函数。但是 Normalizing FLow Module 对函数 $f_\\theta(\\boldsymbol z)$ 的要求很苛刻，要求它同时满足可逆性和雅可比矩阵的计算是简单的。因此 $f_\\theta(\\boldsymbol z)$ 的选取往往是很简单的。一般会多层嵌套来使用：\n$$ p(x) = p_z(z_0) \\prod_{i=1}^{n} \\left| \\det \\frac{\\partial f_i^{-1}(z_{i-1})}{\\partial z_i} \\right| $$本质上讲，Flow Module 学习了一组可逆且非线性的映射，讲一个复杂的分布映射到了一个简单的数据空间。具体的例子可以参考 Normalizing Flows。\n1.4. Variational Autoencoder（变分自编码） 对于原始数据 $\\boldsymbol{x}$，和自编码器类似，我们希望可以估计出两个分布 $p_\\theta(\\boldsymbol{z}\\mid \\boldsymbol{x})$ 和 $p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$，将原始数据分布变换到一个隐藏层 $\\boldsymbol{z}$ 的分布上。\n我们希望隐藏层 $\\boldsymbol{z}$ 的分布是简单的，例如 $p(\\boldsymbol{z}) \\sim \\mathcal G$。事实上这个思路与自编码器十分类似，主要区别在于我们引入了对概率分布的采样来描述数据的生成过程。那么，VAE 的推断过程我们可以总结为：\n首先，从 $\\boldsymbol{z}$ 中采样一个先验分布 $p_\\theta(\\boldsymbol{z})$ 通过神经网络预测 $x$ 的分布 $p_\\theta(\\boldsymbol{x}) = \\int p_\\theta(\\boldsymbol{x}\\mid \\boldsymbol{z})p(\\boldsymbol{z})d\\boldsymbol{z}$ 训练时可以直接使用最大似然：\n$$ \\theta = \\arg \\max_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\log p_\\theta(\\boldsymbol{x}_i) $$那么现在唯一的问题是，$p_\\theta(\\boldsymbol{x}) = \\int p_\\theta(\\boldsymbol{x}\\mid \\boldsymbol{z})p(\\boldsymbol{z})d\\boldsymbol{z}$该怎么算。下面我们需要对这个问题进行一定的化简，即变分下界或者证据下界。\n变分下界是变分贝叶斯中的一个重要问题，它源于贝叶斯推断中一个十分重要的问题，我们如何计算或近似给定数据的模型证据（也称为边缘似然）$p(x)$，即在所有可能的潜在变量 $z$ 上积分模型概率 $p(x,z)$。显然的，对于复杂模型，这个积分的形式是极其复杂的。变分推断则引入了一个近似于后验分布 $q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\approx p(\\boldsymbol{z} \\mid \\boldsymbol{x})$，并且定义了一个新的函数 ELBO，证明了它是模型证据 $p(\\boldsymbol{x})$ 的下界。\n对于原始 $p_\\theta(\\boldsymbol{x})$，我们有：\n$$ \\begin{aligned} p_\\theta(\\boldsymbol{x}) \u0026= \\int p_\\theta(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z} \u0026= \\int \\frac{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}p_\\theta(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z} \\end{aligned} $$此处，我们使用贝叶斯定理分解 $p_\\theta(\\boldsymbol{x}, \\boldsymbol{z})$，有：\n$$ \\int p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})\\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})}{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})} d\\boldsymbol{z} $$此处我们引如 Jensen 下界：\n$$ \\log \\int p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})\\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})}{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})} d\\boldsymbol{z} \\geq \\int p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\log\\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})}{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})} d\\boldsymbol{z} $$分解积分项，可以得到：\n$$ \\begin{aligned} \\log p_\\theta(\\boldsymbol{x}) \u0026\\geq \\int p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\log\\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})}{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})} d\\boldsymbol{z}\\\\ \u0026= \\int p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})\\log p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) d\\boldsymbol{z}+ \\int p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\log\\frac{p(\\boldsymbol{z})}{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})} d\\boldsymbol{z}\\\\ \u0026= \\mathbb E_{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})\\right] - \\mathbb D_{KL}\\big(p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\mid\\mid p(\\boldsymbol{z})\\big) \\end{aligned} $$这就是ELBO的数学定义：\n$$ ELBO(\\theta, \\phi) = \\mathbb E_{p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})\\right] - \\mathbb D_{KL}\\big(p_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\mid\\mid p(\\boldsymbol{z})\\big) $$1.5. Score Function Based Data Generating 前面已经介绍了很多解决概率化模型的方法，但是往往对模型有很多甚至有些过分苛刻的要求。下面将介绍 Score Function Based Method （基于得分函数的模型）。事实上这也是常见的 Diffusion 方法所使用的方法。对于一个概率分布 $p_\\theta(\\boldsymbol{x})$，可以定义得分函数：\n$$ s_\\theta(\\boldsymbol{x}) = \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x}) $$而 Score Function Based Method 则认为是通过学习的分函数来生成数据的方法。求前面提到的玻尔兹曼函数的得分函数，有：\n$$ \\begin{aligned} s_\\theta(\\boldsymbol{x}) \u0026= \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x})\\\\ \u0026= -\\nabla_{\\boldsymbol{x}} f_\\theta(\\boldsymbol{x}) - \\nabla_x\\log Z_\\theta\\\\ \u0026= -\\nabla_x f_\\theta(\\boldsymbol{x}) \\end{aligned} $$可以看到求得的的分函数并不依赖于归一化常数。这是一个非常重要的性质，显然的，使用 Score Funcction Based Method 时，我们没有对概率分布 $p_\\theta(\\boldsymbol x)$ 的性质有任何假设，这极大的扩展了模型的适用范围。与基于似然函数的模型类似，我们可以使用费歇尔信息（Fisher）信息来衡量数据集与模型的分数函数之间的相似水平：\n$$ \\mathbb E \\left[\\| \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x}) - s_\\theta(\\boldsymbol{x}) \\|^2\\right] $$使用费歇尔信息来衡量两个分布之间的 $\\mathcal{l}^2$ 距离是一个很直觉的方法，但是它并不易于操作，因为我们需要计算一个未知的分布的得分函数 $\\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x})$。幸运的是，我们有一种名为 Score Match 的方法在不需要求 $\\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x})$ 时最小化费歇尔信息。费希尔散度本身并不要求 $s_\\theta(\\boldsymbol{x})$ 是任何归一化分布的实际得分函数，它只是比较真实数据得分与基于得分的模型之间的 $\\mathcal l ^2$ 距离。事实上我们对 $s_\\theta(\\boldsymbol{x})$ 的唯一要求只是它的维度和真实分布相同而已。\n1.6. Langevin Dynamics 在学习了得分函数 $s_\\theta(\\boldsymbol{x}) \\approx \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x})$ 后，我们还需要将其转换回原本的概率分布，这实际上是要解决一个随机微分方程。这同样并不容易，但是我们可以直接跳过求原本的分布这一步，直接从 Score Function 采样，这个方法被称为郎之万动力学 (Langevin Dynamics)。郎之万动力学提供了一种马尔可夫的方法；具体而言，从任意先验分布 $x_0 \\sim \\pi(x)$ 初始化，然后进行如下迭代：\n$$ \\begin{array}{ll} x_{i+1} = x_i + \\epsilon \\nabla_{\\boldsymbol{x}} \\log p(\\boldsymbol{x}) + \\sqrt{2\\epsilon} \\boldsymbol{z}_i \u0026 i = 0, 1, \\cdots K \\end{array} $$其中 $\\boldsymbol{z_i} \\sim \\mathcal N(0, I)$，$\\epsilon \\to 0$, $K\\to \\infty$。实际上这个行为类似与数值微分方程的求解，后面的高斯分布项可以有效的避免陷入局部最优。事实上只要这个过程足够，误差几乎可以忽略。\n2. Diffusion 推荐大家看Lil大佬的博客，我后面的内容知识对这篇博客的拙略模仿。\n2.1. Forward diffusion process 好了，现在我们已经掌握了 diffusion 的所有原料，可以开始进入 diffusion 的世界了。假设我们有一个从真实数据分布采样的数据 $\\boldsymbol{x}_0 \\sim q(\\boldsymbol{x})$。我们定义一步的 diffusion 向前过程为向数据集添加一个很小的高斯噪声，有 $\\beta \\in (0, 1)$\n$$ p(x_t \\mid x_{t-1}) = \\mathcal N (x_t \\mid \\sqrt{1 - \\beta} x_{t-1}, \\beta_t I) $$我们需要酱这个过程重复 $T$ 次，从而产生一个数据序列：$X_1, X_2, \\cdots, X_T$。只要步骤数 $T$ 足够多，我们可以获得一个从原始数据缓慢增加噪声，直到一个服从各向异性的高斯分布。\n对于上面的加噪序列中的任意一时刻 $t$，令 $\\alpha_t = 1 - \\beta_t$，我我们可以直接卸出其加噪之后的形式:\n$$ \\begin{aligned} x_t \u0026= \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t}\\epsilon_{t-1}\\\\ \u0026= \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-2} + \\sqrt{1 - \\alpha_t\\alpha_{t-1}}\\epsilon_{t-2}\\\\ \u0026= \\cdots\\\\ \u0026= \\sqrt{\\hat \\alpha_t} x_0 + \\sqrt{1 - \\hat \\alpha_t}\\epsilon \\end{aligned} $$其中 $\\epsilon_i \\sim \\mathcal N(0, I)$, $\\hat \\alpha_t = \\prod_{i=1}^t \\alpha_i$。我们可以写出，对于序列中任意一帧 $x_t$，有分布：\n$$ p(x_t\\mid x_0) = \\mathcal N(x_t\\mid \\sqrt{\\hat \\alpha_t} x_0, \\sqrt{1 - \\hat \\alpha_t} I) $$2.2. Reverse diffusion process 如果说向前过程是向无噪声的数据增加噪声的过程，逆向过程就是一个去除噪声的过程。显然的，如果我们掌握了 $p(x_{t-1} \\mid x_t)$，那么可以从中抽样出向前过程中所添加的噪声，那么简单的减去这个噪声，就实现了数据集的降噪。事实上如果加噪的参数 $\\beta$ 足够小，应有 $p(x_{t-1} \\mid x_t)$ 服从高斯分布。事实上 diffusion 通过增加高斯噪声的方式将一个很难的任务（直接估计高维分布）变为多个简单的任务（逐渐降低一个数据的噪声），从而实现的高维数据估计。那么逆向过程可以写成：\n$$ \\begin{array}{c} p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \\\\ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)) \\end{array} $$在知道 $x_0$ 的情况下，通过贝叶斯法则，有：\n$$ p(x_{t-1}\\mid x_t, x_0) = p(x_t\\mid x_{t-1}, x_0) \\frac{p(x_{t-1} \\mid x_0)}{p(x_{t}\\mid x_0)} $$这一步将向后过程转换为向前的过程，向前过程我们都是已知的，有：\n$$ \\begin{array}{rcl} p(x_t\\mid x_{t-1}, x_0) \u0026\\sim\u0026 \\mathcal N(x_t \\mid \\sqrt{\\alpha_t} x_{t-1}, \\beta_t I)\\\\ p(x_{t-1}\\mid x_0)\u0026\\sim\u0026 \\mathcal N(x_{t-1} \\mid \\sqrt{\\hat \\alpha_{t-1}} x_0, (1 - \\hat \\alpha_{t-1}) I)\\\\ p(x_{t}\\mid x_0)\u0026\\sim\u0026 \\mathcal N(x_t \\mid \\sqrt{\\hat \\alpha_{t}} x_0, (1 - \\hat \\alpha_t) I)\\\\ \\end{array} $$那么我们可以直接将逆向的形式展开：\n$$ \\begin{aligned} \u0026p(x_t\\mid x_{t-1}, x_0)\\\\ \u0026\\propto \\exp \\Big(-\\frac{1}{2} \\big(\\frac{(\\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\big) \\Big) \\\\ \u0026= \\exp\\Big( -\\frac{1}{2} \\big( (\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}) \\mathbf{x}_{t-1}^2 - (\\frac{2\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0) \\mathbf{x}_{t-1} + C(\\mathbf{x}_t, \\mathbf{x}_0) \\big) \\Big)\\\\ \\end{aligned} $$其中 $C(\\mathbf{x}_t, \\mathbf{x}_0)$ 是不包含 $x_{t-1}$ 的函数。对于一般的高斯函数 $\\mathcal G(x\\mid \\mu, \\sigma) \\propto \\exp\\left(-\\frac 1 2 \\left(\\frac{1}{\\sigma^2}x^2 + \\frac{2\\mu}{\\sigma^2} + \\frac{\\mu^2}{\\sigma^2}\\right)\\right)$。不妨先假设：\n$$ \\begin{array}{c} q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}) \\end{array} $$与上面的形式对应，我们可以总结出：\n$$ \\begin{aligned} \\frac{1}{\\sigma^2} \u0026= \\frac{1}{\\tilde{\\beta}_t} = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\\\\ \\tilde{\\beta}_t \u0026= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t\\\\ \\frac{\\mu}{\\sigma^2} \u0026= \\frac{\\mu(x_t, x_0)}{\\tilde{\\beta_t}}=\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\\\ \\mu(x_t, x_0) \u0026= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\mathbf{x}_0\\\\ \\end{aligned} $$根据 $x_t = \\sqrt{\\hat \\alpha_t} x_0 + \\sqrt{1 - \\hat \\alpha_t}\\epsilon$，有：\n$$ x_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}}\\left(x_t - \\sqrt{1 - \\bar\\alpha_t} \\epsilon_t\\right) $$带入上面求出的参数，有：\n$$ \\begin{aligned} \\tilde{\\boldsymbol{\\mu}}_t \u0026= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t} \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t) \\\\ \u0026= \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big) \\end{aligned} $$其中 $\\epsilon_t$ 为模型需要预测的用于降噪的噪声。现在问题似乎解决了，DDPM（denoising diffusion probabilistic models）的推理可以总结成下面几个步骤：\n通过训练好的神经网络，结合 $x_t, t$（作为参数）预测高斯噪声 $\\epsilon_t$，通过 $\\tilde{\\mu_t} = \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big)$ 求出均值。\n根据 $\\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t$ 得到方差。在 GLIDE 中，则通过神经网络预测出该方差。\n根据 $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I})$ 实现去噪。\n2.3. Training 在了解了 diffusion 的推断过程后，下一个问题是如何训练我们的模型让它预测出正确的 $\\mu_\\theta(x_t, t)$ 和 $\\Sigma_\\theta(x_t, t)$。可以回忆一下在 VAE 介绍过的ELBO，此处亦有应用。由于 KL 散度非负，总有：\n$$ \\begin{aligned} -\\log p_\\theta(x) \u0026\\leq -\\log p_\\theta(x) + D_{KL}\\left(q_\\phi(\\boldsymbol{x}_{1:T} \\mid \\boldsymbol{x}_0) \\mid p_\\theta(\\boldsymbol{x}_{1:T} \\mid \\boldsymbol{x}_0)\\right)\\\\ \u0026= -\\log p_\\theta(x) + \\mathbb E_{q_\\phi(\\boldsymbol{x}_{1:T} \\mid \\boldsymbol{x}_0)}\\left[\\log \\frac{q_\\phi(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0)p_\\theta(\\boldsymbol{x}_0)}{p_\\theta(\\boldsymbol{x}_{0:T}) }\\right]\\\\ \u0026= -\\log p_\\theta(x) + \\mathbb E_{q_\\phi(\\boldsymbol{x}_{1:T} \\mid \\boldsymbol{x}_0)}\\left[\\log \\frac{q_\\phi(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0)}{p_\\theta(\\boldsymbol{x}_{0:T}) } + \\log p_\\theta(\\boldsymbol{x}_0)\\right]\\\\ \u0026= \\mathbb E_{q_\\phi(\\boldsymbol{x}_{1:T} \\mid \\boldsymbol{x}_0)}\\left[\\log \\frac{q_\\phi(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0)}{p_\\theta(\\boldsymbol{x}_{0:T})}\\right] \\end{aligned} $$令：\n$$ \\boldsymbol{L}_{VIB} = \\mathbb E_{q_\\phi(\\boldsymbol{x}_{1:T} \\mid \\boldsymbol{x}_0)}\\left[\\log \\frac{q_\\phi(\\boldsymbol{x}_t \\mid \\boldsymbol{x}_0)}{p_\\theta(\\boldsymbol{x}_{0:T})}\\right] \\geq -\\mathbb E_{q}\\left[\\log p_\\theta(x)\\right] $$为了保证每一项都是解析的，需要做如下变形：\n$$ \\begin{aligned} L_\\text{VLB} \u0026= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ \\log\\frac{\\prod_{t=1}^T q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{ p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t) } \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\Big( \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)}\\cdot \\frac{q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}\\vert\\mathbf{x}_0)} \\Big) + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0)} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)} + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big]\\\\ \u0026= \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_T)} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1) \\Big] \\\\ \u0026= \\mathbb{E}_q [\\underbrace{D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T} + \\sum_{t=2}^T \\underbrace{D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t))}_{L_{t-1}} \\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)}_{L_0} ] \\end{aligned} $$我们可以标记下面的每一项为：\n$$ \\begin{aligned} L_\\text{VLB} \u0026= L_T + L_{T-1} + \\dots + L_0 \\\\ \\text{where } L_T \u0026= D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T)) \\\\ L_t \u0026= D_\\text{KL}(q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})) \\text{ for }1 \\leq t \\leq T-1 \\\\ L_0 \u0026= - \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1) \\end{aligned} $$可以看到，除去$- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)$项外，其余项均均为两个高斯分布的 KL 散度，可以解析的计算。并且由于向前过程 $q$ 没有可以学习的参数，$x_T$ 为纯高斯分布，所以 $L_T$ 在优化过程总可以忽略。我们几乎只需要关注 $L_t$ 项。由于 $L_t$ 项几乎可以认为是两个高斯函数 $q(\\mathbf{x}_t \\vert \\mathbf{x}_{t+1}, \\mathbf{x}_0)$ 和 $p_\\theta(\\mathbf{x}_t \\vert\\mathbf{x}_{t+1})$ 的KL散度，可以直接解析的写出：\n$$ L_t = \\mathbb{E}_{q} \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) \\|^2 \\Big] + C\\\\ $$其中 $C$ 为与待优化变量 $\\theta$ 无关的常数，可以忽略。将之前求出的均值的形式：\n$$ Math $$带入 $L_t$ 中，有：\n$$ \\begin{aligned} L_t \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) \\|^2 \\Big] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\|\\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\| \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_t \\Big) - \\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\Big) \\|^2 \\Big] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ \u0026= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned} $$从上式可以看出，diffusion训练的核心就是取学习高斯噪声 $\\bar{z}_t$ 与 $z_\\theta$ 之间的最大似然。经验上讲，Denoising Diffusion Probabilistic Models 发现简化掉加权项后效果更好：\n$$ \\begin{aligned} L_t^\\text{simple} \u0026= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ \u0026= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned} $$最终的简化目标为：\n$$ L_{simple} = L_t^{simple} + C $$好了，现在训练的问题也解决了。diffusion 的训练问题可以总结为下面几个步骤：\n获得输入 $x_0$，从 $1, 2, \\cdots, T$ 中抽样出一个 $t$ 从标准高斯分布中抽样出一个噪声 $\\epsilon_t \\sim \\mathcal N(0, I)$ 最小化 $\\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big]$ 现在还有最后一个问题，如何用自然语言控制图片的生成。事实上这也很简单，哪著名的 stable diffusion 为例，它使用了 CLIP 自然语言端的编码器，将自然语言编码为一个embedding。将这个embedding 作为 Unet 的输入监督噪声的预测。这样就实现建立了 CLIP 的 embedding 与生成的图片之间的关系。\n3. DreamFusion 现在我们已经介绍了使用 diffusion 实现的 2D AIGC。但是想将之直接应用于 3D AIGC 是不现实的。局限显而易见，相比人类已经积累了数亿计的二维图像数据（事实上 clip 在训练时就用了 4 亿规模的数据），但是三维数据却小的可怜。因此，3D AIGC 主要分为直接使用三维你数据训练和从二维 diffusion 迁移两条路。关于这两条技术路线的具体讨论可以参考 胡渊明 的博客 Taichi NeRF (下): 关于 3D AIGC 的务实探讨。我们要介绍的 DreamFusion 可以说是第二条路线的奠基之作。\n3.1. 基本实现 一个直观的想法是直接使用训练完成的 2D diffusion 模型生成照片，之后使用 nerf/3DGS 来三维重建。但是事实上这几乎是不可能的。因为不论是 nerf 还是 3DGS，对照片的几何一致性和位姿都有很高的要求，同时还需要提供相机的位姿和焦距。但是现有的 diffusion 模型只能通过很粗糙的自然语言来约束生成图片的几何，更遑论按照一定的相机内参来生成图片了。\n但是事实上 diffusion module 中确实存储了巨量的真实的三维几何在二维的投影，我们目前面临的问题是如何把其中的信息 “取出来”。在前面的讨论中我们知道，diffusion module 生成图片的过程是通过一个 unet（事实上也有用transformer的）来预测噪声。通过不断预测高斯噪声来恢复图片原有的结构。那么只要预测的噪声足够准确，从噪声中恢复结构的图片就会足够准确。那么 diffusion 的 lossfunciton 可以写成一个 UNet 预测的噪声 $\\rm{UNet}(\\alpha x_{render} - \\sigma_t \\epsilon_t )$ 与 添加的噪声 $\\epsilon$ 之间差异。其中 $\\|\\cdot\\|$ 表示衡量两个概率分布的差异：\n$$ \\mathcal L_{diff} = \\mathbb E\\left[ \\| \\epsilon_{pred} - \\epsilon \\|\\right] $$事实上，对于一个训练好的 diffusion module，与一个随机初始的 Nerf 网络相比，我们可以认为它几乎是准确的。那么一个显然的思路是模仿 diffusion 的训练过程，通过给 Nerf 渲染的照片添加噪声，再用训练好的 diffusion module 降噪。我们可以写出此时的 Loss：\n$$ \\mathcal L_{diff} = \\mathbb E\\left[ \\omega(t) \\| \\rm{UNet}(\\alpha x_{render} - \\sigma_t \\epsilon_t ) - \\epsilon \\|\\right] $$其中 $\\omega(t)$ 是一个与 timestep 有关的函数，是一个收到超参数控制的用于调整生成过程的函数，$x_{render}$ 为 Nerf 渲染的图片。\n这是 dreamfusion 的 pipline。首先通过 Nerf 出的照片 $rendering$ 添加噪声 $\\epsilon \\sim \\mathcal N(0, I)$。添加噪声后的照片可以写作：$z_t \\sim U(0, 1)$。之后通过 Unet 预测出降噪后的图片 $\\check x (z_t \\mid y; t)$；之后将求出该步骤中去除的噪声 $\\epsilon_\\phi(z_t\\mid y; t)$。有了预测的噪声后，我们将预测的噪声与设计添加的噪声做差，就可以作为训练 Nerf 的 Loss 了。我们可以看到，dreamfusion 过程实际上是用 Nerf 作为几何的监督，通过 diffusion 来产生新的 “数据分布”，从而解决传统的 diffusion 缺乏几何一致性的问题。实际上是用 diffusion 生成的图片作为监督，来驱动优化 Nerf 模型，即：\n$$ \\boldsymbol{\\theta} = \\arg\\min_\\theta \\mathbb E\\left[ \\omega(t) \\| \\rm{UNet}(\\alpha x_{render} - \\sigma_t \\epsilon_t ) - \\epsilon_t \\|_2^2 \\right] $$3.2. SDS Loss：从 diffusion module 中提取知识 事情似乎解决了。但是自己观察 Loss 后不难发现，这个 Loss 几乎是无法计算的。我们希望通过 $\\mathcal L_{diff}$ 传递梯度，需要计算 $\\frac{\\partial \\mathcal L_{diff}}{\\partial x_{render}}$，这个过程需要计算整个 UNet 的梯度。而一般 UNet 的参数量及其巨大，只是单纯的进行 diffusion 推理可能还能吃得消，但是我们需要 nerf 每一轮训练的每个 timestep 都完整的计算整个 UNet 的梯度，这在实现上几乎是不可行的。因此我们需要找到一个办法，跳过 UNet 的梯度传播，直接计算 $\\mathcal L_{diff}$。\n我们不妨写出 $\\mathcal L_{diff}$ 对 $\\boldsymbol{\\theta}$ 的梯度：\n$$ \\nabla_{\\theta} \\mathcal{L}_{Diff }(\\phi, x=g(\\theta))=\\mathbb{E}_{t, \\epsilon}[w(t) \\underbrace{\\left(\\hat{\\epsilon}_{\\phi}\\left(z_{t} ; y, t\\right)-\\epsilon\\right)}_{\\text{Noise Residual} } \\underbrace{\\frac{\\partial \\hat{\\epsilon}_{\\phi}\\left(z_{t} ; y, t\\right)}{z_{t}}}_{\\text{U-Net Jacobian} } \\underbrace{\\frac{\\partial x}{\\partial \\theta}}_{\\text{Nerf Jacobian} }] $$其中从 $\\alpha x_{render} - \\sigma_t \\epsilon_t$ 到 $x$ 的梯度传播为 $\\frac{\\partial \\alpha x_{render} - \\sigma_t \\epsilon_t}{\\partial x} = \\alpha I$，直接归纳入 $\\omega(t)$ 。此外，直接省略 UNet 的梯度，有：\n$$ \\nabla_{\\theta} \\mathcal{L}_{SDS}(\\phi, x=g(\\theta)) \\triangleq \\mathbb{E}_{t, \\epsilon}\\left[w(t)\\left(\\hat{\\epsilon}_{\\phi}\\left(z_{t} ; y, t\\right)-\\epsilon\\right) \\frac{\\partial x}{\\partial \\theta}\\right] $$这就是 SDS Loss Function。事实上在实际操作时，我们通过 KL 三度来衡量两个分布之间的误差。\n$$ \\nabla_{\\theta} \\mathcal{L}_{SDS}(\\phi, x = g(\\theta))=\\nabla_{\\theta} \\mathbb{E}_{t}\\left[\\sigma_{t} / \\alpha_{t} w(t) \\text{KL}\\left(q\\left(z_{t} | g(\\theta) ; y, t\\right) \\| p_{\\phi}\\left(z_{t} ; y, t\\right)\\right)\\right] $$可以说 $\\nabla_{\\theta} \\mathcal{L}_{SDS}(\\phi, x = g(\\theta))$ 是从 diffusion 估计出的分数函数 (Score Function)。\n4. Reference 由浅入深了解 Diffusion Model\nA Tutorial of Normalizing Flow Module\nGenerative Modeling by Estimating Gradients of the Data Distribution \u0026mdash;- Yang Song\nWhat are Diffusion Models?\nSliced score matching: A scalable approach to density and score estimation\nFrom Autoencoder to Beta-VAE\nDreamFusion: Text-To-3D Using 3D Diffusion\n5. Append 5.1. Boltzmann Distribution 玻尔兹曼分布是统计力学中描述粒子在不同能级上分布概率的重要概念。以下是详细的推导步骤和依据。\n系统的熵定义为：\n$$ S = -k_B \\sum P_i \\ln P_i $$其中：\n$P_i$ 是系统处于第 $i$ 个能量状态的概率， $k_B$ 是玻尔兹曼常数。 推导过程中需要满足以下约束条件：\n概率归一化条件： $$ \\sum P_i = 1 $$ 固定平均能量： $$ \\sum P_i E_i = \\langle E \\rangle $$其中：\n$E_i$ 是第 $i$ 个能量状态的能量， $\\langle E \\rangle$ 是系统的平均能量。 为了在约束条件下最大化熵，引入拉格朗日乘子 $\\alpha$ 和 $\\beta$，构造拉格朗日函数：\n$$ \\mathcal{L} = -k_B \\sum P_i \\ln P_i + \\alpha \\left( \\sum P_i - 1 \\right) + \\beta \\left( \\sum P_i E_i - \\langle E \\rangle \\right) $$对每个 $P_j$ 求偏导并设为零：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial P_j} = -k_B (\\ln P_j + 1) + \\alpha + \\beta E_j = 0 $$解得：\n$$ \\ln P_j = \\frac{\\alpha}{k_B} - 1 - \\frac{\\beta E_j}{k_B} $$因此：\n$$ P_j = A e^{-\\gamma E_j} $$其中：\n$A = e^{(\\alpha / k_B - 1)}$， $\\gamma = \\beta / k_B$。 通过归一化条件 $\\sum P_j = 1$，得到：\n$$ A = \\frac{1}{Z} $$其中，配分函数 $Z = \\sum e^{-\\gamma E_j}$。\n通过热力学关系确定 $\\gamma = 1 / (k_B T)$，其中 $T$ 是温度。\n最终的玻尔兹曼分布为： $$ P_j = \\frac{e^{-E_j / (k_B T)}}{Z} $$ 其中，配分函数 $Z = \\sum e^{-E_j / (k_B T)}$。\n5.2. DDPM 和 Score Module 之间的关系 我们不妨先复习一下，前面有讲到的 DDPM 的更新步骤为：\n$$ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}t}}\\, \\epsilon\\theta(x_t,t) \\right) + \\sqrt{\\sigma_t^2}\\, z $$此外，Langevin Dynamics 的迭代步骤为：\n$$ x_{k+1} = x_k + \\frac{\\eta}{2} \\nabla_{x} \\log p(x_k) + \\sqrt{\\eta}\\, z_k,\\quad z_k\\sim \\mathcal{N}(0,\\mathbf{I}) $$对于一个高斯分布 $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}t}\\,x_0,\\, (1-\\bar{\\alpha}t)\\mathbf{I})$，其 score function（即 $\\nabla{x_t} \\log q(x_t|x_0)$）可以直接计算：\n$$ \\nabla{x_t} \\log q(x_t|x_0) = -\\frac{1}{1-\\bar{\\alpha}_t} \\left(x_t - \\sqrt{\\bar{\\alpha}_t}\\,x_0\\right) $$利用前向过程的表达式 $x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon$，我们有：\n$$ x_t - \\sqrt{\\bar{\\alpha}_t}\\,x_0 = \\sqrt{1-\\bar{\\alpha}t}\\,\\epsilon $$因此，score function 可写为：\n$$ \\nabla{x_t} \\log q(x_t|x_0) = -\\frac{\\sqrt{1-\\bar{\\alpha}_t}}{1-\\bar{\\alpha}_t}\\,\\epsilon = -\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}\\,\\epsilon $$在 DDPM 的训练过程中，我们用 $\\epsilon_\\theta(x_t, t)$ 来预测前向过程中实际添加的噪声 $\\epsilon$。如果网络训练得足够好，就有：\n$$ \\epsilon_\\theta(x_t, t) \\approx \\epsilon $$那么，根据上面的推导，我们可以得到：\n$$ \\nabla_{x_t} \\log q(x_t|x_0) \\approx -\\frac{1}{\\sqrt{1-\\bar{\\alpha}t}}\\,\\epsilon\\theta(x_t,t) $$这就说明，网络预测的噪声与数据在当前噪声水平下的 score function 之间只相差一个与时间（或噪声水平）有关的比例因子 $\\frac{1}{\\sqrt{1-\\bar{\\alpha}_t}}$。\n因此我们知道，DDPM实质上是学习了一个隐式的 Score Function。DDPM的反向过程（去噪过程）在数学上可以看作是对目标数据分布（或其在不同噪声水平下的变体）进行梯度上升采样的离散化实现，而Langevin Dynamics正是利用score function（即 $\\nabla_x \\log p(x)$）进行采样的方法。\n","permalink":"https://wangjv0812.cn/2024/12/dreamfusion/","summary":"\u003ch2 id=\"1-使用神经网路进行数据生成\"\u003e1. 使用神经网路进行数据生成\u003c/h2\u003e\n\u003cp\u003e使用神经网络生成一个高维度数据是机器学习中非常重要的一个工作。我们假设数据集 $\\left\\{\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n}\\right\\}$ 为一个大小为$n$的数据集，该数据集统一的服从一个概率分布 $p_{data}(\\boldsymbol{x})$ 。我们假设对数据集的抽样都是独立同分布的，即：\u003c/p\u003e\n$$\n\\left\\{\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_n}\\right\\} \\sim p_{data}(\\boldsymbol{x})\n$$\u003cp\u003e那么丛现有数据生成新的数据的核心就是使用神经网络学习这个概率分布。不妨假设学习的概率分布为 $\\hat p_\\theta(\\boldsymbol x)$。我们会希望 $\\hat p_\\theta(\\boldsymbol x)$ 尽可能的接近 $p_{data}(\\boldsymbol(x))$ 。为了衡量真是分布和我们学习的分布之间的差距，我们需要定义一个距离函数 $D(\\cdot \\mid \\cdot)$ 我们可以定义优化目标：\u003c/p\u003e\n$$\n\\hat \\theta = \\arg \\min_{\\theta} D\\left(p_{data}(\\boldsymbol{x}) \\mid \\hat p_\\theta(\\boldsymbol x) \\right)\n$$\u003cp\u003e关于距离函数，我们可以定义 $D(\\cdot \\mid \\cdot)$ 为 f-divergence 定义为：\u003c/p\u003e\n$$\nD_f(p_{data}(\\boldsymbol(x)) \\mid \\hat p_\\theta(\\boldsymbol x)) = \\int p_\\theta(\\boldsymbol x) f \\left(\\frac{p_{data}(\\boldsymbol x)}{p_\\theta(\\boldsymbol x)}\\right) d\\boldsymbol x\n$$\u003cp\u003e不妨取 $f(x) = x\\log x$ ，我们可以得到 KL 散度：\u003c/p\u003e\n$$\n\\begin{aligned}\nD_{KL}(p_{data}(\\boldsymbol(x)) \\mid \\hat p_\\theta(\\boldsymbol x))\n\u0026= \\int p_{data}(\\boldsymbol x) \\log \\frac{p_{data}(\\boldsymbol x)}{p_\\theta(\\boldsymbol x)} d\\boldsymbol x\\\\\n\u0026= \\mathbb E_{p_{data}(\\boldsymbol x)}\\left[ \\log \\frac{p_{data}(\\boldsymbol x)}{p_\\theta(\\boldsymbol x)} \\right]\n\\end{aligned}\n$$\u003cp\u003e我们可以用抽样的均值来代替期望，有：\u003c/p\u003e","title":"DreamFusion"},{"content":"ps: 为了更快的写出来这个文档，我不会很注意公式的粗细体，请见谅。\n1. 最大后验估计 1.1. 状态估计问题描述 我们假设有一个线性系统，其噪声可以用高斯函数来描述。这个线性系统可以如下描述：\n$$ \\begin{array}{l} x_k = A_{k-1}x_{k-1} + v_k + \\omega_k\\\\ y_k = Cx_k + n_k \\end{array} $$其中，有：\n$$ \\begin{array}{ll} \\text{初始噪声} \u0026 x_0 \\sim \\mathcal G (x \\mid 0, P_0) \\\\ \\text{过程噪声} \u0026 x_k \\sim \\mathcal G (x \\mid 0, Q_k) \\\\ \\text{观测噪声} \u0026 \\omega_k \\sim \\mathcal G (x \\mid 0, R_k) \\end{array} $$我们认为除了系统的输入 $v_k$ 之外，其余所有变量皆为随机变量。此外我们称 $A_k$ 为状态转移矩阵，$C_k$ 为观测矩阵。对于这个系统而言，系统的初始状态 $x_0$、系统输入 $v_k$ 和 系统输出是已知的。状态估计的目标就是通过这些已知的参数，估计出系统的状态 $x_k$。\n1.2. 最大后验估计 最大后验估计需要完成如下一个优化问题：\n$$ \\hat x = \\arg \\max_{x} p(x \\mid y, v) $$对于上面这个问题，通过贝叶斯定理，可以变形得：\n$$ \\begin{aligned} p(x\\mid y, v) \u0026= \\frac{p(x, y, v) p(x, v) p(v)}{p(y, v) p(x,v) p(v)}\\\\ \u0026= \\frac{p(y\\mid x, v) p(x \\mid v)}{p(y \\mid v)} \\end{aligned} $$显然，$y$ 与 $v$ 无关，那么有：$p(y\\mid x, v) = p(y\\mid x)$；$p(y \\mid v)$ 与优化目标无关，可以忽略。那么优化目标可以整理出：\n$$ \\hat x = p(y \\mid x)p(x \\mid v) $$我们有假设，每次观测之间相互独立，那么应该有：\n$$ p(y \\mid x) = \\prod_{k=0}^{K} p(y_k \\mid x_k) $$此外，通过贝叶斯定理，我们有，我们可以分解 $p(x \\mid v)$ 为：\n$$ p(x \\mid v) = p(x_0 \\mid \\check x_0)\\prod_{k=1}^{K} p(x_k \\mid x_{k-1}, v_k) $$我们展开上面的高斯分布的形式：\n$$ \\begin{array}{c} p(x_0 \\mid \\check x_0) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det P_0}} \\exp \\left( -\\frac{1}{2} (x_0 - \\check x_0)^T P_0^{-1} (x_0 - \\check x_0) \\right)\\\\ p(x_k \\mid x_{k-1}, v_k) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det Q_k}} \\exp \\left( -\\frac{1}{2} (x_k - A_{k-1}x_{k-1} - v_k)^T Q_k^{-1} (x_k - A_{k-1}x_{k-1} - v_k) \\right)\\\\ p(y_k \\mid x_k) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det R_k}} \\exp \\left( -\\frac{1}{2} (y_k - C_k x_k)^T R_k^{-1} (y_k - C_k x_k) \\right) \\end{array} $$我们这里停一下，思考 $p(x_k \\mid x_{k-1}, v_k)$ 的具体形式，不妨做如下变形：\n$$ \\begin{aligned} p(x_k \\mid x_{k-1}, v_k) \u0026= \\frac{1}{\\sqrt{(2\\pi)^2 \\det Q_k}} \\exp \\left( -\\frac{1}{2} (x_k - A_{k-1}x_{k-1} - v_k)^T Q_k^{-1} (x_k - A_{k-1}x_{k-1} - v_k) \\right)\\\\ \u0026= \\frac{1}{\\sqrt{(2\\pi)^2 \\det Q_k}} \\exp \\left( -\\frac{1}{2} (v_k - (x_k - A_{k-1}x_{k-1}))^T Q_k^{-1} (v_k - (x_k - A_{k-1}x_{k-1})) \\right)\\\\ \u0026= \\mathcal G(v_k \\mid x_k - A_{k-1}x_{k-1}, Q_k) \\end{aligned} $$可以看到，过程协方差矩阵 $Q_k$ 可以理解为通过状态评估系统输入的协方差矩阵，这个形式有助于理解后面协方差提升形式有很大意义。我们对上面的优化目标取 $\\ln$，并忽略常数项有：\n$$ \\begin{aligned} J(x) =\u0026 \\frac{1}{2} \\sum_{k=0}^{K} (y_k - C_k x_k)^T R_k^{-1} (y_k - C_k x_k)\\\\ \u0026+ \\frac{1}{2} \\sum_{k=1}^{K} (v_k - (x_k - A_{k-1}x_{k-1}))^T Q_k^{-1} (v_k - (x_k - A_{k-1}x_{k-1}))\\\\ \u0026+ \\frac{1}{2} (x_0 - \\check x_0)^T P_0^{-1} (x_0 - \\check x_0) \\end{aligned} $$此时的优化目标转化为求上式的最大值。但是直接计算上面的加法优化形式并不清晰，我们不妨将待估计的状态 $x_{1:k}$ 和 已知的变量 $y_{0:k}$ 和 $v_{1:k}$ 分开堆叠起来，那么有：\n$$ \\begin{array}{c} x = \\begin{bmatrix} x_1 \u0026 x_2 \u0026 \\cdots \u0026 x_K \\end{bmatrix}^T\\\\ z = \\begin{bmatrix} x_0 \\mid \u0026 v_1\u0026 v_2\u0026 \\cdots \u0026 v_k \\mid \u0026 y_0\u0026 y_1\u0026 \\cdots \u0026 y_k \\end{bmatrix}^T \\end{array} $$类似的，我们还需要将状态转移矩阵 $A$ 和 观测矩阵 $C$ 堆叠起来。需要注意，这里的堆叠主要是系统将状态 $x$ 映射到系统输出 $y$ 和系统输入 $v$。\n$$ H = \\begin{bmatrix} I\u0026 \u0026 \u0026 \\\\ -A_0\u0026 I\u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 -A_{K-1} \u0026 I\\\\ \\hline C_0\u0026 \u0026 \u0026 \\\\ \u0026 C_1\u0026 \u0026 \\\\ \u0026 \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 \u0026 C_K\\\\ \\end{bmatrix}^T\\\\ $$显然 $Hx$ 将 $x$ 映射到 $\\check z$ 上，但是此时的 $\\check z$ 是通过 $x$ 计算得到的，与真实的 $z$ 会存在残差从而可以驱动优化。此外我们还需要堆叠协方差：\n$$ W = \\begin{bmatrix} P_0\u0026 \u0026 \u0026 \\\\ \u0026 Q_1\u0026 \u0026 \\\\ \u0026 \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 \u0026 Q_K\\\\ \\hline \u0026 \u0026 \u0026 \u0026 R_0\u0026 \u0026 \u0026\\\\ \u0026 \u0026 \u0026 \u0026 \u0026 R_1\u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 \u0026 R_K\\\\ \\end{bmatrix} $$此时前面对对 $p(x\\mid v)$ 的变形起了作用。事实上矩阵 $H$ 描述了你通过状态估计已知参数 $z$ 的置信度。那么有：\n$$ p(z \\mid x) = \\mathcal G(z \\mid Hx, W) $$我们对上式求导，有：\n$$ \\begin{array}{c} \\frac{\\partial p(z \\mid x)}{\\partial x} = -H^TW^{-1}(z - Hx) = 0\\\\ (H^TW^{-1}H)\\hat x = H^TW^{-1}z \\end{array} $$解决上面这个问题，可以通过直接求 $(H^TW^{-1}H)$ 的彭罗斯逆。但是事实上矩阵 $H^TW^{-1}H$ 是稀疏的，会有更简单的解法叫Cholesky 平滑算法，但是这与这篇文章的主题无关。\n读者看到上面这个形式时可能会说，这在数学上很严谨，但是它一点都不概率！这里有一个更\u0026rsquo;物理\u0026rsquo;的解释：\n$W$ 描述了已知信息的置信程度 $(H^TW^{-1}H)^T$ 描述了将已知信息的置信变换到了状态 $x$ 上的置信。 那么 $(H^TW^{-1}H)\\hat x$ 描述了通过通过协方差的信息矩阵对状态 $x$ 的进行的加权。 $W^{-1}z$ 描述了直接通过协方差的信息矩阵对已知信息的加权。 $H^TW^{-1}z$ 描述了加权的信息已知信息变换到状态上的结果。 这个理解是很直观的，事实上方程两边就是同一件事！这也解释了最大后验估计相较于直接求最小二乘解的优势所在：我们通过协方差对已知信息进行了加权，从而使得估计的结果更接近真实值，降低分布过于离谱的观测的影响。\n2. 卡尔曼滤波 可以说 MAP 是线性高斯系统下最优的估计方法，但是它有如下几个明显的问题：\n它利用了所有时刻的数据，是离线估计的，无法再现更新 我们的系统具有马尔可夫性，不妨假设我们已经获得了关于前一时刻的估计 $\\hat x_{k-1}$ 和其协方差 $P_{k-1}$。可以直接将 MAP 问题变化为一个从 $k-1$ 时刻开始，到 $k$ 时刻结束的优化问题。此时如果继续使用MAP的方法推导 Kalman Filter 会很复杂，这里将介绍一个更简单的方法：贝叶斯推断。\n如果将 MAP 理解为求解后验概率 $p(x_k \\mid y_{1:k}, v_{1:k})$ 概率最大点的状态 $x_k$， 贝叶斯推断则是求解后验概率 $p(x_k \\mid y_{1:k}, v_{1:k})$ 的期望。对于线性高斯系统而言，后验概率和贝叶斯估计是等价的。但是对于非线性系统而言就不一定来。\n但至少对我们现在所面临的问题而言，它足够好用。\n2.1. 贝叶斯推断推导 kalman filter 在 k-1 时刻，状态的先验有：\n$$ p(x_{k-1} \\mid x_0, y_{1:k-1}, v_{1:k-1}) = \\mathcal G(x\\mid \\hat x_{k-1}, P_{k-1}) $$对于预测，可以写出：\n$$ \\begin{array}{c} p(\\check x_{k} \\mid x_{k-1}, v_k) = \\mathcal G(x\\mid \\hat x_k, \\check P_k)\\\\ \\end{array} $$对于上式中的均值，我们可以按照定义写出：\n$$ \\begin{aligned} \\check x_k \u0026= E[A_kx_{k-1} + v_k]\\\\ \u0026= A_{k-1}E[x_{k-1}] + E[v_k] \\\\ \u0026= A_{k-1}\\hat x_{k-1} + v_k \\end{aligned} $$对于预测的协方差，有：\n$$ \\begin{aligned} \\check P_k \u0026= E[(A_{k-1}x_{k-1} + v_k - A_{k-1}\\hat x_{k-1} - v_k)(A_{k-1}x_{k-1} + v_k - A_{k-1}\\hat x_{k-1} - v_k)^T]\\\\ \u0026= E[(A_{k-1}(x_{k-1} - \\hat x_{k-1}))(A_{k-1}(x_{k-1} - \\hat x_{k-1}))^T] + E[\\omega_k\\omega_k^T]\\\\ \u0026= A_{k-1}E[((x_{k-1} - \\hat x_{k-1}))((x_{k-1} - \\hat x_{k-1}))^T]A_{k-1}^T + E[\\omega_k\\omega_k^T]\\\\ \u0026= A_{k-1}\\hat P_{k-1} A_{k-1}^T + Q_k \\end{aligned} $$ps: 上面的推导中省略了一步，即 $\\check x_k$ 与 $\\omega_k$ 独立，因此$\\check x_k$ 与 $\\omega_k$的互相关即交叉项 $E[\\check x_k\\omega_k^T] = 0$。\n总结上面的推导，有：\n$$ \\begin{array}{c} \\check x_k = A_{k-1}\\hat x_{k-1} + v_k\\\\ \\check P_k = A_{k-1}P_{k-1}A_{k-1}^T + Q_k \\end{array} $$我们可以很容易的写出 $x_k, y_k$ 的联合概率分布：\n$$ p(x_k, y_k \\mid \\check x_{k-1}, v_k, y_{1:k-1}) = \\mathcal G \\left( \\begin{bmatrix} x\\\\ y \\end{bmatrix} \\mid \\begin{bmatrix} \\check x_k\\\\ C_k\\check x_k \\end{bmatrix}, \\begin{bmatrix} \\check P_k \u0026 \\check P_kC_k^T\\\\ C_k\\check P_k\u0026 C_k\\check P_kC_k^T + R_k \\end{bmatrix} \\right) $$2.2. 数学基础：联合高斯概率分布的分解（推断） 对于一个形如下式的联合高斯概率分布：\n$$ p(x, y) = \\mathcal G \\left( \\begin{bmatrix} x\\\\ y \\end{bmatrix} \\mid \\begin{bmatrix} \\mu_x\\\\ \\mu_y \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{xx} \u0026 \\Sigma_{xy}\\\\ \\Sigma_{yx} \u0026 \\Sigma_{yy} \\end{bmatrix} \\right) $$我们有舒尔补（Schur complement）：\n$$ \\begin{bmatrix} \\Sigma_{xx} \u0026 \\Sigma_{xy}\\\\ \\Sigma_{yx} \u0026 \\Sigma_{yy} \\end{bmatrix} = \\begin{bmatrix} I\u0026 \\Sigma_{xy}\\Sigma_{yy}^{-1}\\\\ 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} \\Sigma_{xx} -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx} \u0026 0\\\\ 0 \u0026 \\Sigma_{yy} \\end{bmatrix} \\begin{bmatrix} I\u0026 0\\\\ \\Sigma_{yy}^{-1}\\Sigma_{yx} \u0026 I \\end{bmatrix} $$对 Schur Complement 求逆，有：\n$$ \\begin{bmatrix} \\Sigma_{xx} \u0026 \\Sigma_{xy}\\\\ \\Sigma_{yx} \u0026 \\Sigma_{yy} \\end{bmatrix}^{-1} = \\begin{bmatrix} I\u0026 0\\\\ -\\Sigma_{yy}^{-1}\\Sigma_{yx} \u0026 I \\end{bmatrix}\\begin{bmatrix} \\Sigma_{xx} -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}^{-1} \u0026 0\\\\ 0 \u0026 \\Sigma_{yy}^{-1} \\end{bmatrix} \\begin{bmatrix} I\u0026 -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\\\ 0 \u0026 I \\end{bmatrix} $$思考：本质上是方阵不好求逆，所以很难解析的把高斯分布的形式分离开，所以用了 Schur Complement。事实上这个方法有时也会带来麻烦。因为分解有 LDU 和 UDL 两种，有时 LUD 分解的结论会更简洁，有时 UDL 会更简洁。这时需要 Sherman-Morrison-Woodbury 等式来帮忙。但是 SMW 等式的形式本身就很复杂。\n将逆的形式带入高斯分布的二次型形式中，有：\n$$ \\begin{aligned} \u0026\\begin{bmatrix} x - \\mu_x\\\\ y - \\mu_y \\end{bmatrix}^T \\begin{bmatrix} \\Sigma_{xx} \u0026 \\Sigma_{xy}\\\\ \\Sigma_{yx} \u0026 \\Sigma_{yy} \\end{bmatrix}^{-1} \\begin{bmatrix} x - \\mu_x\\\\ y - \\mu_y \\end{bmatrix}\\\\ =\u0026\\begin{bmatrix} x - \\mu_x\\\\ y - \\mu_y \\end{bmatrix}^T \\begin{bmatrix} I\u0026 0\\\\ -\\Sigma_{yy}^{-1}\\Sigma_{yx} \u0026 I \\end{bmatrix}\\begin{bmatrix} \\Sigma_{xx} -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx}^{-1} \u0026 0\\\\ 0 \u0026 \\Sigma_{yy}^{-1} \\end{bmatrix} \\begin{bmatrix} I\u0026 -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\\\ 0 \u0026 I \\end{bmatrix} \\begin{bmatrix} x - \\mu_x\\\\ y - \\mu_y \\end{bmatrix}\\\\ =\u0026 (x-\\mu_x -\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-\\mu_y))^T(\\Sigma_{xx} -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx})^{-1}(x-\\mu_x -\\Sigma_{xy}\\Sigma_{yy}^{-1}(y-\\mu_y))\\\\ \u0026+ (y-\\mu_y)^T\\Sigma_{yy}^{-1}(y-\\mu_y) \\end{aligned} $$我们将这个形式带回高斯函数中，则有：\n$$ \\begin{aligned} p(x, y) \u0026= p(x \\mid y)p(y)\\\\ p(x \\mid y) \u0026= \\mathcal G(x \\mid \\mu_x + \\Sigma_{xy}\\Sigma_{yy}^{-1}(y-\\mu_y), \\Sigma_{xx} -\\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{yx})\\\\ p(y) \u0026= \\mathcal G(y \\mid \\mu_y, \\Sigma_{yy}) \\end{aligned} $$不难发现，观测 $y$ 对 $\\mu_x$ 进行了修正，同时协方差缩小了一些。那么问题就解决了，观测的后验概率可以写为：\n$$ \\begin{aligned} p(x_k \\mid y_{1:k}, v_{1:k}) = \\mathcal G\\left(\\begin{array}{l}x_k \\mid \\check P_kC_k^T(C_k\\check P_kC_k^T + R_k)^{-1}(y_k - C_k\\check x_k) + \\check x_k,\\\\ \\check P_k - \\check P_kC_k^T(C_k\\check P_kC_k^T + R_k)^{-1}C_k\\check P_k \\end{array} \\right) \\end{aligned} $$我们令：\n$$ K_k = P_kC_k^T(C_k\\check P_kC_k^T + R_k)^{-1} $$即为著名的卡尔曼增益，可以总结出经典的 kalman filter 的步骤：\n$$ \\begin{array}{ll} \\text{预测：} \u0026 \\check x_k = A_{k-1}\\hat x_{k-1} + v_k\\\\ \\text{预测协方差：} \u0026 \\check P_k = A_{k-1}P_{k-1}A_{k-1}^T + Q_k\\\\ \\text{卡尔曼增益：} \u0026 K_k = P_kC_k^T(C_k\\check P_kC_k^T + R_k)^{-1}\\\\ \\text{更新协方差：} \u0026 P_k = (1-K_kC_k)\\check P_k\\\\ \\text{更新：} \u0026 \\hat x_k = \\check x_k + K_k(y_k - C_k\\check x_k)\\\\ \\end{array} $$3. Extended Kalman Filter（EKF） 3.1. 高斯分布的非线性变换（线性化方法） 我们希望对一个分布做一个非线性变换，事实上就是解决这样一个问题：\n$$ \\begin{array}{c} p(y) = \\int p(y \\mid x)p(x)dx\\\\ p(y \\mid x) = \\mathcal G(y \\mid f(x), R)\\\\ p(y) = \\mathcal G(x \\mid \\mu_x, \\Sigma_{xx}) \\end{array} $$其中 $y=g(x)$ 是一个非线性函数，受到协方差为 $R$ 的高斯分布的影响。对于一个非线性变换 $y=g(x)$，可以做泰勒展开：\n$$ \\begin{array}{c} g(x) = g(\\mu_x) + G(x-\\mu_x) \\\\ G = \\left.\\frac{\\partial\\boldsymbol{g}(\\boldsymbol{x})}{\\partial\\boldsymbol{x}}\\right|_{\\boldsymbol{x}=\\boldsymbol{\\mu}_x} \\end{array} $$回到上式子，有：\n$$ \\begin{aligned} p(y) =\u0026 \\eta\\int \\mathcal G(y \\mid g(\\mu_x) + G(x-\\mu_x), R) \\mathcal G(x \\mid \\mu_x, \\Sigma_{xx})dx\\\\ =\u0026 \\eta \\int \\exp\\left(-\\frac{1}{2}(y-g(\\mu_x) - G(x-\\mu_x))^T R^{-1}(y-g(\\mu_x) - G(x-\\mu_x)) -\\frac{1}{2}(x-\\mu_x)^T \\Sigma_{xx}^{-1}(x-\\mu_x)\\right)dx\\\\ =\u0026 \\eta \\exp\\left( -\\frac{1}{2}(y-\\mu_y)^T R^{-1}(y-\\mu_y)\\right)\\\\ \u0026\\int \\exp\\left( -\\frac{1}{2}(x-\\mu_x)^T G^T R^{-1}G(x-\\mu_x) + (y-\\mu_y) R^{-1}G(x-\\mu_x) -\\frac{1}{2}(x-\\mu_x)^T \\Sigma_{xx}^{-1}(x-\\mu_x) \\right)dx\\\\ =\u0026 \\eta \\exp\\left( -\\frac{1}{2}(y-\\mu_y)^T R^{-1}(y-\\mu_y)\\right)\\\\ \u0026 \\int \\exp\\left( -\\frac{1}{2}(x-\\mu_x)^T \\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)(x-\\mu_x) + (y-\\mu_y)^T R^{-1}G(x-\\mu_x) \\right)dx\\\\ \\end{aligned} $$其中 $\\eta$ 是归一化常数，需要将指数项中的两个二次型化简为两个二次型相乘的形式。在此之前需要先推导一个简单的配方方法：\n$$ \\begin{aligned} \u0026(x-y)^TA(x-y) = x^TAx - 2x^TAy + y^TAy\\\\ \\Rightarrow\u0026 x^T A x - 2y^TAx = (x-y)^TA(x-y) - 2y^TAy\\\\ \\end{aligned} $$不妨假设存在一个矩阵 $F$，有：\n$$ R^{-1}G = F^T\\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right) $$那么有：\n$$ \\begin{aligned} \u0026-\\frac{1}{2}(x-\\mu_x)^T \\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)(x-\\mu_x) + (y-\\mu_y) R^{-1}G(x-\\mu_x)\\\\ =\u0026 -\\frac{1}{2}(x-\\mu_x)^T \\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)(x-\\mu_x) + (F(y-\\mu_y))^T \\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)(x-\\mu_x)\\\\ =\u0026 -\\frac{1}{2} (x-\\mu_x - F(y-\\mu_y))^T\\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)(x-\\mu_x - F(y-\\mu_y))\\\\ \u0026 + \\frac{1}{2}(y-\\mu_y)^TF^T\\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)F(y-\\mu_y) \\end{aligned} $$回到高斯函数中，第二项与积分无关，可以移到积分外。第一项可以看作一个与形如\n$$ \\mathcal G(x \\mid \\mu_x + F(y-\\mu_y), G^T R^{-1}G + \\Sigma_{xx}^{-1}) $$的高斯分布。那么根据高斯分布的归一化性质可以知道积分形式一定为一个常数，可以收入归一化系数中。我们再整理积分形式外。我们可以只考虑指数项的协方差部分：\n$$ \\begin{aligned} \u0026R^{-1} + F^T\\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)\\\\ =\u0026 R^{-1} + R^{-1}G\\left(G^T R^{-1}G + \\Sigma_{xx}^{-1}\\right)^{-1}G^TR^{-1}\\\\ =\u0026 (R + G\\Sigma_{xx}G^T)^{-1} \\end{aligned} $$上面的推导的最后一步使用了 SMW 等式。具体形式为：\n$$ (A + BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1} $$那么新的高斯分布为：\n$$ p(y) = \\mathcal G(y \\mid g(\\mu_x), R + G\\Sigma_{xx}G^T) $$3.2. Extended Kalman Filter 对于一个非线性系统，有：\n$$ \\begin{array}{c} \\text{预测：} \u0026 \\check x_k = f(x_{k-1}, v_k, \\omega_k)\\\\ \\text{观测：} \u0026 y_k = h(x_k, n_k) \\end{array} $$我们分别对上面的形式做泰勒展开：\n对于预测模型： $$ \\begin{array}{c} f(x_{k-1}, v_k, \\omega_k) = \\check x_k + F_{k-1}(x_{k-1} - \\hat x_{k-1}) + \\omega_k'\\\\ \\check x_k = f(\\hat x_{k-1}, v_k, 0)\\\\ F_{k-1} = \\frac{\\partial f(x_{k-1}, v_k, \\omega_k)}{\\partial x}|_{\\hat x_{k-1}, v_k, 0}\\\\ \\omega_k' = \\frac{\\partial f(x_{k-1}, v_k, \\omega_k)}{\\partial \\omega}|_{\\hat x_{k-1}, v_k, 0}\\omega_k \\end{array} $$ 对于观测模型： $$ \\begin{array}{c} h(x_k, n_k) = \\check y_k + H_k(x_k - \\check x_k) + n_k'\\\\ \\check y_k = h(\\check x_k, 0)\\\\ H_k = \\frac{\\partial h(x_k, n_k)}{\\partial x}|_{\\check x_k, 0}\\\\ n_k' = \\frac{\\partial h(x_k, n_k)}{\\partial n}|_{\\check x_k, 0}n_k \\end{array} $$给定过去状态和观测，有状态的统计学特征：\n$$ \\begin{array}{c} p(x_k \\mid x_{k-1}, v_k) = \\mathcal G(\\check x_k + F_{k-1}(x_{k-1} - \\hat x_{k-1}), Q_k')\\\\ Q_k' = \\left(\\frac{\\partial f(x_{k-1}, v_k, \\omega_k)}{\\partial \\omega}|_{\\hat x_{k-1}, v_k, 0}\\right)Q_k\\left(\\frac{\\partial f(x_{k-1}, v_k, \\omega_k)}{\\partial \\omega}|_{\\hat x_{k-1}, v_k, 0}\\right)^T \\end{array} $$类似的：\n$$ \\begin{array}{c} p(y_k \\mid x_k) = \\mathcal G(\\check y_k + H_k(x_k - \\check x_k), H_kP_kH_k^T + R_k')\\\\ R_k' = \\left(\\frac{\\partial h(x_k, n_k)}{\\partial n}|_{\\check x_k, 0}\\right)R_k\\left(\\frac{\\partial h(x_k, n_k)}{\\partial n}|_{\\check x_k, 0}\\right)^T \\end{array} $$此时我们可以直接按照线性 kalman filter 的方法来更新状态的统计学特征：\n$$ \\begin{array}{ll} \\text{预测：} \u0026 \\check x_k = f(\\hat x_{k-1}, v_k, 0)\\\\ \\text{预测协方差：} \u0026 \\check P_k = F_{k-1}\\hat P_{k-1}F_{k-1}^T + Q_k'\\\\ \\text{卡尔曼增益：} \u0026 K_k = \\check P_kG_k^T(G_k\\check P_kG_k^T + R_k')^{-1}\\\\ \\text{更新协方差：} \u0026 \\hat P_k = (1-K_kG_k)\\check P_k\\\\ \\text{更新：} \u0026 \\hat x_k = \\check x_k + K_k(y_k - g(\\check x_k, 0))\\\\ \\end{array} $$4. Error State Kalman Filter（ESKF） 不妨假设有一个无噪声的状态转移模型：\n$$ x_{k+1} = f(x_k, v_k) $$另有我们所使用的估计模型：\n$$ \\check x_{k+1} = f(\\check x_k, v_k) + \\omega $$可以定义一个误差形式：\n$$ \\delta x_{k+1} = f(\\check x_k, v_k) + \\omega - f(x_k, v_k) $$不妨对 $f(x_k, v_k)$ 在 $\\check x_{k}$ 处展开，有：\n$$ \\begin{array}{c} f(x_k, v_k) = f(\\check x_k, v_k) + F_k(x_k - \\check x_k)\\\\ \\text{where: }F_k = \\frac{\\partial f(x, v)}{\\partial x}|_{\\check x_k, v_k} \\end{array} $$那么有：\n$$ \\delta x_{k+1} = F_k\\delta x_k + \\omega $$类似的，对于观测模型可以写出：\n$$ \\begin{array}{c} \\delta y_k = H_k\\delta x_k + n_k\\\\ \\text{where: }H_k = \\frac{\\partial h(x, n)}{\\partial x}|_{\\check x_k, 0} \\end{array} $$应为我们认为初始状态 $x_0$ 已知。那么显然的，初始状态误差 $\\delta x_0 = 0$。\n$$ \\begin{array}{ll} \\text{预测：}\u0026 \\delta \\check x_{k+1\\mid k} = F_k\\delta \\hat x_{k\\mid k}\\\\ \u0026 P_{k+1| k} = F_kP_{k\\mid k}F_k^T + Q_k\\\\ \\\\ \\text{更新:}\u0026 K_k = P_{k+1| k}H_k^T(H_kP_{k+1| k}H_k^T + R_k)^{-1}\\\\ \u0026 \\delta x_{k+1\\mid k+1} = \\delta \\check x_{k+1\\mid k} + K_k(y_k - h(\\check x_{k+1\\mid k}, 0))\\\\ \u0026 P_{k+1| k+1} = (I - K_kH_k)P_{k+1| k} \\end{array} $$回到一开始状态误差形式的定义，我们可以通过状态转移模型得到：\n$$ \\check x_{k+1} = f(\\check x_k, v_k) + \\omega $$那么：\n$$ \\hat x_{k+1\\mid k+1} = \\check x_{k+1\\mid k} + \\delta x_{k+1\\mid k+1} $$","permalink":"https://wangjv0812.cn/2024/11/kalman_filter/","summary":"\u003cp\u003eps: 为了更快的写出来这个文档，我不会很注意公式的粗细体，请见谅。\u003c/p\u003e\n\u003ch2 id=\"1-最大后验估计\"\u003e1. 最大后验估计\u003c/h2\u003e\n\u003ch3 id=\"11-状态估计问题描述\"\u003e1.1. 状态估计问题描述\u003c/h3\u003e\n\u003cp\u003e我们假设有一个线性系统，其噪声可以用高斯函数来描述。这个线性系统可以如下描述：\u003c/p\u003e\n$$\n\\begin{array}{l}\n    x_k = A_{k-1}x_{k-1} + v_k + \\omega_k\\\\\n    y_k = Cx_k + n_k\n\\end{array}\n$$\u003cp\u003e其中，有：\u003c/p\u003e\n$$\n\\begin{array}{ll}\n    \\text{初始噪声} \u0026 x_0 \\sim \\mathcal G (x \\mid 0, P_0) \\\\\n    \\text{过程噪声} \u0026 x_k \\sim \\mathcal G (x \\mid 0, Q_k) \\\\\n    \\text{观测噪声} \u0026 \\omega_k \\sim \\mathcal G (x \\mid 0, R_k)\n\\end{array}\n$$\u003cp\u003e我们认为除了系统的输入 $v_k$ 之外，其余所有变量皆为随机变量。此外我们称 $A_k$ 为状态转移矩阵，$C_k$ 为观测矩阵。对于这个系统而言，系统的初始状态 $x_0$、系统输入 $v_k$ 和 系统输出是已知的。状态估计的目标就是通过这些已知的参数，估计出系统的状态 $x_k$。\u003c/p\u003e\n\u003ch3 id=\"12-最大后验估计\"\u003e1.2. 最大后验估计\u003c/h3\u003e\n\u003cp\u003e最大后验估计需要完成如下一个优化问题：\u003c/p\u003e\n$$\n\\hat x = \\arg \\max_{x} p(x \\mid y, v)\n$$\u003cp\u003e对于上面这个问题，通过贝叶斯定理，可以变形得：\u003c/p\u003e","title":"Kalman_filter"},{"content":"1. 图像平面对齐 思考我们有一系列位姿$x$不准确的图像。我们希望找到一个图像的扭曲变换（warp transform）$\\mathcal W(x,p)$ 我们知道，两个平面之间的投影变换可以通过homography变换来描述。变换 $\\mathcal W(x,p)$ 受到参数 $p$控制。我们希望能通过不断修正参数 $p = p + \\Delta p$来修正图像的位姿，实现最终的图像对齐。假设我们有两张照片 $\\mathcal I_1(x), \\mathcal I_2(x)$这个对齐过程可以通过下面这样一个最优指标来描述：\n$$ \\min \\sum_x \\lvert \\mathcal I_1(\\mathcal W(x,p)) - \\mathcal I_2(x) \\rvert^2 $$我们讲对 $p$ 的修正 $\\mathcal W(x,p)$ 带入$\\mathcal I_1(x)$中，并做泰勒展开：\n$$ \\mathcal I_1(W(x,p+\\Delta p)) = \\mathcal I_1(\\mathcal W(x,p)) + J(x,p) \\Delta p $$带入误差形式中，有：\n$$ E(\\Delta p) = \\lvert \\sum_x(\\mathcal I_1(\\mathcal W(x,p)) + J(x,p) \\Delta p - \\mathcal I_2(x))\\rvert^2 $$使用梯度下降，有：\n$$ \\frac{\\partial E(\\Delta p)}{\\partial \\Delta p} =2\\sum_x J^T(x,p) [\\mathcal I_1(\\mathcal W(x,p)) + J(x,p) \\Delta p - \\mathcal I_2(x)] = 0 $$重新整理，有：\n$$ \\sum_x J^T(x,p)J(x,p)\\Delta p = -\\sum_x J^T(x,p)\\left[\\mathcal I_1(\\mathcal W(x,p)) - \\mathcal I_2(x)\\right] $$我们令 $A(x,p) = J^T(x,p)J(x,p)$，有：\n$$ \\Delta p = -A(x,p)^{-1}\\sum_x J^T(x,p)\\left[\\mathcal I_1(\\mathcal W(x,p)) - \\mathcal I_2(x)\\right] $$那么很显然的，只要我们能计算出对 warp transform 的参数的修正的导数 $J^T(x,p)$，就可以求出对位姿的修正。我们用链式法则，有：\n$$ \\mathbf{J(x;p)}=\\frac{\\partial\\mathcal{I}_1(\\mathcal{W}(\\mathbf{x};\\mathbf{p}))}{\\partial\\mathcal{W}(\\mathbf{x};\\mathbf{p})}\\frac{\\partial\\mathcal{W}(\\mathbf{x};\\mathbf{p})}{\\partial\\mathbf{p}} $$我们主要需要求取 $\\frac{\\partial\\mathcal{I}_1(\\mathcal{W}(\\mathbf{x};\\mathbf{p}))}{\\partial\\mathcal{W}(\\mathbf{x};\\mathbf{p})}$。那么后面的工作就很显然来，要想办法求出由 Nerf 渲染出的图像对位置的梯度。\n2. Nerf 的表达和优化 我们假设nerf训练出的 mlp 可以表达为一个函数 $\\mathbf{y}=[\\mathbf{c};\\sigma]^\\top=f(\\mathbf{x};\\boldsymbol{\\Theta})$，其中$\\boldsymbol{\\Theta}$ 是待训练的参数。我们简单的将一个像素点上计算出的参数（该像素点对应的射线上的体渲染参数）记作: $\\{\\mathbf{y}_1,\\ldots,\\mathbf{y}_N\\}$。体渲染方程我们记作一个函数:\n$$ \\hat{\\mathcal{I}}(\\mathbf{u})=g\\left(\\mathbf{y}_1,\\ldots,\\mathbf{y}_N\\right) $$那么整个形式我们可以写出：\n$$ \\hat{\\mathcal{I}}(\\mathbf{u};\\mathbf{p})=g\\Big(f(\\mathcal{W}(z_1\\bar{\\mathbf{u}};\\mathbf{p});\\mathbf{\\Theta}),\\ldots,f(\\mathcal{W}(z_N\\bar{\\mathbf{u}};\\mathbf{p});\\mathbf{\\Theta})\\Big) $$梯度可以写出：\n$$ \\mathbf{J}(\\mathbf{u};\\mathbf{p})=\\sum_{i=1}^N\\frac{\\partial g(\\mathbf{y}_1,\\ldots,\\mathbf{y}_N)}{\\partial\\mathbf{y}_i}\\frac{\\partial\\mathbf{y}_i(\\mathbf{p})}{\\partial\\mathbf{x}_i(\\mathbf{p})}\\frac{\\partial\\mathcal{W}(z_i\\bar{\\mathbf{u}};\\mathbf{p})}{\\partial\\mathbf{p}} $$我们总结一下：\n$\\frac{\\partial g}{\\partial y}$ 是体渲染过程的梯度 $\\frac{\\partial y}{\\partial x}$ 是nerf的mlp神经网络的梯度 $\\frac{\\partial W}{\\partial p}$ 是wrap变换的梯度 ","permalink":"https://wangjv0812.cn/2024/10/barf-bundle-adjusting-neural-radiance-field/","summary":"\u003ch2 id=\"1-图像平面对齐\"\u003e1. 图像平面对齐\u003c/h2\u003e\n\u003cp\u003e思考我们有一系列位姿$x$不准确的图像。我们希望找到一个图像的扭曲变换（warp transform）$\\mathcal W(x,p)$ 我们知道，两个平面之间的投影变换可以通过homography变换来描述。变换 $\\mathcal W(x,p)$ 受到参数 $p$控制。我们希望能通过不断修正参数 $p = p + \\Delta p$来修正图像的位姿，实现最终的图像对齐。假设我们有两张照片 $\\mathcal I_1(x), \\mathcal I_2(x)$这个对齐过程可以通过下面这样一个最优指标来描述：\u003c/p\u003e\n$$\n\\min \\sum_x \\lvert\n    \\mathcal I_1(\\mathcal W(x,p)) -\n    \\mathcal I_2(x)\n\\rvert^2\n$$\u003cp\u003e我们讲对 $p$ 的修正 $\\mathcal W(x,p)$ 带入$\\mathcal I_1(x)$中，并做泰勒展开：\u003c/p\u003e\n$$\n\\mathcal I_1(W(x,p+\\Delta p)) = \\mathcal I_1(\\mathcal W(x,p)) + J(x,p) \\Delta p\n$$\u003cp\u003e带入误差形式中，有：\u003c/p\u003e\n$$\nE(\\Delta p) = \\lvert \\sum_x(\\mathcal I_1(\\mathcal W(x,p)) + J(x,p) \\Delta p - \\mathcal I_2(x))\\rvert^2\n$$\u003cp\u003e使用梯度下降，有：\u003c/p\u003e\n$$\n\\frac{\\partial E(\\Delta p)}{\\partial \\Delta p} =2\\sum_x J^T(x,p) [\\mathcal I_1(\\mathcal W(x,p)) + J(x,p) \\Delta p - \\mathcal I_2(x)] = 0\n$$\u003cp\u003e重新整理，有：\u003c/p\u003e","title":"BARF: Bundle Adjusting Neural Radiance Field"},{"content":"1. Abstract \u0026amp; Introduction 3D Gaussian Splatting 面临着一个几乎看起来无法规避的问题，就是我们需要给每个高斯函数分配一定的存储空间，并在训练时对其优化；并且在训练和渲染时需要同时将所有的高斯函数加载到设备的现存中，这导致训练和渲染在计算上是十分昂贵的。这导致我们总是要在渲染、重建质量和速度之间作出权衡，甚至很多时候是没办法训练的。这制约了 Splatting 在大场景的工作（例如城市级）上的应用。\n那么一个很显然的想法，就是在较远时提供一个较低的分辨率，实现一个分层级的渲染和训练，并且只加载视角可见的部分。那么需要的方法有两点：\n引入结构层次（Hierarchy），使用一种高效细节级别解决方案（Level of Detial）。 引入分置策略（divide-and-conquer），让我们可以在独立的训练和渲染每一个小块。 同时，通过不同层级的结构（Guassian Function）可以用来优化中间层的高斯函数。这篇文章所提出的策略可以实时的渲染非常大的场景，覆盖长达几公里的轨迹，持续长达一小时。\n2. 概述和背景 2.1. 背景 3DGS 提供了一种基于体积基元的空间场景表达方法，每个体积基元含有如下特征：\n位置（或者说均值$\\mu$） 协方差矩阵$\\Sigma$ 透明度（$o$） 球谐系数（$SH$）用于表达与视角相关的颜色，或者直接使用颜色 三维基元可以投影到二维屏幕空间上，并且通过 $\\alpha\\text{-blander}$ 来实现光栅化。 $\\alpha\\text{-blander}$ 的权重为： $$ \\begin{aligned} \\alpha \u0026= \\text{oG}\\\\ G(x,y) \u0026= \\exp \\left\\{ -\\frac 12 ([x,y]^T-\\mu')^T\\Sigma'^{-1}([x,y]^T-\\mu') \\right\\} \\end{aligned} $$ 其中 $\\mu'$ 是三维空间基元投影到二维相机平面上基元的均值，$\\Sigma'$ 投影的二维基元的协方差。\n3. 3DGaussian 的结构化 (hierarchy) 的细节层次 (LOD) 在处理大型场景以允许有效渲染大量内容时，细节级别 (LOD) 解决方案至关重要；因此，我们的目标是创建一个层次结构，表示原始 3DGS 优化生成的原语。遵循图形中的传统LOD方法，我们需要\n找到候选3DGS基元，并定义如何将它们合并到中间节点 提供一种有效的方法来确定层次结构中的切割，从而在质量和速度之间提供良好的折衷 层次结构级别之间的平滑过渡策略 3.1. 生成不同分辨率的高斯球 我们为每个块创建一个具有内部节点和叶节点的基于树的层次结构。每个节点都与一个 3D 高斯相关联，该高斯要么是来自原始优化的叶节点，要么是合并的内部节点。我们对中间节点的要求是它们应该：\n保持与叶节点相同的快速光栅化例程 尽可能准确地表示子节点的外观。因此，我们需要定义具有 3DGS 原语所有属性的 3D 高斯的中间节点。例如保持它原本所有的特征：均值$\\mu$、协方差$\\Sigma$、透明度 $o$ 等等。 对于均值和协方差，有很多文献详尽的描述了这个混合过程。可以通过如下公式混合 $N$ 个在第 $l$ 级的均值为 $\\mu_i^l$，协方差为 $\\Sigma_i^l$ 高斯函数。我们可以通过评估这 $N$ 个高斯函数和待估计的高斯函数之间的 3D Kullback-Leibler divergence。3DKL 散度描述了两个高斯函数之间的相关性。那么显然的，假设$f = \\sum_{i=1}^{N}\\mathcal \\alpha_i N(\\mu_i, \\Sigma_i)$，g为我们所需要新的高斯函数，应该有： $$ d(f,g)=\\sum_{i=1}^k\\alpha_i\\min_{j=1}^mKL(f_i||g_j) $$ 即原始的混合高斯函数 $f = \\sum_{i=1}^{N}\\mathcal \\alpha_i N(\\mu_i, \\Sigma_i)$ 变为新估计的高斯函数 $g$ 的过程中所损失的信息最少，即上式有最小值。我们忽略推导过程，其结果应为： $$ \\begin{gather} \\mu^{(l+1)}=\\sum_i^Nw_i\\mu_i^{(l)}\\\\ \\Sigma^{(l+1)}=\\sum_i^Nw_i(\\Sigma_i^{(l)}+(\\mu_i^{(l)}-\\mu^{(l+1)})(\\mu_i^{(l)}-\\mu^{(l+1)})^T) \\end{gather} $$ 其中 $\\omega_i$ 为一个归一化系数，其定义为： $$ w_i = \\frac{w_i'}{\\sum_1^N w_i'} $$ 其中具体我们定义了非标准化的合并权值 $w_i'$，它们与每个子高斯对创建的父高斯的贡献成正比。事实上这个权重与传统的高斯混合模型中“属于该高斯函数的概率”这一参数有着相似的含义。我们不妨将三维高斯函数投影到二维相机空间上称为二维高斯（即带 $'$ 的参数所对应的）。考虑其中一个高斯函数$g_i$，其的参数为$c_i, o_i$，它对于在$(x,y)$的颜色 $C(x,y)$的贡献有： $$ C(x,y) = o_ic_iG(x,y) $$ 我们能投影到该点的全部高斯函数所产生的结果，应有： $$ \\begin{aligned}C_i\u0026=\\mathrm{~o_i~c_i}\\int_X\\int_YG(x,y)\\\\\u0026=\\mathrm{~o_i~c_i}\\sqrt{(2\\pi)^2|\\Sigma^{\\prime}|}\\end{aligned} $$ 考虑高斯函数的性质，我们可以作出如下简化假设：\n高斯函数几乎是各向同性的， 高斯函数有较少的重叠、透视失真。 在这个简化条件下，我们不妨先考虑两个高斯函数混合的情形，我们需要保证生成的高斯函数 $g_p$ 等于两个子函数$g_2,g_2$的和。这是一个很好的性质，相当于我们将原本按照体积渲染累计的高斯函数简化为一般的高斯高斯混合模型。那么忽略常数项，有： $$ w_i^{\\prime}=\\mathrm{~o_i}\\sqrt{|\\Sigma_i^{\\prime}|} $$ 在实践中，由于高斯二维协方差的行\t列式的平方根与相应的三维椭球的(投影)表面成正比，我们计算每个椭球的表面 $\\sqrt{\\det(\\Sigma_i)}$ 而不是 $\\sqrt{\\det(\\Sigma_i')}$。\n这样我们可以用这个混合参数来混合球谐系数： $$ SH^{(l+1)}=SH_1^{(l)}w_1+SH_2^{(l)}w_2. $$ 我们也可以使用权重来合并不透明度。然而，我们的合并策略改变了中间节点的不透明度属性的含义。\n可以看到，我们在将两个高斯函数混合为一个的过程中，可能会出现混合结果的值超过了1，然而我们需要将其限制为1，大于1的不透明度意味着不透明，当然这和不透明度为1有着相同的效果，但是这样的操作显然改变了不透明度的含义。事实上产生这样的结果主要是由两个高斯函数之间不是由严谨的体积渲染实现的，而是通过一定的简化假设变为高斯混合模型，这样的简化假设导致了不透明度含义的改变。在一些特别的场景中，高斯函数各向同性的假设无法成立，这在后面会有描述。\n我们现在有了将多个高斯函数混合成一个高斯函数的办法。在给定一组原始的 3DGS 场景上，我们首先可以在其上构建一个自顶向下的轴对齐的边界框 (axis-aligned bounding box) 边界体层次结构 (Bounding Volume Hierarchy) 。我们从一个包含所有高斯原语的AABB开始，使用3倍的存储大小来捕获它们的范围。初始AABB是BVH的根节点。为了获得子节点，我们在当前节点上递归地执行二进制中位数分割。首先，我们将一组中所有高斯函数的均值投影到它们的边界框的最长轴上。然后，我们根据其投影均值相对于所有投影中位数的位置分配每个原语来划分组。生成的BVH树确保每个内部节点的子节点在空间上是紧凑的。然后，我们从它们各自的子节点计算中间节点高斯，从叶子节点开始，递归地向上合并树。\n3.2. 层次切割选择和水平切换 对于一个给定的 3DGS 的分辨率层次树结构和一个视角$V$，我们需要找到一个对分辨率层次树的裁切，需要保证在保证渲染质量的同时最大程度的提高渲染的效率。我们首先定义我们首先将给定层次节点 $n$ 的粒度$\\epsilon(n)$，定义为给定视图在屏幕上的投影大小。\n具体来说，我们在节点中包含的叶子高斯上使用边界框，然后取边界框的最大维度并在这个纬度上计算投影大小。我们通过比较边界框的投影与一个该节点预设的粒度阈值（例如一个像素）来判断是否进行切割。例如对于 (b) 中的紫色节点，其显然包含着至少两个子节点——即蓝色和红色节点，这两个子节点各包含一个高斯函数。那么紫色节点的边界即位这两个子节点的高斯函数所包含的最大范围。基于生成的层级节点和他的边界，AABA方法可以保证它的夫节点一定不会比子节点小。这保证我们可以在线性时间可以完成这个计算。在大规模并行计算中，每个节点在恒定时间内完成：如果节点𝑛的边界满足粒度条件，但其子节点的边界不满足，则选择节点𝑛作为给定设置并将其包含在cut中（上图(b)中的绿色曲线）。请注意，该逻辑要么选择所有子节点，要么选择父节点。\n任何分层呈现解决方案的一个关键元素是允许分层级别之间平滑转换的能力。我们通过插值单个高斯属性来实现平滑过渡。当一个节点不再是当前粒度下最合适的分割时，它会被它的子节点代替。但是当该节点没有子节点时，可以通过在父节点和子节点间插值产生新节点。我们通过选择恰好超过粒度阈值 $\\epsilon$ 的节点作为裁切的结果。差值的权重通过该裁切中的每个节点的粒度 $\\epsilon(n)$ 以及粒度的变化趋势 $\\epsilon(p)$，其中 $p$ 是 $n$ 的父节点。插值的权重可以写作： $$ t_n=\\frac{\\tau_\\epsilon-\\epsilon(n)}{\\epsilon(p)-\\epsilon(n)}. $$ 位置（均值）、协方差、颜色可以通过这个插值系数线性的计算出来。对于协方差，即便对旋转使用线性插值而不是更昂贵的“球面插值”，分别插值尺度和旋转也会产生比直接对协方差矩阵插值更好的效果。但是有时高斯函数的旋转和缩放上并不相同，但是有着相同的协方差矩阵。\n如 (a) 所示，左右两个高斯函数有着相同的协方差；但是其x-y坐标有90度的旋转，其旋转和缩放显然是不同的。因此，在直接插值它们的属性时，意料之外的旋转会产生视觉上的干扰。例如上述两个协方差插值时，我们希望得到的高斯函数的方向应与之是相同的，如果直接插值可能会产生一个与之成45度的并不合理的协方差矩阵。\n为了避免这种情况，在层次结构生成过程中，我们还执行方向匹配：从根节点开始，我们递归地重新定义每个子高斯的方向轴，让他们之间的旋转有最小值。具体来说我们旋转每一个节点的高斯函数，并计算其与父节点的角度差，最终选取角度差最小的作为其旋转轴。\n对于不透明度的处理也需要一些技巧。在从父节点到子节点的转化过程中，子节点分享了父节点的全部参数，这些属性逐渐地通过插值的方式分享到每个子节点中去。那么显然的，我们需要让生成的两个子节点按照体渲染的方式混合后的结果应和父节点相同。思考最简单的两个子节点和一个孤立的父节点的问题，应有父节点对颜色混合的贡献$\\alpha_pc_p$，我们需要找到对子节点的权重$\\alpha'$： $$ \\alpha_pc_p=\\alpha^{\\prime}c_p+(1-\\alpha^{\\prime})\\alpha^{\\prime}c_p $$ 因此子节点的颜色就会父节点的颜色，应有： $$ \\alpha^{\\prime}=1-\\sqrt{1-\\alpha_p} $$使用权重$\\alpha'$，我们可以像其他参数一样使用线性插值从其父节点计算子节点的混合系数 $\\alpha$ $$ \\alpha(t)=t\\alpha_i+(1-t)\\alpha^{\\prime} $$ 通过这种插值方案，我们实现了层次结构的平滑过渡。下图中说明了两个不同目标粒度的层次结构的结果渲染：\n4. 优化和结构层次压缩 3DGS的层次结构由集合基元的集合构成，最终我们需要更加明确清晰考虑场景的外观。因为层次结构的每个中间节点本身就是一个三维高斯基元，它就可以通过优化的方式提升其视觉质量。为了做到这一点，我们需要通过中间节点来传播梯度，引入一个其中间节点可以被优化的层次结构。下面我们将介绍这是如何实现的，还引入一个额外的步骤来压缩层次结果。\n图形学中传统的层次 LOD 算法往往是为了在远处观察时实现一个简化版本的场景。我们前面介绍的粒度阈值通过计算在屏幕上投影的粒度了实现了这一效果。\n4.1. 层次结构的优化 我们自顶向下的层次结构构造产生了一个运行良好的数据结构。一旦完成了构件，中间层级的节点就可以像叶节点一样优化。因此我们可以通过层次结构中的中间节点来提高整体的视觉质量。那么问题在于如何在不同的规模间进行优化。一个优化层次结构的方案是随机选取一个输入的照片和一个降采样系数，较低的分辨率自然意味着较低的粒度，对应于结构层次中的裁剪。但是这种方法有显然的缺点：当降低分辨率时，低频细节无法保存。\n在优化过程中，我们并不降低分辨率，而是直接渲染最高分辨率的照片并随机的选择颗粒度阈值。这具有在许多不同切割中优化节点的预期效果，同时保留视觉细节。当我们拥有有限的资源时，这将非常有用，因为我们可以在保持更好的视觉质量的同时应用更积极的LOD策略。\n具体来说，我们直接加载生成的层次结构，选择一个训练视角并在一个给定的区间内选择粒度阈值 $\\tau_\\epsilon$来训练。这个目标阈值的区间可以写作： $$ [\\tau_{min}, \\tau_{max}) $$ 训练视角的选择对应来所选择的裁切，为了实现对所有结构层级进行有效的采样，我们的采样粒度阈值 $\\tau_\\epsilon$ 由一个均匀随机变量$\\xi\\in[0,1]$定义，有： $$ \\tau_\\epsilon = \\tau_{max}^\\xi \\tau_{min}^{1-\\xi} $$ 裁切和插值在训练过程中均有使用，为了同时优化子节点和父节点并使用平滑切换，我们需要将梯度合理的传播到该节点临近的两层，从而同时优化父节点和子节点。这要求我们能通过插值权重和不透明度混合的权重$\\alpha'$的表达式来传递梯度。通过优化层次结构的多个级别，我们解决了一个比3DGS更复杂的问题：优化更高的LOD级别也可能由于插值而降低叶子的质量。为了避免这种情况，我们在优化过程中不改变叶节点。\n原始的3DGS无法很好的处理频域混叠的问题，由于层次结构可以工作在不同的尺度上，正确的抗混叠是十分重要的，本文中会使用 EWA 滤波器来滤出高频混叠。\n4.2. 结构层次压缩 结构层次增加了内存上的开销。更重要的是，为了进行优化层次结构，我们希望避免父节点只比子节点大一点点的情况出现。如果这种情况出现，由于其粒度差别很小，着可能会导致在训练过程中起很少被选中进行优化。为了避免这一点，我们对生成的结构层次树进行稀疏化。\n首先我们标记所有叶节点，即所有3DGS优化产生的节点，他们在裁切过程中不应该被删除。之后我们我们在所有训练视图上找到最小目标粒度 $\\tau_{min}=3$ 中的切割构成一个集合。然后，我们在这个集合中找到最底部的节点，这再次产生一个切割。这些节点被认为是与所选粒度相关的最相近的节点，它们之间的所有节点和已经标记的节点都从树中删除。然后我们将目标的粒度提高 $2$ 倍并重复这个过程知道达到 $\\tau_{max}$，达到图像分辨率的一半。\n5. 大场景训练 我们现在可以建立三维高斯函数的有效层次结构；这些对于处理非常大的场景是必不可少的，因为从远处看到的场景部分可以在更粗的层次结构中渲染。为了训练大型场景，我们建立了用于实时渲染大数据的通用计算机图形学方法。特别是，我们通过将大型场景细分为块来引入分而治之的方法。\n对于捕获的场景，我们将块大小定义为 $50\\times50 \\rm m$，对于用车辆捕获的场景，我们将块大小定义为 $100\\times100 \\rm m$。虽然大小有限，但这些块仍然比原始 3DGS 方法能处理的能力大。此外，这种场景的捕获风格必然比大多数亮度场解决方案所期望的要稀疏得多。因此，我们调整了三维高斯函数的优化来考虑这些差异。\n我们的目标是允许并行处理单个块，在给定足够计算资源的情况下，允许在合理的时钟时间内处理大型场景。但是单独处理可能会导致重合部分的不一致，在每个单独的块中完成处理后，需要一个整合步骤来处理这些差异。\n5.1. 粗初始化和块细分 为了使所有部分的训练一致，我们需要为所有后续步骤提供一个基本框架和天空盒。为了使所有部分的训练一致，我们需要为所有后续步骤提供一个基本框架和天空盒。我们通过对整个数据集进行非常粗略的初始优化来实现这一点。具体来说，我们使用可用的 SfM 点启动整个场景的默认3DGS优化，并添加一个辅助的天空盒。这个粗略模型作为提供背景细节的最小基础，即场景中给定部分之外的部分。在极大场景的情况下，如果存储SfM点会超出RAM容量，粗略优化本身可以分成多个步骤进行，部分中间结果会被流式传输到磁盘。我们将场景划分为足够大的块，以便为常见的现实世界元素（包括汽车、建筑物和纪念碑）建立足够的上下文。对于每个块，我们选择所有在块范围内或在块范围的2倍以内且在块范围内有超过50个SfM点的摄像机。\n5.2. 块尺度的训练 我们对每个块独立进行训练；然后将结果用于为每个块创建层次结构，随后进行优化和整合步骤。在大型数据集中，典型的块与原始3DGS设置中使用的数据有显著不同。特别是，场景的范围显著大于Mip-NeRF 360等数据集的小场景。此外，捕获密度也要低得多，而且不是“以物体为中心”的。这使得优化更加困难，因为光线空间没有均匀覆盖。数据还包含曝光变化、人类和需要在优化中去除和忽略的移动物体（如汽车、自行车等）。\n我们定义了一个围绕场景范围的天空盒，即在直径为场景十倍的球体上放置100,000个3DGS基元以捕获天空的效果。我们加载场景的粗略优化结果，这将用于块之外的所有内容；这也防止了每个块在天空上生成不一致的内容。我们使用3DGS优化方法和正确的抗锯齿技术来训练块内的内容。我们对块外的粗略环境和天空盒进行小范围的临时优化，特别是仅优化不透明度和SH系数。\n原始的3DGS优化通过收集统计数据来决定是否在固定间隔内对高斯进行稠密化。具体来说，稠密化策略基于固定次数迭代中屏幕空间位置梯度的平均值。然而，在无界场景中存在两个主要问题。首先，这一策略很少阻止高斯稠密化，无论它们是否足够精细以建模局部细节。其次，具有稀疏、分散摄像机的数据集（例如城市穿越捕捉的情况）总体上稠密化的倾向更低。我们通过将稠密化策略改为考虑观察到的屏幕空间梯度的最大值而不是其平均值来解决这两个问题，这在稀疏捕捉的情况下不再可靠。我们拥有的稀疏摄像机捕捉不足以提供高质量的重建信息，例如在城市驾驶场景中的街道。我们进行单目深度预测，基于SfM点缩放和移动深度，并使用它来正则化优化。这改善了视觉质量，特别是对于道路。\n5.3. 块整合和渲染 每个块从COLMAP的每块细化处理中获取SfM点，以及其相邻块中基本框架中包含的3D高斯。与一个块及其层次结构相关的3D高斯因此有时会位于块本身之外。在整合阶段，如果与块 $i$ 相关但位于块外的原语更接近另一个块$j\\neq i$，则将其删除。整合还会创建一个具有整个场景根节点的全局层次结构。渲染是通过设置粒度阈值并找到相应的剪切点来执行的。我们每2帧更新一次剪切点，通过从CPU RAM传输节点到GPU来添加细节，并每100帧进行一次清理。\n6. 附录 6.1. 如何降低高斯混合模型的复杂度 假设我们有一个分布，其由 $k$ 个 $d$ 纬高斯函数的高斯混合模型构成，可以写作： $$ f(y)=\\sum_{i=1}^k\\alpha_iN(y;\\mu_i,\\Sigma_i)=\\sum_{i=1}^k\\alpha_if_i(y) $$ 我们希望找到另一个高斯混合模型 $g(y)$，有： $$ g(y) = \\sum_{i=1}^{m}\\beta_ig_i(y) $$ 其中$m","permalink":"https://wangjv0812.cn/2024/07/hierarchical-gaussian-splatting/","summary":"\u003ch2 id=\"1-abstract--introduction\"\u003e1. Abstract \u0026amp; Introduction\u003c/h2\u003e\n\u003cp\u003e3D Gaussian Splatting 面临着一个几乎看起来无法规避的问题，就是我们需要给每个高斯函数分配一定的存储空间，并在训练时对其优化；并且在训练和渲染时需要同时将所有的高斯函数加载到设备的现存中，这导致训练和渲染在计算上是十分昂贵的。这导致我们总是要在渲染、重建质量和速度之间作出权衡，甚至很多时候是没办法训练的。这制约了 Splatting 在大场景的工作（例如城市级）上的应用。\u003c/p\u003e\n\u003cp\u003e那么一个很显然的想法，就是在较远时提供一个较低的分辨率，实现一个分层级的渲染和训练，并且只加载视角可见的部分。那么需要的方法有两点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e引入结构层次（Hierarchy），使用一种高效细节级别解决方案（Level of Detial）。\u003c/li\u003e\n\u003cli\u003e引入分置策略（divide-and-conquer），让我们可以在独立的训练和渲染每一个小块。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e同时，通过不同层级的结构（Guassian Function）可以用来优化中间层的高斯函数。这篇文章所提出的策略可以实时的渲染非常大的场景，覆盖长达几公里的轨迹，持续长达一小时。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"db286f9b0b818bd938a3ef6ea35d1c7a_0_Figure_1_-1273433434\" loading=\"lazy\" src=\"/2024/07/hierarchical-gaussian-splatting/images/db286f9b0b818bd938a3ef6ea35d1c7a_0_Figure_1_-1273433434.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"2-概述和背景\"\u003e2. 概述和背景\u003c/h2\u003e\n\u003ch3 id=\"21-背景\"\u003e2.1. 背景\u003c/h3\u003e\n\u003cp\u003e3DGS 提供了一种基于体积基元的空间场景表达方法，每个体积基元含有如下特征：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e位置（或者说均值$\\mu$）\u003c/li\u003e\n\u003cli\u003e协方差矩阵$\\Sigma$\u003c/li\u003e\n\u003cli\u003e透明度（$o$）\u003c/li\u003e\n\u003cli\u003e球谐系数（$SH$）用于表达与视角相关的颜色，或者直接使用颜色\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e三维基元可以投影到二维屏幕空间上，并且通过 $\\alpha\\text{-blander}$ 来实现光栅化。 $\\alpha\\text{-blander}$ 的权重为：\n\u003c/p\u003e\n$$\n\\begin{aligned}\n\\alpha \u0026= \\text{oG}\\\\\nG(x,y) \u0026= \\exp \n\\left\\{\n-\\frac 12 ([x,y]^T-\\mu')^T\\Sigma'^{-1}([x,y]^T-\\mu')\n\\right\\}\n\\end{aligned}\n$$\u003cp\u003e\n其中 $\\mu'$ 是三维空间基元投影到二维相机平面上基元的均值，$\\Sigma'$ 投影的二维基元的协方差。\u003c/p\u003e\n\u003ch2 id=\"3-3dgaussian-的结构化-hierarchy-的细节层次-lod\"\u003e3. 3DGaussian 的结构化 (hierarchy) 的细节层次 (LOD)\u003c/h2\u003e\n\u003cp\u003e在处理大型场景以允许有效渲染大量内容时，细节级别 (LOD) 解决方案至关重要；因此，我们的目标是创建一个层次结构，表示原始 3DGS 优化生成的原语。遵循图形中的传统LOD方法，我们需要\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e找到候选3DGS基元，并定义如何将它们合并到中间节点\u003c/li\u003e\n\u003cli\u003e提供一种有效的方法来确定层次结构中的切割，从而在质量和速度之间提供良好的折衷\u003c/li\u003e\n\u003cli\u003e层次结构级别之间的平滑过渡策略\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"31-生成不同分辨率的高斯球\"\u003e3.1. 生成不同分辨率的高斯球\u003c/h3\u003e\n\u003cp\u003e我们为每个块创建一个具有内部节点和叶节点的基于树的层次结构。每个节点都与一个 3D 高斯相关联，该高斯要么是来自原始优化的叶节点，要么是合并的内部节点。我们对中间节点的要求是它们应该：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e保持与叶节点相同的快速光栅化例程\u003c/li\u003e\n\u003cli\u003e尽可能准确地表示子节点的外观。因此，我们需要定义具有 3DGS 原语所有属性的 3D 高斯的中间节点。例如保持它原本所有的特征：均值$\\mu$、协方差$\\Sigma$、透明度 $o$ 等等。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于均值和协方差，有很多文献详尽的描述了这个混合过程。可以通过如下公式混合 $N$ 个在第 $l$ 级的均值为 $\\mu_i^l$，协方差为 $\\Sigma_i^l$ 高斯函数。我们可以通过评估这 $N$ 个高斯函数和待估计的高斯函数之间的 3D Kullback-Leibler divergence。3DKL 散度描述了两个高斯函数之间的相关性。那么显然的，假设$f = \\sum_{i=1}^{N}\\mathcal \\alpha_i N(\\mu_i, \\Sigma_i)$，g为我们所需要新的高斯函数，应该有：\n\u003c/p\u003e","title":"Hierarchical Gaussian Splatting"},{"content":"1. 矩阵求导的常用方法 1.1. 矩阵求导的一般方法 在矩阵论的课程中，我们学习过如下几种分析相关的知识，分别是：\n向量对标量求导 向量对向量求导 向量对矩阵求导 矩阵对标量求导 矩阵对向量求导 矩阵对矩阵求导 事实上不难发现，我们只需要搞明白了矩阵对标量求导和矩阵对矩阵求导的方法，其他问题均可从这个两个原则推理开去。因此我们叙述的重点放在这两个问题上。\n1.1.1. 矩阵对标量求导 假设我们有矩阵 $\\mathbf A$ 和标量 $k$，其中矩阵 $\\mathbf A$ 的展开形式为：\n$$\r\\mathbf A= \\left[\r\\begin{matrix}\ra_{11}\u0026 a_{12}\u0026 \\cdots\u0026 \\ a_{1n} \\\\\ra_{21}\u0026 a_{22}\u0026 \\cdots\u0026 \\ a_{2n} \\\\\r\\vdots\u0026 \\vdots\u0026 \\ddots\u0026 \\vdots \\\\\ra_{n1}\u0026 a_{n2}\u0026 \\cdots\u0026 \\ a_{nn} \\\\\r\\end{matrix}\r\\right]\r$$那么，$\\frac{d \\mathbf A}{d k}$被定义为：\n$$\r\\frac{d \\mathbf A}{d k} =\r\\left[\r\\begin{matrix}\r\\frac{d a_{11}}{d k}\u0026 \\frac{d a_{12}}{d k}\u0026 \\cdots\u0026 \\ \\frac{d a_{1n}}{d k}\u0026 \\\\\r\\frac{d a_{21}}{d k}\u0026 \\frac{d a_{22}}{d k}\u0026 \\cdots\u0026 \\ \\frac{d a_{2n}}{d k}\u0026 \\\\\r\\vdots\u0026 \\vdots\u0026 \\ddots\u0026 \\vdots \\\\\r\\frac{d a_{n1}}{d k}\u0026 \\frac{d a_{n2}}{d k}\u0026 \\cdots\u0026 \\ \\frac{d a_{nn}}{d k}\u0026 \\\\\r\\end{matrix}\r\\right]\r$$于上述定义类似，如果标量对矩阵求导，即$\\frac{d k}{d \\mathbf A}$，其定义为：\n$$\r\\frac{d k}{d \\mathbf A} =\r\\left[\r\\begin{matrix}\r\\frac{d k}{d a_{11}}\u0026 \\frac{d k}{d a_{12}}\u0026 \\cdots\u0026 \\ \\frac{d k}{d a_{1n}}\u0026 \\\\\r\\frac{d k}{d a_{21}}\u0026 \\frac{d k}{d a_{22}}\u0026 \\cdots\u0026 \\ \\frac{d k}{d a_{2n}}\u0026 \\\\\r\\vdots\u0026 \\vdots\u0026 \\ddots\u0026 \\vdots \\\\\r\\frac{d k}{d a_{n1}}\u0026 \\frac{d k}{d a_{n2}}\u0026 \\cdots\u0026 \\ \\frac{d k}{d a_{nn}}\u0026 \\\\\r\\end{matrix}\r\\right]\r$$1.1.2. 矩阵对矩阵求导 矩阵对矩阵求导是更加复杂的矩阵对标量的求导，假设我们有矩阵 $\\mathbf A, \\mathbf B$，有：\n$$\r\\mathbf A= \\left[\r\\begin{matrix}\ra_{11}\u0026 a_{12}\u0026 \\cdots\u0026 \\ a_{1n} \\\\\ra_{21}\u0026 a_{22}\u0026 \\cdots\u0026 \\ a_{2n} \\\\\r\\vdots\u0026 \\vdots\u0026 \\ddots\u0026 \\vdots \\\\\ra_{n1}\u0026 a_{n2}\u0026 \\cdots\u0026 \\ a_{nn} \\\\\r\\end{matrix}\r\\right]\r$$那么有：\n$$\r\\frac{d\\mathbf A}{d\\mathbf B} =\r\\left[\r\\begin{matrix}\r\\frac{d a_{11}}{d \\mathbf B}\u0026 \\frac{d a_{12}}{d \\mathbf B}\u0026 \\cdots\u0026 \\ \\frac{d a_{1n}}{d \\mathbf B}\u0026 \\\\\r\\frac{d a_{21}}{d \\mathbf B}\u0026 \\frac{d a_{22}}{d \\mathbf B}\u0026 \\cdots\u0026 \\ \\frac{d a_{2n}}{d \\mathbf B}\u0026 \\\\\r\\vdots\u0026 \\vdots\u0026 \\ddots\u0026 \\vdots \\\\\r\\frac{d a_{n1}}{d \\mathbf B}\u0026 \\frac{d a_{n2}}{d \\mathbf B}\u0026 \\cdots\u0026 \\ \\frac{d a_{nn}}{d \\mathbf B}\u0026 \\\\\r\\end{matrix}\r\\right]\r$$之后套用矩阵与标量之间的微分运算的定义即可计算。但是不难发现，这样需要将整个矩阵展开来求导过于繁琐负责了，也破坏了原本矩阵作为一个整体所蕴含的性质。我们需要一些更加简洁的方法来实现矩阵求导。\n1.2. 矩阵求导的全微分方法 在具体的优化问题中，我们面对的往往是一个多多变量的复杂计算过程。这要求我们处理多变量，多步骤的矩阵多项式函数计算微分。假设我们一个计算链的输入是 $s_i$，输出是 $s_o$，计算链中的某一步计算是：\n$$\rC = f(A, B)\r$$1.2.1. 正向传播的计算 那么显而易见的，其全微分为：\n$$\rdC = \\frac{\\partial f}{\\partial A} dA + \\frac{\\partial f}{\\partial B} dB\r$$当输入，即 $s_i$ 上一个扰动时（记作 $\\dot{S_i}$），在 $C$ 上的扰动为：\n$$\r\\dot{C} = \\frac{\\partial f}{\\partial A} \\dot A + \\frac{\\partial f}{\\partial B} \\dot B\r$$1.2.2. 反向传播的计算 我们分析从运算链中的某一步骤向输出传到的过程：\n$$\rs_0 = \\sum C_{ij}\r$$那么，有：\n$$\rds_o = \\sum \\frac{\\partial s_0}{\\partial C_{ij}} dC_{ij} = Tr(\\frac{\\partial s_0}{\\partial C} ^T dC)\r$$带入计算我们之前给出的计算链 $C = f(A, B)$，有：\n$$\r\\begin{aligned}\rd(S_0) \u0026= Tr(C^T dC)\\\\\rdC \u0026= \\frac{\\partial f}{\\partial A} dA + \\frac{\\partial f}{\\partial B} dB\r\\end{aligned}\r$$根据迹的分配律，有：\n$$\r\\begin{aligned}\rd(S_0)\r\u0026= Tr(C^T, dC) \\\\\r\u0026= Tr(C^T, \\frac{\\partial f}{\\partial A}dA) + Tr(C^T, \\frac{\\partial f}{\\partial B} dB)\\\\\r\u0026= Tr(\\frac{\\partial f}{\\partial A}^TC, dA) + Tr(\\frac{\\partial f}{\\partial B}^T C, dB)\\\\\r\u0026= Tr(\\frac{\\partial s_o}{\\partial A}, dA) + Tr(\\frac{\\partial s_o}{\\partial B}, dB)\r\\end{aligned}\r$$这样我们可以知道：\n$$\r\\begin{aligned}\r\\frac{\\partial s_o}{\\partial A} = \\frac{\\partial f}{\\partial A}^TC \\\\\r\\frac{\\partial s_o}{\\partial B} = \\frac{\\partial f}{\\partial B}^T C\r\\end{aligned}\r$$1.2.3. Frobenius内积 为了更好的表达运算关系，我们定义一个二元运算 $\\langle \\cdot , \\cdot \\rangle$。这个二元组定义的本质在于矩阵的微分运算是对其中每个元素进行的，不能简单套用对矩阵全体定义的矩阵乘法运算。\n$$\r\\langle X, Y\\rangle = Tr(X^T Y) = Vec(X)^T Vec(Y) \\in \\mathbb R\r$$其性质有：\n$$\r\\begin{aligned}\r\\langle X,Y\\rangle \u0026 =\\langle Y,X\\rangle, \\\\\r\\langle X,Y\\rangle \u0026 =\\langle X^\\top,Y^\\top\\rangle, \\\\\r\\langle X,YZ\\rangle \u0026 =\\langle Y^{\\top}X,Z\\rangle=\\langle XZ^{\\top},Y\\rangle, \\\\\r\\langle X,Y+Z\\rangle \u0026 =\\langle X,Y\\rangle+\\langle X,Z\\rangle.\r\\end{aligned}\r$$用这个二元运算重新叙述上面的推导过程，可能会更加清晰。假设我们有一个函数 $f(X), X\\in \\mathbb R^{m\\times n}$，并且 $X = AY, where. A\\in \\mathbb R^{m\\times p}, Y\\in \\mathbb R^{p\\times n}$，\n我们可以把 $f$ 写作任意标量 $x$ 的梯度： $$\r\\frac{\\partial f}{\\partial x} = \\langle\\frac{\\partial f}{\\partial X}, \\frac{\\partial X}{\\partial x} \\rangle\r$$我们有：\n$$\r\\partial f = \\langle \\frac{\\partial f}{\\partial X}, \\partial X \\rangle\r$$并定义：$G = \\frac{\\partial f}{\\partial X}$，有：\n$$\r\\begin{aligned}\r\\frac{\\partial f}{\\partial x}\u0026 =\\langle G,\\frac{\\partial(AY)}{\\partial x}\\rangle \\\\\r\u0026=\\langle G,\\frac{\\partial A}{\\partial x}Y\\rangle+\\langle G,A\\frac{\\partial Y}{\\partial x}\\rangle \\\\\r\u0026=\\langle GY^{\\top},\\frac{\\partial A}{\\partial x}\\rangle+\\langle A^{\\top}G,\\frac{\\partial Y}{\\partial x}\\rangle.\r\\end{aligned}\r$$对比，可归纳出：\n$$\r\\frac{\\partial f}{\\partial A}=GY^\\top\\in\\mathbb{R}^{m\\times p},\\quad\\frac{\\partial f}{\\partial Y}=A^\\top G\\in\\mathbb{R}^{p\\times n}.\r$$1.3. 常见矩阵运算的求导 1.3.1. 加法 假设有 $C = A+B$，那么有：\n$$\rdC = dA + dB\r$$且有：\n$$\r\\begin{aligned}\rds_o\r\u0026= \\langle \\frac {\\partial s_o}{\\partial C} , dC\\rangle\\\\\r\u0026= \\langle \\frac {\\partial s_o}{\\partial C} , dA\\rangle + \\langle \\frac {\\partial s_o}{\\partial C} , dB\\rangle\\\\\r\u0026= \\langle \\frac {\\partial s_o}{\\partial A} , dA\\rangle + \\langle \\frac {\\partial s_o}{\\partial B} , dB\\rangle\\\\\r\\end{aligned}\r$$那么：\n$$\r\\begin{aligned}\r\\frac{\\partial s_o}{\\partial A} = \\frac{\\partial s_o}{\\partial C}\\\\\r\\frac{\\partial s_o}{\\partial B} = \\frac{\\partial s_o}{\\partial C}\r\\end{aligned}\r$$1.3.2. 乘法 我们假设有：$C=AB$，那么：\n$$\rdC = dA\\; B + A\\; dB\r$$类似的，有：\n$$\r\\begin{aligned}\rds_o\r\u0026= \\langle \\frac{ds_o}{dC}, dC \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dC}, dA\\; B \\rangle + \\langle \\frac{ds_o}{dC}, A\\; dB\\rangle \\\\\r\u0026= \\langle \\frac{ds_o}{dC} B^T, dA \\rangle + \\langle A^T \\frac{ds_o}{dC}, dB\\rangle \\\\\r\u0026= \\langle \\frac{\\partial s_o}{\\partial A}, dA\\rangle + \\langle \\frac{\\partial s_o}{\\partial B}, dB\\rangle\r\\end{aligned}\r$$那么，我们可以归纳出:\n$$\r\\begin{aligned}\r\\frac{\\partial s_o}{\\partial A} \u0026= \\frac{ds_o}{dC} B^T \\\\\r\\frac{\\partial s_o}{\\partial B} \u0026= A^T \\frac{ds_o}{dC}\r\\end{aligned}\r$$1.3.3. 求逆 假设我们有：$C = A^{-1}$，可推得：\n$$\r\\begin{aligned}\r\u0026CA = I \\\\\r\\Longrightarrow \u0026 dC\\; A + C\\; dA = 0\\\\\r\\Longrightarrow \u0026 dC = -C dA C \\\\\r\\end{aligned}\r$$那么有正向传播：\n$$\r\\frac{d C}{d s_i} = -C \\frac{d A}{d s_i} C\r$$类似的，反向传播有：\n$$\r\\begin{aligned}\rds_o\r\u0026= \\langle \\frac{ds_o}{dC}, dC \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dC}, -C dA C \\rangle\\\\\r\u0026= \\langle -C^{T} \\frac{ds_o}{dC} C^T , dA\\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dA} , dA\\rangle\r\\end{aligned}\r$$那么有：\n$$\r\\frac{ds_o}{dA} = -C^{T} \\frac{ds_o}{dC} C^T\r$$1.3.4. 二次型 假设：$C= x^T A x$，那么有：\n$$\rdC = x^TA\\, dx + d(x^T A)x\r$$应为二次型结果是标量，那么 $d(x^T A)x$ 也应是标量，则有$(d(x^T A)x)^T = d(x^T A)x$，则有：\n$$\r\\begin{aligned}\rdC\r\u0026=x^{T}Adx+x^{T}d\\left(A^{T}x\\right) \\\\\r\u0026=x^{T}Adx+x^{T}A^{T}dx \\\\\r\u0026=x^{T}\\left(A+A^{T}\\right)dx\r\\end{aligned}\r$$类似的，反向传播结果为：\n$$\r\\begin{aligned}\rds_o\r\u0026= \\langle \\frac{ds_o}{dC}, dC \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dC}, x^{T}(A+A^{T})dx \\rangle\\\\\r\u0026= \\langle (A+A^{T})x\\frac{ds_o}{dC}, dx \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dx}, dx \\rangle\\\\\r\\end{aligned}\r$$那么有：\n$$\r\\frac{ds_o}{dx} = (A+A^{T})x\\frac{ds_o}{dC}\r$$1.3.5. 行列式 有：$C=det\\, A$。有 $A$ 的伴随矩阵 $\\tilde A$，伴随矩阵的定义为元素 $A_{ij}$ 的代数余子式，具体形式为：\n$$\r\\tilde A_{ij} = (-1)^{i+j}\\det(\\widehat{A}_{ij})\r$$根据行列式的定义，有：\n$$\r\\det A=\\sum_jA_{i,j}\\tilde{A}_{i,j}.\r$$我们固定 $i$，很容易可以得到：\n$$\r\\frac{\\partial C}{\\partial A_{i,j}}=\\tilde{A}_{i,j}\r$$可推知：\n$$\rdC = \\sum_{ij} \\tilde A_{ij} dA_ij\r$$将矩阵除法的定义 $A^{-1}=(\\det A)^{-1}\\tilde{A}^T$ 带入上式中，则有：\n$$\r\\begin{aligned}\rdC\r\u0026= \\sum_{ij} \\tilde A_{ij} dA_{ij}\\\\\r\u0026= \\sum_{ij} A^{-1}_{ij} \\det A dA_{ij}\\\\\r\u0026= \\det A\\sum_{ij} A^{-1}_{ij} dA_{ij}\\\\\r\u0026= C\\, Tr(A^{-1} dA)\\\\\r\\end{aligned}\r$$反向梯度有：\n$$\r\\begin{aligned}\rds_o\r\u0026= \\langle \\frac{ds_o}{dC}, dC \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dC}, C\\, Tr(A^{-1} dA) \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dC} C A^{-T}, dA \\rangle\\\\\r\u0026= \\langle \\frac{ds_o}{dA}, dA \\rangle\\\\\r\\end{aligned}\r$$那么有：\n$$\r\\frac{ds_o}{dA} = \\frac{ds_o}{dC} C A^{-T}\r$$2. 3DGS 中光栅化的正向和反向过程 2.1. 深度合成过程 2.1.1. 正向过程 此处的正向过程是指已经投影在二维像素平面上的二维高斯函数合成的过程。我们不加说明的给出合成的形式，具体推导可以参考上一篇文章。\n$$\r\\begin{aligned}\rC_i = \\sum_{n\\leq N} c_n \\cdot \\alpha_n \\cdot T_n\\\\\r\\text{where. } T_n = \\prod_{m","permalink":"https://wangjv0812.cn/2024/05/mathematics-in-3dgs-2/","summary":"\u003ch2 id=\"1-矩阵求导的常用方法\"\u003e1. 矩阵求导的常用方法\u003c/h2\u003e\n\u003ch3 id=\"11-矩阵求导的一般方法\"\u003e1.1. 矩阵求导的一般方法\u003c/h3\u003e\n\u003cp\u003e在矩阵论的课程中，我们学习过如下几种分析相关的知识，分别是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e向量对标量求导\u003c/li\u003e\n\u003cli\u003e向量对向量求导\u003c/li\u003e\n\u003cli\u003e向量对矩阵求导\u003c/li\u003e\n\u003cli\u003e矩阵对标量求导\u003c/li\u003e\n\u003cli\u003e矩阵对向量求导\u003c/li\u003e\n\u003cli\u003e矩阵对矩阵求导\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e事实上不难发现，我们只需要搞明白了矩阵对标量求导和矩阵对矩阵求导的方法，其他问题均可从这个两个原则推理开去。因此我们叙述的重点放在这两个问题上。\u003c/p\u003e\n\u003ch4 id=\"111-矩阵对标量求导\"\u003e1.1.1. 矩阵对标量求导\u003c/h4\u003e\n\u003cp\u003e假设我们有矩阵 $\\mathbf A$ 和标量 $k$，其中矩阵 $\\mathbf A$ 的展开形式为：\u003c/p\u003e\n$$\r\n\\mathbf A=  \r\n\\left[\r\n\\begin{matrix}\r\na_{11}\u0026  a_{12}\u0026  \\cdots\u0026 \\ a_{1n} \\\\\r\na_{21}\u0026  a_{22}\u0026  \\cdots\u0026 \\ a_{2n} \\\\\r\n\\vdots\u0026  \\vdots\u0026  \\ddots\u0026  \\vdots \\\\\r\na_{n1}\u0026  a_{n2}\u0026  \\cdots\u0026 \\ a_{nn} \\\\\r\n\\end{matrix}\r\n\\right]\r\n$$\u003cp\u003e那么，$\\frac{d \\mathbf A}{d k}$被定义为：\u003c/p\u003e\n$$\r\n\\frac{d \\mathbf A}{d k} =\r\n\\left[\r\n\\begin{matrix}\r\n\\frac{d a_{11}}{d k}\u0026  \\frac{d a_{12}}{d k}\u0026   \\cdots\u0026 \\ \\frac{d a_{1n}}{d k}\u0026  \\\\\r\n\\frac{d a_{21}}{d k}\u0026  \\frac{d a_{22}}{d k}\u0026   \\cdots\u0026 \\ \\frac{d a_{2n}}{d k}\u0026  \\\\\r\n\\vdots\u0026  \\vdots\u0026  \\ddots\u0026  \\vdots \\\\\r\n\\frac{d a_{n1}}{d k}\u0026  \\frac{d a_{n2}}{d k}\u0026   \\cdots\u0026 \\ \\frac{d a_{nn}}{d k}\u0026  \\\\\r\n\\end{matrix}\r\n\\right]\r\n$$\u003cp\u003e于上述定义类似，如果标量对矩阵求导，即$\\frac{d k}{d \\mathbf A}$，其定义为：\u003c/p\u003e","title":"Mathematics In 3DGS 2"},{"content":"1. 体渲染 体渲染的提出时为了解决如云、烟等非刚体的光学行为。可以理解为用于解决对光学不是完全反射，有复杂透射的光学行为。为了对这个光学行为建模，我们将云团（为了叙述方便，我们后面统一将被渲染物体称为云团）视为一团飘忽不定的粒子。光沿直线方向穿过一堆粒子 (粉色部分)，如果能计算出每根光线从最开始发射，到最终打到成像平面上的辐射强度，我们就可以渲染出投影图像。而渲染要做的就是对这个过程进行建模。为了简化计算，我们就假设光子只跟它附近的粒子发生作用，这个范围就是图中圆柱体大小的区间。\n1.1. 渲染行为分析 光线与粒子发生发生的作用有如下几类：\n吸收 (absorption)：光子被粒子吸收，会导致入射光的辐射强度减弱 放射 (emission)：粒子本身可能发光，这会进一步增大辐射强度。 外散射 (out-scattering)：光子在撞击到粒子后，可能会发生弹射，导致方向发生偏移，会减弱入射光强度。 内散射 (in-scattering)：其他方向的光子在撞到粒子后，可能和当前方向上的光子重合，从而增强当前光路上的辐射强度。 那么对于任意一个云团块而言，出射光与入射光之间的变化量，可以表示为这四个过程的叠加。我们假设入射光线的强度为$I_i$，出射光线为$I_o$，那么有：\n$$ l_o-\\mathrm{I}_i= dL(x,\\omega) =emission+inscattering-outscatting-absorption $$ 下面针对吸收、发射、内散射、外散射四个环节进行分析。\n1.1.1 吸收 我们假设半透明物体中的每个粒子的半径为$r$， 每个粒子的投影面积为$A=$ $\\pi r^2$， 并假设圆柱体中粒子的密度为$\\rho$，圆柱体的底面积是$E$，并且圆柱体的厚度足够薄。\n假定这个厚度是$\\Delta s$，那么在这个厚度内，圆柱体体积为$E\\Delta s$，粒子总数为$\\rho E \\Delta s$。这些粒子遮挡的面积为$\\rho E \\Delta s A$，占整个底面积的比例为$\\rho E\\Delta sA/E=\\rho A\\Delta s_{\\mathrm{o}}$。也就是说，当一束光通过这个圆柱体的时候，有$\\rho A\\Delta s$的概率会被遮挡。\n换句话说，如果我们在圆柱体的一端发射无数光线 (假设都朝相同的方向)，在另一端接收，会发现有些光线安然通过，有些则被粒子遮挡 (吸收)。但可以确定的是，这些接受到的光线总强度，相比入射光线总强度而言，会有$\\rho A\\Delta s$比例的衰减，即接受到的光的强度均值是入射光的$\\rho A\\Delta s$倍。其数学形式可以写作： $$ I_0 - I_i = \\Delta I = -\\rho(s)AI(s)\\Delta s $$ 这是一个关于粒子密法$\\rho$和$s$的函数，在空间中每个位置的密度是不同的。我们将上面的薄的圆柱体仍为时一个微元，那么可以将其转化为微分方程：\n$$ \\frac{dI}{ds}=-\\rho(s)AI(s)=-\\tau_{a}(s)I(s) $$那么有：\n$$ I(s)=I_{0}\\exp(-\\int_{0}^{s}\\tau_{a}(t)dt) $$其中$I_o$时表示了光线的起始点。那么针对出射光而言有：\n$$ I_{o}=I_{i}\\exp(-\\int_{i}^{o}\\tau_{a}(t)dt)_{0} $$此式的物理含义是显而易见的：如果离子云是均匀的，那么射入粒子云的光线会指数衰减，这被称为：比尔-朗伯吸收定律 (Beer-Lambert law)。\n我们可以接着定义出投射比：\n$$ T(s)=\\frac{I_o}{I_i}=\\exp(-\\int_i^o\\tau_a(t)dt) $$投射比代表了一个粒子云团的透明度。数值越大，说明粒子云团越透明，光线衰减越小。透明度本身时$\\tau_a(t)$的函数。$\\tau_a(t)$越大，透明度$T(s)$越小。我们称$\\tau_a(t)$为光学厚度。\n1.1.2. 放射 这里主要考虑粒子发光的情况。假设某个自立发射出的一束光的辐射强度为$I_e$，那么按照前文的描述，在圆柱体足够薄的情况下，粒子总数是$\\rho E \\Delta s A$，总发光强度为$I_e\\rho E \\Delta s A$。我们在接收粒子发出的光线时，并不是所有的光线都能接收到（因为光线路径上可能没有发光粒子）。能接收到光线的概率是$\\frac {\\rho AE\\Delta s}{E}=\\rho A\\Delta s$。那么接收到光线的平均强度为$I_e\\rho \\Delta s$。那么可以写出放射光强接受的微分方程：\n$$ \\frac{dI}{ds}=I_e(s)\\rho(s)A=I_e(s)\\tau_a(s) $$1.1.3. 外散射 除了吸收光子外，还可能会弹开光子，这个过程成为外散射。或者说光子被弹开原来的光路，导致之前的光线强度变弱。同吸收一样，外散射对光线有削弱作用也和光学厚度有关系。只不过这里的光学厚度影响因素负责很多。我们用$\\tau_s$来表示外散射对光线的削弱能力。那么有： $$ \\frac{dI}{ds}=-\\tau_{s}(s)I(s) $$1.1.4. 内散射 光线可以被弹离原本的光路，自然而然也会有其他光路的光子被弹到了我们看到的光路上，这个过程就是内散射。内散射要复杂很多，我们直接给出结论：假设其他光路的辐射强度为$I_s$，弹射到其他光路的能量损失为$\\tau_s$（因为都是散射损失，这里与外散射的损失系数相同）。那么这个过程可以描述为：\n$$ \\frac{dI}{ds}=\\tau_{s}(s)I_{s}(s) $$1.2. 渲染方程 我们将上面几个方程统一到一起，有：\n$$ \\frac{dI}{ds}= -\\tau_a(s)I(s) -\\tau_s(s)I(s) +\\tau_a(s)I_e(s) +\\tau_s(s)I_s(s) $$其中$-\\tau_a(s)I(s) -\\tau_s(s)I(s)$会衰减辐射强度，成为衰减项，$\\tau_a(s)I_e(s) +\\tau_s(s)I_s(s)$是独立的光源，称为源项。\n定义$\\tau = \\tau_a + \\tau_s$，那么有：\n$$ \\frac{dI}{ds}=-\\tau(s)I(s)+\\tau_a(s)I_e(s)+\\tau_s(s)I_s(s) $$求解，则有：\n$$ \\begin{aligned} I(s) =\u0026\\int_0^s\\exp(-\\int_0^r\\tau_t(t)dt)[\\tau_a(t)I_e(t)+\\tau_s(t)I_s(t)]dt\\\\ \u0026+I_{0}\\exp(-\\int_{0}^{s}\\tau_{t}(t)dt) \\end{aligned} $$可以看到，最终到达像平面的某一条光线主要由背景光$I_0$、发光$I_e$和其他光线$I_s$三个光构成。在传播的过程中都伴随着相同的衰减。由于从$I_0$到$I_s$的整个光路上处处都会发生我们所说的光学现象，因此需要从光源开始（$0$）积分到像平面（$s$），因此对应的积分就是$\\int ^s_0$\n为了方便运算对系统进行一定的简化，我们假设$\\tau， \\tau_a, \\tau_s$是不变且相等的，用$\\sigma$来表示。同时令：$C = I_e + I_s$，那么有：\n$$ \\begin{aligned} I(s) =\u0026\\int_0^s\\exp(-\\int_0^t\\sigma(u)du)\\sigma(t)C(t)dt \\\\ \u0026+I_{0}\\exp(-\\int_{0}^{s}\\sigma(t)dt) \\\\ =\u0026\\int_0^sT(t)\\sigma(t)C(t)dt+T(s)I_0 \\end{aligned} $$其中$T(s) = exp(-\\int^s_o\\sigma(t)dt)$\n这就是Nerf论文中所给出的渲染公式了。在3D Gaussian Splatting中，渲染方程还可以更简化一些，因为每个点都是由多个高斯函数叠加而来的，那么实际上渲染方程将简化为多个高斯函数的不透明度的叠加： $$ C=\\sum_{i\\in\\mathcal{N}}c_i\\alpha_i\\prod_{j=1}^{i-1}(1-\\alpha_j) $$ 其中$c_\\mathrm{i}$是每个高斯函数的颜色，$\\alpha_\\mathrm{i}$则由三维高斯向像平面投影的二维高斯的协方差$\\Sigma$乘以所优化的该点的不透明度信息(三维高斯向二维高斯投影的方法将在快速光栅化中介绍)。\n2. 快速光栅化 3D Gaussian Splatting通过使用三维高斯函数描述场景，实现了可微渲染的同时给出了解析的渲染方程，从而实现了将三维场景快速光栅化。快速光栅化的过程是将场景中的高斯球投影到相机的平面上。论文中给出的$\\Sigma$投影在二维像平面的形式为： $$ \\Sigma^{\\prime}=JW\\Sigma W^TJ^T $$ 第一步我们仍然将源空间（世界坐标系）下的坐标转换到相机空间里，这是一步仿射变换。但是此时投影得到的二维高斯球和像素坐标是没有对其的，此时与其计算相机空间里的每个基元关于平面的投影，然后计算哪些部分属于哪些像素；不如直接应用一个投影变换，将从相机原点发射的光线，映射为平行光。这样，被变换后的坐标，就可以很方便的进行体渲染了。变换后得到的空间我们称为“光线空间”。\n上图中上面的图片是相机空间的，可以看到从光心透出的光线会斜着穿过多个高斯球，如果直接投影计算会导致像素坐标和对应的高斯球并不对齐，无法计算。因此需要像下图一样，将光线变换为平行光，此时计算就十分容易了。考虑相机空间里的坐标$t=(t_0,t_1,t_2)^T$, 其中$t_2$所在的维度是我们$z$轴，即相机镜头的方向。记变换后坐标$x=(x_1,x_2,x_3)^T$, 那么整个变成： $$ \\begin{pmatrix}x_0\\\\x_1\\\\x_2\\end{pmatrix}=\\phi(\\mathbf{t})=\\begin{pmatrix}\\frac{t_0}{t_2}\\\\\\frac{t_1}{t_2}\\\\\\parallel(t_0,t_1,t_2)\\parallel^T\\end{pmatrix} $$ 此时我们使用的相机模型为针孔模型，此时存在一个视体（frustum），此时我们投影的光线并不平行，而是一个锥体：\n这里每次都计算不同方向的光线可能和哪些高斯椭球是“接触”( 斯椭球时我们往往会用其协方差矩阵作为轴的长度，类似一维时熟悉的比较麻烦的。需要注意的是投影形式中的$||(t_0,t_1,t_2)\\parallel^T$是有着重要意果是常规的透视变换，这里储存的“深度”往往是$t_2$，在3DGS需要这个来判定基元之间的前后顺序。这样带来了一些好处。但是不难察觉到投影变换是一个非线性变换，而高斯分布对非线性变换并不封闭。高斯分布有优良性质就被无情抛弃了。\nEWA Splatting 里提出一种办法，他们称其为“local affine approximation”。我们至少在每次投影时，其实都是知道高斯函数的位置(均值)的，所以我们可以直接计算$\\mathbf{x}_k=\\phi(\\mathbf{t}_k)$。但是此时的投影变换是非线性的，如果直接作用在高斯函数上会破坏原有的优雅的性质（协方差不再是一个线性变换）。因此我们需要将投影变换线性近似，进行泰勒展开，雅可比矩阵会自然的提供一个线性变换，来提供一个$\\mathbf{t}_k$邻域到$\\mathbf{x}_k$邻域之间的变换。即：\n$$ \\phi_k(\\mathbf{t})=\\phi_k(\\mathbf{t}_k)+J_k(\\mathbf{t}-\\mathbf{t}_k) $$ 这里的$J_k$可以由$\\phi_k(\\mathbf{t})$直接求偏导得到： $$ \\mathcal{I}_{\\mathrm{k}}=\\begin{pmatrix}\\frac{\\partial x_0}{\\partial t_0}\u0026\\frac{\\partial x_0}{\\partial t_1}\u0026\\frac{\\partial x_0}{\\partial t_2}\\\\\\frac{\\partial x_1}{\\partial t_0}\u0026\\frac{\\partial x_1}{\\partial t_1}\u0026\\frac{\\partial x_1}{\\partial t_2}\\\\\\frac{\\partial x_2}{\\partial t_0}\u0026\\frac{\\partial x_2}{\\partial t_1}\u0026\\frac{\\partial x_2}{\\partial t_2}\\end{pmatrix}=\\begin{pmatrix}1/t_{k,2}\u00260\u0026-t_{k,0}/t_{k,2}^2\\\\0\u00261/t_{k,2}\u0026-t_{k,1}/t_{k,2}^2\\\\\\frac{t_{k,0}}{\\|\\mathbf{t}_k\\|}\u0026\\frac{t_{k,1}}{\\|\\mathbf{t}_k\\|}\u0026\\frac{t_{k,2}}{\\|\\mathbf{t}_k\\|}\\end{pmatrix} $$ 我们利用二阶近似，解决了投影变换非线性的问题。考虑一个世界坐标系下的坐标$u$，仿射变换$t=\\varphi(\\mathbf{u})$ 可以得到在特定相机空间下的坐标$t$，$x=\\phi_k(t)$可以得到在光线空间下的坐标表示。我们已经保证了这两个映射是线性的，所以他们的复合也是线性的，我们将其记作$\\mathbf{m}_k(\\mathbf{u})$。我们可以将这个复合变换展开： $$ \\begin{gathered} \\mathbf{t}=\\varphi(\\mathbf{u})=\\mathbf{M}\\mathbf{u}+\\mathbf{b} \\\\ \\mathbf{x}=\\phi_{k}(\\mathbf{t})=\\mathbf{x}_{k}+\\mathbf{J}_{k}(\\mathbf{t-t}_{k}) \\\\ =\\mathbf{x}_k+\\mathbf{J}_k(\\mathbf{M}\\mathbf{u}+\\mathbf{b}-\\mathbf{t}_k) \\\\ =\\mathbf{J}_k\\mathbf{M}\\mathbf{u}+\\mathbf{x}_k+\\mathbf{J}_k(\\mathbf{b}-\\mathbf{t}_k) \\end{gathered} $$ 因为高斯函数在放射变化前后仍然保持不变，我们可以写出变换后的协方差矩阵和均值满足： $$ x=\\left(\\frac{1}{z}\\textbf{KT}_{\\mathbf{cw}}\\textbf{P}\\right),\\Sigma'=\\textbf{JR}_{cw}^{-1}\\Sigma\\textbf{R}_{cw}^{-T}\\textbf{J}^{T} $$ 这样我们实现了将三维高斯球函数投影到像平面的二维高斯函数的办法。结合前述的可微渲染，就可以实现高斯函数所表达的场景的快速光栅化了。\n3. 为什么使用高斯函数 在使用3DGS时，我们似乎很自然的认为使用高斯函数作为表达的基本单元是一个很理所当然的，以为它可微，且足够优雅。但是不妨再进一步，为什么一定是高斯函数，和点云、三角面片、简单的球体，甚至是更复杂的流形相比，它的优势究竟在哪？\n事实上，无论是点云、三角面片还是3DGS，作为以一种通过离散形式表达真实的、连续的世界的方式，都难免会涉及到将连续转换为离散的过程。在数学中这个过程被称为采样。\n3.1 采样、混叠和抗混叠 我们简单的回顾（学习）一些离散信号处理的知识。有一个特殊的函数称为$\\sigma$函数，也叫冲击函数。 $$ \\left.\\delta(t)=\\left\\{\\begin{matrix} 1\u0026t=0\\\\0\u0026t\\neq 0\\end{matrix}\\right.\\right. $$ 假设我们有一个连续函数$f(t)$，通过$\\sigma$函数可以简单且解析的将$f(t)$在$t=0$处的值取出： $$ f(0) = f(t)\\delta(t) $$ 虽然我们取出了$f(0)$的值，但是好的是并没有破坏原本的$f(t)$的性质。那么显然的，如果我们需要将$f(t)$按照间隔$m$离散化，只需要构建一个采样序列： $$ Shah(t) = \\sum_{n=-\\infty}^{\\infty} \\delta(t-nm) $$ 将这个采样序列作用在连续函数$f(t)$上： $$ f(k) = f(t)\\sum_{n=-\\infty}^{\\infty} \\delta(t-nm) $$ 就得到了原始的连续函数$f(x)$按照间隔$m$采样后的结果$f(k)$。\n这里需要引入几个数字信号处理中的简单知识：\n时域下的乘法通过傅里叶变换至频域后变为卷积。 一个信号与$\\sigma$函数卷积后仍为这个信号本身。 $Shah$的傅里叶变换仍然为$Shah$函数，只不过间距变为：$SHAH(s)=\\frac 1 T\\sum_{n=-\\infty}^{\\infty}\\delta(s-n\\frac{1}{m})$ 我们假设连续函数傅里叶变换后的结果为$F(\\omega)$，那么连续信号$f(t)$离散化的离散信号$f(k)$在频域下的形式为： $$ F(s)*SHAH(s) = \\frac 1 T\\sum_{n=-\\infty}^{\\infty}F(s-n\\frac1m) $$ 可以看到，原始的信号$f(t)$的频谱被以$\\frac1m$为周期重复了。由于傅里叶变换是一个无损变换，即时域所包含的信息被完全的转换到了频域上。那么频域上的混叠自然而然会导致时域上的失真。\n上图中左侧两张照片是一个频率不断在变化的正弦信号。我们知道正弦信号的傅里叶变换是一条对应周期的频线，就如右上图片所示。而右下则是将正弦信号离散化（采样）后的频谱，很容易看到原本的一条谱线变为了多条（但是只显示了两条），发生了混叠。\n更贴近我们工作的一个例子是摩尔纹。摩尔纹在对图片进行下采样时产生。下图中左右两张照片由于下采样的范围不同（与信号上的采样周期有着相同的含义）。\n不妨进一步的思考，看看三维场景的离散化上。我们可以认为真实场景是一个连续的三维场景，当我们使用点云描述三维场景时，实际上是使用一个三维上的采样序列作用在真实的场景上。那么必然的，会在频域上产生混叠，在时域上产生高频信息的丢失（因为混叠的产生导致高频信息被混叠的低频信号覆盖了）。可以看到此时点云描述并不是一个很好的主义，这也将引出后面使用Gaussian 函数描述（采样）为什么会有着比点云描述更好的效果。\n在搞明白采样的混叠的理论之后，我们对其进行简单的总结：\n假设对于一个连续信号$x_a(t)$按照频率$f=\\frac1T$进行采样，得到的离散序列为： $$ \\hat{x}_{a}\\left(t\\right)=\\sum_{n=-\\infty}^{\\infty}x_{a}\\left(t\\right)\\delta\\left(t-nT\\right) $$ 延拓后的频谱$\\hat{X}_a(j\\Omega)$为： $$ \\hat{X}_{a}(j\\Omega)=\\frac{1}{T}\\sum_{n=-\\infty}^{\\infty}X_{a}\\left(j\\Omega-jn\\frac{2\\pi}{T}\\right) $$面对混叠，我们显而易见的有两种办法来对抗它：\n增加采样频率，增加频域下混叠发生的距离。一般我们认为采样频域至少应该大于被采样信号的频带的一倍，才可以较好的重建信号；这就是大名鼎鼎的奈奎斯特采样定理。 在频域下使用一个门函数，滤去高频下的混叠。 我们主要考虑使用门函数进行滤波的情形。\n3.2 重建核 我们假设在频域下存在这样一个函数$G(j\\Omega)$： $$ \\left.G\\left(j\\Omega\\right)=\\left\\{\\begin{array}{l}T_s,|\\Omega|\\leqslant\\Omega_s/2\\\\0,|\\Omega|\u003e\\Omega_s/2\\end{array}\\right.\\right. $$ 其反变换回时域为： $$ g\\left(t\\right)=\\mathrm{Sa}\\left(\\frac{\\Omega_{s}}{2}t\\right)=\\frac{\\sin\\left(\\frac{\\Omega_{s}}{2}t\\right)}{\\frac{\\Omega_{s}}{2}t} $$ 其图像为：\n可以看到其只在0附近一个区域内有较大的值，我们称这个性质为局部支撑。我们直接对采样后的信号$\\hat{x}_a(t)$及进行卷积，并令数学专业同学心脏骤停的交换求和和积分的顺序：\n$$ \\begin{gathered} y\\left(t\\right)=\\hat{x}_{a}\\left(t\\right)*g\\left(t\\right) \\\\ =\\int_{-\\infty}^{+\\infty}\\left[\\sum_{n=-\\infty}^{\\infty}x_{a}\\left(\\tau\\right)\\delta\\left(\\tau-nT\\right)\\right]g\\left(t-\\tau\\right)\\mathrm{d}\\tau \\\\ =\\sum_{n=-\\infty}^{\\infty}\\int_{-\\infty}^{+\\infty}x_{a}\\left(\\tau\\right)g\\left(t-\\tau\\right)\\delta\\left(\\tau-nT\\right)\\mathrm{d}\\tau \\\\ =\\sum_{n=-\\infty}^\\infty x_a\\left(nT\\right)g\\left(t-nT\\right) \\\\ =\\sum_{n=-\\infty}^\\infty x_a\\left(nT\\right)\\frac{\\sin\\left(\\frac\\pi Tt-n\\pi\\right)}{\\left(\\frac\\pi Tt-n\\pi\\right)} \\end{gathered} $$可以看到最后结果的形势十分特殊，是原始的连续信号与$\\frac{\\sin\\left(\\frac\\pi Tt-n\\pi\\right)}{\\left(\\frac\\pi Tt-n\\pi\\right)}$相互作用。我们称$\\frac{\\sin\\left(\\frac\\pi Tt-n\\pi\\right)}{\\left(\\frac\\pi Tt-n\\pi\\right)}$ 为当前采样（重建）形式下的重建核。最后的重建结果是由原始信号的采样值和重建核张成的。在EWA splatting中，我们将重建过程看作了对一个连续的空间函数进行采样的结果。我们假设这个有待描述的连续场景的数学形式为$f_c(u)$，其中$u$是采样的空间位置。那么通过一个重建核进行描述时可以写成： $$ f_c\\left(\\mathbf{u}\\right)=\\sum_{k\\in\\mathbb{IN}}w_kr_k\\left(\\mathbf{u}\\right) $$ 上式中$\\omega_k$时采样的值，具体的物理意义由重建核的结构决定（有可能是体密度，有可能是别的形式）；$r_k(u)$时重建核。在EWA splating中，第一步是将待描述的原空间里的形式$f_c(u)$映射到像空间中，之后得到在像空间中的连续函数$g_c(\\mathbf x)$，可以描述为： $$ g_c\\left(\\mathbf{x}\\right)=\\left\\{\\mathcal{P}\\left(f_c\\right)\\right\\}\\left(\\mathbf{x}\\right) $$ 其中$\\mathcal{P}$是一个投影算子，$\\mathbf x$是屏幕上的二维坐标。我们对二维上的形式使用重建核进行重建，并交换投影算则$\\mathcal{P}$与求和算子的顺序，即先让投影算子作用在重建核上。则有： $$ \\begin{aligned} g_c(\\mathbf x) \u0026= \\mathcal P \\sum_{k\\in\\mathbb{IN}}w_kr_k\\left(\\mathbf{u}\\right)\\\\ \u0026= \\sum_{k\\in\\mathbb{IN}}w_k \\mathcal P r_k\\left(\\mathbf{u}\\right)\\\\ \u0026= \\sum_{k\\in\\mathbb{IN}}w_k p_k\\left(\\mathbf{u}\\right) \\end{aligned} $$ 我们给屏幕上的信号先加一个限，使用预滤波器$h(x)$： $$ \\hat g_{c}\\left(\\mathbf{x}\\right)=g_{c}\\left(\\mathbf{x}\\right)\\otimes h\\left(\\mathbf{x}\\right) $$ 我们将上面的卷积形式展开进行计算，则有： $$ \\begin{gathered} g_{c}\\left(\\mathbf{x}\\right)=\\int_{\\mathbb{R}^{2}}\\left\\{\\mathcal{P}\\left(\\sum_{k\\in\\mathbb{IN}}w_{k}r_{k}\\right)\\right\\}\\left(\\boldsymbol{\\eta}\\right)h\\left(\\mathbf{x}-\\boldsymbol{\\eta}\\right)\\mathrm{d}\\boldsymbol{\\eta} \\\\ =\\sum_{k\\in\\mathbb{N}}w_k\\int_{\\mathbb{IR}^2}p_k\\left(\\boldsymbol{\\eta}\\right)h\\left(\\mathbf{x}-\\boldsymbol{\\eta}\\right)\\mathrm{d}\\boldsymbol{\\eta} \\\\ =\\sum_{k\\in\\mathbb{IN}}w_k\\rho_k\\left(\\mathbf{x}\\right) \\end{gathered} $$ 其中有： $$ \\rho_k(\\mathbf x) = g_c \\otimes h (\\mathbf x) $$ 不难注意到，**对于场景的任何操作都可以归结为对重建核做操作。**3DGS使用了高斯分布作为重建核，我们可以直观的知道高斯函数对旋转、平移、缩放均保持封闭；其他的优良形式我们后面再讨论。\n3.3 Splatting过程 我们回忆一下前面体渲染和快速光栅化中所构建的重建过程，并使用重建核来形式化的描述这个过程。我们再回忆一下，splate过程分两步，第一步是将场景的三维坐标投影到二维像平面上，是一个仿射变换；第二步是一个线性化后的投影变换。我们称投影为平行光的空间为光线空间。我们记这个仿射变换为$\\varphi(\\cdot)$，将投影变换记作$\\phi(\\cdot)$。\n我们记前两个维度$(x_0, x_1, x_2)^T$为$(\\mathbf x, x_2)^T$。在 EWA Splatting 中使用的渲染方程在形式上更加严谨，但是不重要，我们按照其形式进行推导： $$ I\\left(\\mathbf{x}\\right)=\\int_{0}^{L}c\\left(\\mathbf{x},\\xi\\right)f_{c}^{\\prime}\\left(\\mathbf{x},\\xi\\right)e^{-\\int_{0}^{\\xi}f_{c}^{\\prime}\\left(\\mathbf{x},\\mu\\right)\\mathrm{d}\\mu}\\mathrm{d}\\xi $$ $I_\\lambda(\\cdot)$指的是光强。然后$f_c^{\\prime}\\left(\\mathbf{x},\\xi\\right)$是“消光系数”,用于建模光的“自我遮挡”。然后也有熟悉的 $I_{\\lambda}(\\cdot)$指的是光强。负指数作为衰减因子。$c\\left(\\mathbf{x},\\xi\\right)$是“发射系数”。这些含义有些繁琐，我们不必拘泥于这些复杂的含义中，就按照前面介绍的形式理解即可。\n我们使用重建核对渲染方程中的消光系数进行重建。由于我们将源空间到相机空间记作$\\varphi(\\cdot)$。相机空间到光线空间记作$\\phi(\\cdot)$。那么对于之前定义的$f_c(\\mathbf{u})$，对于一个光线空间里的坐标$\\mathbf{x}$，我们可以依次进行逆变换，然后计算此时光线空间里坐标为$x$的位置，其对应的属性是多少。我们这里假设$f_c(\\mathbf{u})$就是源空间里的消光系数，所以自然有： $$ f_c^{\\prime}\\left(\\mathbf{x}\\right)=f_c\\left(\\varphi^{-1}\\left(\\phi^{-1}\\left(\\mathbf{x}\\right)\\right)\\right)=\\sum_{k\\in\\mathbb{IN}}w_kr_k^{\\prime}\\left(\\mathbf{x}\\right) $$ 显然的，变换的作用可以直接吸收到重建核中。我们先将$f^\\prime_c(\\mathbf x)$带入渲染方程中： $$ \\begin{gathered} \\begin{aligned}I\\left(\\mathbf{x}\\right)\u0026=\\int_{0}^{L}c\\left(\\mathbf{x},\\xi\\right)\\sum_{k\\in\\mathbb{IN}}w_{k}r'_{k}\\left(\\mathbf{x},\\xi\\right)e^{-\\int_{0}^{\\xi}\\sum_{j\\in\\mathbb{IN}}w_{j}r'_{j}(\\mathbf{x},\\mu)\\mathrm{d}\\mu}\\mathrm{d}\\xi\\end{aligned} \\\\ =\\sum_{k\\in\\mathbb{IN}}w_k\\left(\\int_0^Lc\\left(\\mathbf{x},\\xi\\right)r_k^{\\prime}\\left(\\mathbf{x},\\xi\\right)e^{-\\sum_{j\\in\\mathbb{N}}w_j\\int_0^\\xi r_j^{\\prime}(\\mathbf{x},\\mu)\\mathrm{d}\\mu}\\mathrm{d}\\xi\\right) \\\\ =\\sum_{k\\in\\mathbb{N}}w_k\\left(\\int_0^Lc\\left(\\mathbf{x},\\xi\\right)r_k^{\\prime}\\left(\\mathbf{x},\\xi\\right)\\prod_je^{-w_j\\int_0^\\xi r_j^{\\prime}(\\mathbf{x},\\mu)\\mathrm{d}\\mu}\\mathrm{d}\\xi\\right) \\end{gathered} $$ 此时的$I(\\mathbf x)$的形式是一个加权和，其与前式中的$g_c(\\mathbf x)$具有类似的含义，为了表达的一般性，我们使用$g_c(\\mathbf x)$代替$I(\\mathbf x)$。想要精确的计算上面这个形式是复杂甚至的不可能的，我们需要对齐进行一些简化假设：\n重建核是局部支撑的，这就意味着再积分过程中我们只对重建核显著非0的部分感兴趣，可以将积分区域变为重建核支撑的部分积分。此外一般在这条光路上这些支撑区域是补充个和的。于是我们自然的假设，在对应的支撑区域内$c(\\mathbf x, \\xi)$是不变的。\n通过这个假设，我们可以把$c(\\mathbf x, \\xi)$从积分中提出来，并对指数函数做线性展开$e^x \\approx 1-x$，则有： $$ g_{c}\\left(\\mathbf{x}\\right)=\\sum_{k\\in\\mathbb{N}}w_{k}c\\left(\\mathbf{x}\\right)\\left(\\int_{0}^{L}r_{k}^{\\prime}\\left(\\mathbf{x},\\xi\\right)\\prod_{j}\\left(1-w_{j}\\int_{0}^{\\xi}r_{j}^{\\prime}\\left(\\mathbf{x},\\mu\\right)\\mathrm{d}\\mu\\right)\\mathrm{d}\\xi\\right) $$ 为了对齐$\\Pi$内外的积分形式。需要再引入一个假设，即认为我们不关注穿过基元中光线的消耗（即$\\int^\\xi_0$），只关注穿过整个不透明区域后的结果（即$\\int^L_0$）。此时形式变为： $$ g_{c}\\left(\\mathbf{x}\\right)=\\sum_{k\\in\\mathbb{N}}w_{k}c\\left(\\mathbf{x}\\right)\\left(\\int_{0}^{L}r_{k}^{\\prime}\\left(\\mathbf{x},\\xi\\right)\\prod_{j}\\left(1-w_{j}\\int_{0}^{L}r_{j}^{\\prime}\\left(\\mathbf{x},\\mu\\right)\\mathrm{d}\\mu\\right)\\mathrm{d}\\xi\\right) $$ 可以注意到连乘的整个形式和前面的积分和求和无关了： $$ \\begin{gathered} g_{c}\\left(\\mathbf{x}\\right)=\\sum_{k\\in\\mathbb{IN}}w_{k}c\\left(\\mathbf{x}\\right)\\left(\\int_{0}^{L}r_{k}^{\\prime}\\left(\\mathbf{x},\\xi\\right)\\prod_{j}\\left(1-w_{j}\\int_{0}^{L}r_{j}^{\\prime}\\left(\\mathbf{x},\\mu\\right)\\mathrm{d}\\mu\\right)\\mathrm{d}\\xi\\right) \\\\ =\\sum_{k\\in\\mathbb{N}}w_kc\\left(\\mathbf{x}\\right)\\left(\\int_0^Lr_k^{\\prime}\\left(\\mathbf{x},\\xi\\right)\\mathrm{d}\\xi\\right)\\prod_j\\left(1-w_j\\int_0^Lr_j^{\\prime}\\left(\\mathbf{x},\\mu\\right)\\mathrm{d}\\mu\\right) \\end{gathered} $$ 我们记： $$ q_{k}\\left(\\mathbf{x}\\right)=\\int_{\\mathbb{R}}r_{k}^{\\prime}\\left(\\mathbf{x},\\xi\\right)\\mathrm{d}\\xi $$ 此时$q_k(x)$可以很大程度的简化$g_c$的形式： $$ g_{c}\\left(\\mathbf{x}\\right)=\\sum_{k\\in\\mathbb{IN}}w_{k}c\\left(\\mathbf{x}\\right)q_{k}\\left(\\mathbf{x}\\right)\\prod_{j=0}^{k-1}\\left(1-w_{j}q_{j}\\left(\\mathbf{x}\\right)\\right) $$ 形式$q_k(x)$挺有用的，它被称为足迹函数，描述了重建核沿着一个维度积分的结果，描述了三维的重建核被投影到二维（降维）的过程。此时的$g_c(x)$是一个真正意义上的二维连续函数，也是最朴素含义上的splatting，已经十分接近朴素的重建核的$g_{c}\\left(\\mathbf{x}\\right)=\\sum_{k\\in\\mathbb{IN}}w_{k}p_{k}\\left(\\mathbf{u}\\right)$。此时$g_c(\\mathbf x)$还是一个连续函数，我们希望通过$g_{c}^{\\prime}\\left(\\mathbf{x}\\right)=g_{c}\\left(\\mathbf{x}\\right)\\otimes h\\left(\\mathbf{x}\\right)$。的处理将其变为带限的。我们尝试展开卷积： $$ \\begin{gathered} \\begin{aligned}g'_c\\left(\\mathbf{x}\\right)=g_c\\left(\\mathbf{x}\\right)\\otimes h\\left(\\mathbf{x}\\right)\\end{aligned} \\\\ =\\sum_kw_k\\int_{\\mathbb{IR}^2}c_k\\left(\\eta\\right)q_k\\left(\\eta\\right)\\prod_{j=0}^{k-1}\\left(1-w_jq_j\\left(\\eta\\right)\\right)h\\left(\\mathbf{x}-\\eta\\right)\\mathrm{d}\\eta \\end{gathered} $$ 我们假设$c_k(\\cdot)$是常数，那么可以得到： $$ \\begin{gathered} \\begin{aligned}\\prod_{j=0}^{k-1}\\left(1-w_jq_j\\left(\\mathbf{x}\\right)\\right)\\approx o_k\\end{aligned} \\\\ \\begin{aligned}g_{k}\\left(\\mathbf{x}\\right)\\otimes h\\left(\\mathbf{x}\\right)=\\sum_{k}w_{k}c_{k}o_{k}\\int_{\\mathbb{R}^{2}}q_{k}\\left(\\eta\\right)h\\left(\\mathbf{x}-\\eta\\right)\\mathrm{d}\\eta\\end{aligned} \\\\ =\\sum_{k}w_{k}c_{k}o_{k}\\left(q_{k}\\left(\\mathbf{x}\\right)\\otimes h\\left(\\mathbf{x}\\right)\\right) \\\\ =\\sum_kw_kc_ko_k\\rho_k\\left(\\mathbf{x}\\right) \\end{gathered} $$ 不难察觉到，经过经过了对于$c_k(\\cdot)$的简化假设后，第$k$个元（高斯球）的颜色与坐标无关，正好对应了每个高斯球的颜色是一样的并且不随距离发生变化。\n3.4 高斯分布作为重建核所带来的特殊结构 在前面的推导中，我们没有指明重建核的具体形式。但是也能察觉到，。假如这个重建核函数和预滤波器作卷积（当然预滤波器也是我们可以选择的）后仍然有和原来一样的表达，假如这个重建核函数经过线性变换后，其函数参数也可以被线性的变化，假如这个重建函数，沿某个维度积分后，也能保持类似的函数结构，推导的形式将会简化非常非常多。高斯分布此处有很多重要且有意义的性质，我们不做具体的推导了，但是在此处将高斯分布所具有的性质列出。具体的证明在附录给出：\n高斯分布的傅里叶变换还是高斯分布 高斯分布在平移和旋转后还是高斯分布 高斯分布和高斯分布的卷积还是高斯分布 高斯分布进行仿射变换后还是高斯分布 高斯分布的边缘分布还是高斯分布 参考 体渲染简明教程\nNeRF入门之体渲染 (Volume Rendering)\nEWA Splatting\n3D Gaussian Splatting for Real-Time Radiance Field Rendering\n3D Gaussian Splatting中的数学推导\n附录 TODO：附录内容后面补充\nA. 如何解常微分方程 对于形如： $$ \\frac{dy(x)}{dx} = f(x)y(x) $$ 有： $$ \\begin{gather} \\frac{1}{y}dy = f(x)dx\\\\ \\ln y = \\int f(x) dx + C\\\\ y(x) = \\exp\\left\\{\\int f(x) dx + C\\right\\} \\end{gather} $$B. 时域、频域、卷积 C. 在重建中高斯分布的重要性质证明 ","permalink":"https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/","summary":"\u003ch2 id=\"1-体渲染\"\u003e1. 体渲染\u003c/h2\u003e\n\u003cp\u003e体渲染的提出时为了解决如云、烟等非刚体的光学行为。可以理解为用于解决对光学\u003cstrong\u003e不是完全反射\u003c/strong\u003e，有复杂\u003cstrong\u003e透射\u003c/strong\u003e的光学行为。为了对这个光学行为建模，我们将云团（为了叙述方便，我们后面统一将被渲染物体称为云团）视为一团飘忽不定的粒子。光沿直线方向穿过一堆粒子 (粉色部分)，如果能计算出每根光线从最开始发射，到最终打到成像平面上的辐射强度，我们就可以渲染出投影图像。而渲染要做的就是对这个过程进行建模。为了简化计算，我们就假设光子只跟它附近的粒子发生作用，这个范围就是图中圆柱体大小的区间。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Volumn Rendering\" loading=\"lazy\" src=\"/2024/05/mathematics-in-3dgs-1/Images/image-20240125001336326.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"11-渲染行为分析\"\u003e1.1. 渲染行为分析\u003c/h3\u003e\n\u003cp\u003e光线与粒子发生发生的作用有如下几类：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e吸收 (absorption)\u003c/strong\u003e：光子被粒子吸收，会导致入射光的辐射强度减弱\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e放射 (emission)\u003c/strong\u003e：粒子本身可能发光，这会进一步增大辐射强度。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e外散射 (out-scattering)\u003c/strong\u003e：光子在撞击到粒子后，可能会发生弹射，导致方向发生偏移，会减弱入射光强度。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e内散射 (in-scattering)\u003c/strong\u003e：其他方向的光子在撞到粒子后，可能和当前方向上的光子重合，从而增强当前光路上的辐射强度。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"Volumn Rendering\" loading=\"lazy\" src=\"/2024/05/mathematics-in-3dgs-1/Images/image-20240125001538229.png\"\u003e\u003c/p\u003e\n\u003cp\u003e那么对于任意一个云团块而言，出射光与入射光之间的变化量，可以表示为这四个过程的叠加。我们假设入射光线的强度为$I_i$，出射光线为$I_o$，那么有：\u003c/p\u003e\n$$\nl_o-\\mathrm{I}_i= dL(x,\\omega) =emission+inscattering-outscatting-absorption\n$$\u003cp\u003e\n下面针对吸收、发射、内散射、外散射四个环节进行分析。\u003c/p\u003e\n\u003ch4 id=\"111-吸收\"\u003e1.1.1 吸收\u003c/h4\u003e\n\u003cp\u003e我们假设半透明物体中的每个粒子的半径为$r$， 每个粒子的投影面积为$A=$ $\\pi r^2$， 并假设圆柱体中粒子的密度为$\\rho$，圆柱体的底面积是$E$，并且圆柱体的厚度足够薄。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Volumn Rendering\" loading=\"lazy\" src=\"/2024/05/mathematics-in-3dgs-1/Images/image-20240125003153333.png\"\u003e\u003c/p\u003e\n\u003cp\u003e假定这个厚度是$\\Delta s$，那么在这个厚度内，圆柱体体积为$E\\Delta s$，粒子总数为$\\rho E \\Delta s$。这些粒子遮挡的面积为$\\rho E \\Delta s A$，占整个底面积的比例为$\\rho E\\Delta sA/E=\\rho A\\Delta s_{\\mathrm{o}}$。也就是说，当一束光通过这个圆柱体的时候，有$\\rho A\\Delta s$的概率会被遮挡。\u003c/p\u003e\n\u003cp\u003e换句话说，如果我们在圆柱体的一端发射无数光线 (假设都朝相同的方向)，在另一端接收，会发现有些光线安然通过，有些则被粒子遮挡 (吸收)。但可以确定的是，这些接受到的光线总强度，相比入射光线总强度而言，会有$\\rho A\\Delta s$比例的衰减，即接受到的光的强度均值是入射光的$\\rho A\\Delta s$倍。其数学形式可以写作：\n\u003c/p\u003e\n$$\nI_0 - I_i = \\Delta I = -\\rho(s)AI(s)\\Delta s\n$$\u003cp\u003e\n这是一个关于粒子密法$\\rho$和$s$的函数，在空间中每个位置的密度是不同的。我们将上面的薄的圆柱体仍为时一个微元，那么可以将其转化为微分方程：\u003c/p\u003e\n$$\n\\frac{dI}{ds}=-\\rho(s)AI(s)=-\\tau_{a}(s)I(s)\n$$\u003cp\u003e那么有：\u003c/p\u003e\n$$\nI(s)=I_{0}\\exp(-\\int_{0}^{s}\\tau_{a}(t)dt)\n$$\u003cp\u003e其中$I_o$时表示了光线的起始点。那么针对出射光而言有：\u003c/p\u003e\n$$\nI_{o}=I_{i}\\exp(-\\int_{i}^{o}\\tau_{a}(t)dt)_{0}\n$$\u003cp\u003e此式的物理含义是显而易见的：如果离子云是均匀的，那么射入粒子云的光线会指数衰减，这被称为：比尔-朗伯吸收定律 (Beer-Lambert law)。\u003c/p\u003e","title":"Mathematics In 3DGS 1"}]