<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>WangJV Blog</title><link>https://wangjv0812.cn/</link><description>Recent content on WangJV Blog</description><image><title>WangJV Blog</title><url>https://wangjv0812.cn/</url><link>https://wangjv0812.cn/</link></image><generator>Hugo -- 0.152.2</generator><language>en-us</language><lastBuildDate>Tue, 11 Nov 2025 12:11:00 +0800</lastBuildDate><atom:link href="https://wangjv0812.cn/index.xml" rel="self" type="application/rss+xml"/><item><title>SLAM HandbookFrom Localization and Mapping to Spatial Intelligence</title><link>https://wangjv0812.cn/resources/slam-handbook/</link><pubDate>Tue, 11 Nov 2025 12:11:00 +0800</pubDate><guid>https://wangjv0812.cn/resources/slam-handbook/</guid><description>新、全面且系统的 SLAM 教材</description></item><item><title>A Mathematical Introduction of Robotic Manipulation</title><link>https://wangjv0812.cn/resources/mathematicalintroductionofroboticmanipulation/</link><pubDate>Mon, 22 Sep 2025 11:51:00 +0800</pubDate><guid>https://wangjv0812.cn/resources/mathematicalintroductionofroboticmanipulation/</guid><description>应用级的机器人学李代数介绍，没有深入介绍数学基础，更倾向于技术应用</description></item><item><title>Fundamentals of Computer Graphics Fourth Edition</title><link>https://wangjv0812.cn/resources/fundamentalsofcomputergraphicsfourthedition/</link><pubDate>Mon, 22 Sep 2025 11:51:00 +0800</pubDate><guid>https://wangjv0812.cn/resources/fundamentalsofcomputergraphicsfourthedition/</guid><description>著名的《虎书》，计算机图形学的基础教材，涵盖了图形学的基本原理和应用</description></item><item><title>Naive Lie Theory</title><link>https://wangjv0812.cn/resources/naivelietheory/</link><pubDate>Mon, 22 Sep 2025 11:51:00 +0800</pubDate><guid>https://wangjv0812.cn/resources/naivelietheory/</guid><description>Lie Group 和 Lie Algebra 入门教材，没有规避深刻的数学模型，但是使用的方法很处等</description></item><item><title>State Estimation for Robotics</title><link>https://wangjv0812.cn/resources/state-estimation-for-robotics-copy/</link><pubDate>Mon, 22 Sep 2025 10:01:00 +0800</pubDate><guid>https://wangjv0812.cn/resources/state-estimation-for-robotics-copy/</guid><description>机器人状态估计权威教材</description></item><item><title>From Diffusion to Diffusion Language Model</title><link>https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/</link><pubDate>Tue, 04 Nov 2025 18:35:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/</guid><description>&lt;p&gt;&lt;img alt="alt text" loading="lazy" src="https://wangjv0812.cn/2025/11/from-diffusion-to-diffusion-language-model/posts/From%20Diffusion%20to%20Diffusion%20Language%20Model/Images/DLMinrecentyear.png"&gt;&lt;/p&gt;
&lt;p&gt;对于现在的大模型，普遍使用自回归模型。对于一个长度为 $n$ 的文本，自回归模型将其分解为：&lt;/p&gt;
$$
P(x_{1:n}) = \prod_{i=1}^n P(x_i \mid x_{&lt;i})
$$&lt;p&gt;这也是在做自回归时，使用 Decoder Only 的原因。但是一个清晰的问题是，自回归模型是自然语言模型的唯一选择吗？答案可能不是100%的不。diffusion 似乎是一个很符合直接的推理过程，它会先写出一个文本的草稿，之后再对草稿不断修改（降噪），最终得到完整的文本。&lt;/p&gt;
&lt;h2 id="1-无监督学习和流形假说"&gt;1. 无监督学习和流形假说&lt;/h2&gt;
&lt;p&gt;对于经典的无监督学习任务，我们面对的是巨量的、常常高维的、没有人工提取特征的原始数据。无监督学习最核心的任务是在找到这些数据之间的相对关系提取出数据的内在结构。我们不妨以图像生成任务举例子，对于一个尺寸为 $1920 \times 1080$ 的图片，其像素总量为 $2073600$。但是这些维度之不可能是独立的（因为如果完全独立，只能得到没有任何信息的纯噪声），数据的维度之间存在着复杂的相关性质。换句话说，对于图像数据，数据点并不是均匀的分布在 $2073600$ 维的空间中，它只分布在空间中的一小部分区域，或者用微分几何的语言描述，数据分布在一个高维空间中的一条流形（manifold）上。这就是流形假说（manifold hypothesis），它是无监督学习的理论基础。那么无监督学习的任务就变成了如何用一个神经网络来建模这条流形。一个很显然的流形建模工具是使用概率分布，对于不数据流形上的点，概率赋 $0$ 就好了。&lt;/p&gt;
&lt;p&gt;关于流形假说的更多讨论，可以参考 &lt;a href="https://wangjv0812.cn/2025/08/noise-contrastive-estimation/#1-%E5%8A%A8%E6%9C%BA"&gt;Noise Contrastive Estimation/1. 动机&lt;/a&gt; 中对于使用概率分布建模数据分布流形的讨论。&lt;/p&gt;
&lt;h2 id="2-continuous-diffusion"&gt;2. Continuous Diffusion&lt;/h2&gt;
&lt;p&gt;Diffusion 模型可能是目前最成功的高维连续数据的建模工具。相比于 &lt;a href="https://wangjv0812.cn/2025/10/variational-autoencoder/"&gt;VAE&lt;/a&gt;、GAN、&lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 等，Diffusion 很核心的优势在于&lt;strong&gt;它显示式的给出了一条图像生成的路径（从噪声到数据），大大减少了生成器所需要搜索的空间范围&lt;/strong&gt;。如果你现在对这句话一头雾水，不妨先对它有一个大概的印象，看完本节后很大概率会有一个跟深刻的认识。&lt;/p&gt;
&lt;p&gt;Diffusion 模型可以分为三个过程，分别是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Forward Process，向原始数据添加噪声，事实上可以理解为一个数据增强的过程。&lt;/li&gt;
&lt;li&gt;Backward Process，从噪声中恢复数据的过程，可以理解为 Diffusion 的推理。&lt;/li&gt;
&lt;li&gt;训练过程，训练一个神经网络来学习 Backward Process。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="21-forward-process"&gt;2.1. Forward Process&lt;/h3&gt;
&lt;p&gt;对于一个数据样本 $x$，不妨将其标记为 $x_0$，在后面的讨论中，可以将 $x_0$ 视作一个确定性变量。Diffusion 过程每一步都向原始数据添加一个很小的噪声，添加噪声的过程不妨用一个条件概率描述：&lt;/p&gt;
$$
p(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
$$&lt;p&gt;其中 $\beta_t \in (0, 1)$，是一个用于控制噪声大小的超参数。我们希望求得 $x_t$ 在 $x_0$ 下的条件分布：&lt;/p&gt;</description></item><item><title>Geodesic and Lie Algebra in SO(3)</title><link>https://wangjv0812.cn/2025/10/geodesic-and-lie-algebra-in-so3/</link><pubDate>Fri, 31 Oct 2025 11:37:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/10/geodesic-and-lie-algebra-in-so3/</guid><description>&lt;h2 id="1-测地线geodesic"&gt;1. 测地线（Geodesic）&lt;/h2&gt;
&lt;p&gt;对于一个曲率不为 $0$ 的流形，其上任意两点之间的最短路径无法简单的像是欧式几何上一样，直接通过一条平直曲线描述。因此在一个曲面上，我们定一任意两点之间最短的路径为测地线（Geodesic）。我们不妨记在 $SO(3)$ 上受到参数 $t$ 控制的任意曲线为 $R(t)$。对于一个 Riemann 度量 $\| \cdot \|$，我们可以定义曲线的弧长：&lt;/p&gt;
$$
\begin{gather}
L(R)\int_a^b \sqrt{\| dR \|^2} = \int_a^b \sqrt{\| \dot R \|^2} dt
\end{gather}
$$&lt;p&gt;这就是在一个流形（或者说曲面）上关于任意曲线的弧长泛函。对于上个泛函，我们只需要对 $R(t)$ 变分，得到任意两点之间弧长泛函最小的曲线 $R(t)$ 满足的形式，测地线问题就解决了。但是如果你对变分和 Eular-Lagrange 方程熟悉的话，会自然的发现上面这个形式要处理并不容易。我们一般会扔掉 $\sqrt{\cdot}$，直接取曲线的能量泛函：&lt;/p&gt;
$$
\begin{gather}
E(R) = \frac{1}{2} \int_a^b \| \dot R \|^2 dt
\end{gather}
$$&lt;h3 id="11-so3-上的-riemann-度量"&gt;1.1. SO(3) 上的 Riemann 度量&lt;/h3&gt;
&lt;p&gt;在任意的 Riemann 流形 $M$ 上的每个点 $p \in M$，切空间 $T_pM$ 是一个向量空间。我们在这个空间上定义一个内积：&lt;/p&gt;
$$
\langle \cdot, \cdot \rangle_p : T_pM \times T_pM \to \mathbb{R}
$$&lt;p&gt;内积需要满足四个性质：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对称性：$\langle X, Y \rangle_p = \langle Y, X \rangle_p$&lt;/li&gt;
&lt;li&gt;双线性：线性于两个分量&lt;/li&gt;
&lt;li&gt;正定性：$\langle X, X \rangle_p &gt; 0$ 当 $X \neq 0$&lt;/li&gt;
&lt;li&gt;平滑性：对于任意光滑向量场 $X, Y$，$\langle X, Y \rangle$ 是一个光滑函数&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当我们将每个点的内积拼接起来时，可以得到一个光滑的张量场：&lt;/p&gt;</description></item><item><title>Variational Bayesian Inference</title><link>https://wangjv0812.cn/2025/10/variational-bayesian-inference/</link><pubDate>Tue, 28 Oct 2025 23:35:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/10/variational-bayesian-inference/</guid><description>&lt;h2 id="1-动机"&gt;1. 动机&lt;/h2&gt;
&lt;p&gt;对于一个状态 $x$，在通过传感器进行一次观测 $y$ 后，我们希望可以计算出后验分布 $p(x\mid y)$，即在观测到 $y$ 的情况下，状态 $x$ 的分布。为了计算后验分布的形式，我们可以使用贝叶斯公式：&lt;/p&gt;
$$
\begin{aligned}
p(x\mid y)
&amp;= \frac{p(y\mid x) p(x)}{p(y)}\\
&amp;= \frac{p(y\mid x) p(x)}{\int p(y\mid x) p(x) \mathrm{d}x}
\end{aligned}
$$&lt;p&gt;对于状态估计问题，我们只需要找到使得 $p(x\mid y)$ 最大的状态 $x$。 由于 $p(y)$ 是一个常数，我们可以将其忽略，只需要最大化分子部分。但是一些应用场景中，还需要我们计算出具体的后验分布的形式。但是这往往非常的难，边缘似然 $p(y)$ 的计算往往是几乎不可能的。因此一个或许可行的办法是，使用一个相对简单的分布 $q(x)$ 代替后验分布。那么原本的贝叶斯问题变成一个变分优化问题，不妨用 KL 散度衡量差异。&lt;/p&gt;
&lt;h2 id="2-kl-散度的不对称性"&gt;2. KL 散度的不对称性&lt;/h2&gt;
&lt;p&gt;但是一个很核心的问题是，KL 散度不具有对称性，$D_{KL}(p\| q) \neq D_{KL}(q\| p)$，在实际操作中，我们应该如何选择？不妨先观察这两个对称的 KL 散度的具体形式：&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(p \| q)
&amp;= -\int p(x\mid y) \log \frac{q(x)}{p(x\mid y)} \mathrm{d}x\\
&amp;= -\int p(x\mid y) \log q(x) \mathrm{d}x + \int p(x\mid y) \log p(x\mid y) \mathrm{d}x
\end{aligned}
$$&lt;p&gt;其中，$\int p(x\mid y) \log q(x) \mathrm{d}x$ 实质上是 $p$ 与 $q$ 之间的交叉熵，不妨定义 $H(p, q) = \int p(x\mid y) \log q(x) \mathrm{d}x$。而 $\int p(x\mid y) \log p(x\mid y) \mathrm{d}x$ 则是关于 $p$ 的熵。$p$ 的熵实质上是一个常数，因此实质上：&lt;/p&gt;</description></item><item><title>Variational AutoEncoder</title><link>https://wangjv0812.cn/2025/10/variational-autoencoder/</link><pubDate>Thu, 23 Oct 2025 21:24:00 +0800</pubDate><guid>https://wangjv0812.cn/2025/10/variational-autoencoder/</guid><description>&lt;h2 id="1-动机和推导"&gt;1. 动机和推导&lt;/h2&gt;
&lt;p&gt;对于无监督的样本生成问题，我们之前已经提到过很多次，对于一系列同类型的数据 $\{x_1, x_2, \cdots, x_n\}$，我们假设存在一个理想的分布 $p(x)$，这些数据都是从这个分布中采样得到的。但是想直接学习这个分布非常难。那么是否有可能通过一个足够复杂的神经网络来近似这个理想分布？此外，有一定神经经验的朋友往往有一个信念：“&lt;em&gt;压缩即智能&lt;/em&gt;”。那么是否可以先学习一个编码器 $p(z\mid x)$ 将原有的随机向量 $x$ 变换到一个低维的潜空间 $z$，然后再通过一个解码器 $p(x\mid z)$ 将潜空间的向量 $z$ 重新映射回原始空间，从而实现对数据的生成和重构？&lt;/p&gt;
&lt;p&gt;变分自编码器（Variational AutoEncoder）就是这个思路的具体实现。显然，通过解码器重建分布 $p(x)$ 可以描述为：&lt;/p&gt;
$$
p_\theta(x) = \int p_\theta(x\mid z) p(z) \, dz
$$&lt;p&gt;那么训练 $p_\theta(x)$ 的一个显然的方法是优化其与真实分布 $p(x)$ 之间的 KL 散度：&lt;/p&gt;
$$
\theta^* = \arg \min_\theta D_{KL}\bigg(p(x) \| p_\theta(x)\bigg)
$$&lt;p&gt;不妨展开 KL 散度，容易发现：&lt;/p&gt;
$$
\begin{aligned}
\theta^*
&amp;= \arg \min_\theta D_{KL}\left(p(x) \| p_\theta(x)\right)\\
&amp;= \arg \min_\theta \bigg\{\mathbb{E}_{x\sim p(x)}\left[\log p(x) \right]- \mathbb{E}_{x\sim p(x)}\left[\log p_\theta(x)\right]\bigg\}
\end{aligned}
$$&lt;p&gt;其中 $\mathbb{E}_{x\sim p(x)}\left[\log p(x) \right]$ 实际上是真实分布 $p(x)$ 的熵，不包含可以优化的参数。可以直接扔掉。那么优化目标可以写作：&lt;/p&gt;
$$
\theta^* = \arg \max_\theta \mathbb{E}_{x\sim p(x)}\left[\log p_\theta(x)\right]
$$&lt;p&gt;离散的，对于一批数据 $\{x_1, x_2, \cdots, x_n\}$，我们可以将优化目标改写为：&lt;/p&gt;</description></item><item><title>Denoising Score Matching</title><link>https://wangjv0812.cn/2025/10/denoising-score-matching/</link><pubDate>Mon, 13 Oct 2025 20:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/10/denoising-score-matching/</guid><description>&lt;h2 id="1-动机"&gt;1. 动机&lt;/h2&gt;
&lt;p&gt;以数据生成为代表的自监督学习往往希望设计出一种独特且有效的机制，通过网络结构和训练方法的设计，迫使模型找到代表一个数据最核心和关键的信息或者说特征，或者希望让模型自己总结出数据的内在结构。或者用一个更概率的表达，就像是之前我们在 &lt;a href="https://wangjv0812.cn/2025/08/noise-contrastive-estimation/"&gt;NCE 中对于数据流形&lt;/a&gt; 讨论过的。数据是一个隐藏在高维空间中的低维流形，而概率分布恰好为我们提供了一个方便的描述流形的数学工具。&lt;/p&gt;
&lt;p&gt;我们假设真实数据有概率分布 $p(x)$，而我们希望寻找一个收到参数 $\theta$ 控制的概率分布 $q(x\mid \theta)$，尽可能的接近真实分布。生成模型学习事实上希望解决两个实质性的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们不知道真实分布 $p(x)$，只有对于 $p(x)$ 的一系列采样 $\{x_1, x_2, \cdots x_n\}$（就是我们的数据集），如何利用这些采样尽可能好的找到一组参数 $\theta$，使得 $q(x\mid \theta)$ 尽可能接近 $p(x)$。&lt;/li&gt;
&lt;li&gt;对于一个完成学习的分布 $p(x\mid \theta)$，如何对其采样，获得一组新的数据。进一步将，如何让采样满足一定的条件。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这两个问题看说来容易，但做起来却何其难。从次引出了巨量的问题，例如如何规避分布的归一化系数、如何避免学习一个恒等映射（例如 AutoEncoder）、如何避免只学到一个很窄的分布（SM）等等。归一化系数我们之前在 &lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 中讨论过。为了 DSM 叙述的连贯性，我们不妨先从 Autoencoder 的缺陷和改进聊起。&lt;/p&gt;
&lt;h3 id="11-denoising-autoencoder"&gt;1.1. Denoising AutoEncoder&lt;/h3&gt;
&lt;p&gt;AutoEncoder 是一个非常直觉的无监督学习方法。它基于一个很直觉的认识：无监督学习希望学习一条分布在高维空间中的低维流形。那么如果我们使用一个维度恰好为低维流形独立维度的瓶颈层来强迫模型学习一个有效的数据压缩和恢复，是否恰好可以提取出数据最根本的内在结构。但是这个方法依然有很多缺陷，例如模型学习到的 latent space 不具有连续性，无法直接插值（这个问题被 VAE 解决了），模型很容易学到一个恒等映射等等。&lt;/p&gt;
&lt;p&gt;为了解决恒等映射这个问题，DAE 的思路是：如果简单的要求模型自己通过 编码-解码 的方式破坏重建数据无法保证模型学到可靠的特征，那么何不我来破坏呢？我们直接给数据添加噪声，将带有噪声的数据输入编码器，让模型恢复出没有噪声的，源初的数据。&lt;/p&gt;
&lt;p&gt;形式化的讲，对于数据 $x$，我们添加服从高斯分布的噪声 $\epsilon \sim \mathcal N(x\mid 0, \sigma^2I)$，有被污染的数据：&lt;/p&gt;
$$
\tilde{x} = x + \epsilon
$$&lt;p&gt;那么，损失可以写作：&lt;/p&gt;
$$
J_{DATA}(\theta) = \mathbb{E}\left(\left\|
\text{Decoder}(\text{Encoder} (x + \epsilon)) - x
\right\|^2\right)
$$&lt;h3 id="12-score-matching"&gt;1.2. Score Matching&lt;/h3&gt;
&lt;p&gt;我们之前在 &lt;a href="https://wangjv0812.cn/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 中讨论过 Score Matching 的基本原理。这里简单回顾一下。Score Matching 最核心的创新是学习分布的 Score Function，而不是直接学习分布本身。学习 Score Function 最核心的优势是，我们对 Score Function 的形式没有任何要求，可以用任意一个神经网络拟合，从本质上解决了归一化系数的问题。希望学习到分布的 Score Function 最直接的方式，即使直接使用 &lt;a href="https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/"&gt;Fisher Divergence&lt;/a&gt;。&lt;/p&gt;</description></item><item><title>Fisher Information and Fisher Divergence</title><link>https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/</link><pubDate>Sat, 13 Sep 2025 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/09/fisher-information-and-fisher-divergence/</guid><description>&lt;p&gt;在开始长篇大论之前，不妨先对费雪信息 (Fisher Information) 和 费雪散度 (Fisher Divergence) 有一个先验的、直观的理解。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fisher Information 衡量的是对于一个概率分布模型，它的参数有多么敏感或者说确定。信息量越大，我们用数据来估计这个参数时就越有信心。&lt;/li&gt;
&lt;li&gt;Fisher Divergence 衡量的是两个不同的概率分布，它们的“形状”有多么相似。散度越小，两个分布越接近。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面则给出一些不那么直观的，数学形式上的解释。&lt;/p&gt;
&lt;h2 id="1-statistical-manifold"&gt;1. Statistical Manifold&lt;/h2&gt;
&lt;p&gt;和之前我们讨论的数据分布流形一样，我们可以认为，一种类别的概率分布（例如高斯分布），控制分布的参数同样可以构成一个流形。我们不妨就拿高斯分布举例子，对于一个标准的一维高斯分布，其受到参数 $\sigma^2, \mu$ 控制。那么所有的高斯分布的参数 $\sigma^2, \mu$ 所构成的空间便形成一个 “统计流形”。&lt;/p&gt;
&lt;p&gt;那么如果对于一族分布（或者任意分布），我们希望测量两个分布的差异（这在 Learning 中是十分常用的，可以度量两个分布的差异，就可以驱动优化）。定义分布的差异事实上就是希望可以在统计流形上定义一个有效的度量。&lt;/p&gt;
&lt;h2 id="2-score-function"&gt;2. Score Function&lt;/h2&gt;
&lt;p&gt;对于一个数据集 $\{x_1, x_2, \cdots, x_n\}$，我们假设其服从于一个理想的概率分布 $p(x)$，我们不知道 $p(x)$ 的具体形式，只知道它的一系列采样（就是数据集）。我们希望可以通过一个受到参数 $q(x, \theta)$ 控制的分布族来近似它。那么一个很容易想到办法是，通过 KL 散度来衡量 $p(x)$ 和 $q(x, \theta)$ 之间的差异：&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(p(x) \| q(x, \theta))
&amp;= \mathbb{E}_{x\sim p(x)} \bigg[\log p(x) -\log q(x, \theta)\bigg]\\
&amp;= \mathbb{E}_{x\sim p(x)} \bigg[\log p(x)\bigg] - \mathbb{E}_{x\sim p(x)} \bigg[\log q(x, \theta)\bigg]\\
\end{aligned}
$$&lt;p&gt;其中 $\mathbb{E}_{x\sim p(x)} \bigg[\log p(x)\bigg]$ 是 $p(x)$ 的熵，与待优化参数 $\theta$ 无关，因此：&lt;/p&gt;</description></item><item><title>Noise Contrastive Estimation</title><link>https://wangjv0812.cn/2025/08/noise-contrastive-estimation/</link><pubDate>Mon, 18 Aug 2025 18:08:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/08/noise-contrastive-estimation/</guid><description>&lt;h2 id="1-动机"&gt;1. 动机&lt;/h2&gt;
&lt;p&gt;在无监督学习时，我们往往都需要处理维度非常大的数据。例如无监督学习的一个经典案例：图像生成。对于一个尺寸为 $1920 \times 1080$ 的图片，其像素总量为 $2073600$，生成一张这样的照片时，我们需要对一个维度为 $2073600$ 的随机分布建模和采样。这些维度之间不可能是完全独立的，这个原因很显然，如果所有的维度完全独立，生成的数据就是完全随机的，不会包含任何信息。相邻像素的颜色、纹理高度相关，真正需要被建模的自由度远小于像素个数&lt;/p&gt;
&lt;p&gt;可以说，我们希望通过无监督学习学习到的数据之间的规律，就建模在数据维度之间的约束中，或者换一个更常用的说法，数据之间的规律。由于数据维度之间约束的存在，数据的维度一定是小于（甚至可以说远远小于）其随机向量的维度。我们希望建模的数据事实上存在于一个高维空间上的流形上，而无监督学习实质上是通过神经网络，建模这个高位空间中数据分布的流形。&lt;/p&gt;
&lt;p&gt;有了上面的理解，原本的如何从数据中挖掘关系这个问题就被转换为如何对数据所在的流形建模。这个问题并不容易，流形的复杂性和高维数据的稀疏性都给建模带来了挑战。目前一个主流的思路是通过概率分布对流形建模。显然，概率分布可以很方便的表达流形上的几何结构；对于一个维度为 $d$ 的概率分布，可以理解为一个从 $R^d \to R$ 的映射，当然这个映射需要满足非负和归一化。那么对于不属于流形上的点，映射到 $0$ 就好了。当然，概率分布带给我们的好处远不止于此：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;概率分布本身可以很方便的处理噪声，可以很方便的用于处理真实的，带噪声的真实数据。&lt;/li&gt;
&lt;li&gt;概率分布的采样和优化十分方便，有很多现成的研究成果&lt;/li&gt;
&lt;li&gt;概率分布本身赋予流形一个 “软边界”。这让模型的泛化能力有保障。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但是概率模型依然不是完美的。一般而言，我们假设存在一个理想分布 $p_d(x)$，它表达了完美的，真实的数据的分布。这是一个 “可望而不可达” 的理想分布，我们所用的数据集 $x_1, x_2, \cdots x_n$ 可以认为从分布 $p_d(x)$ 中做的采样。（一种柏拉图式的哲学）。我们希望可以通过一个受到参数 $\theta$ 控制的模型分布 $p(x, \theta)$ 来逼近和代替真实分布 $p_d(x)$。&lt;/p&gt;
&lt;p&gt;我们的神经网络不可能直接建模非归一化模型，模型本身几乎一定是非归一化的。假设我们只能建模一个非归一化模型 $q(x\mid \theta)$，则需要通过归一化系数将其转换为归一化的。&lt;/p&gt;
$$
\begin{aligned}
p(x\mid \theta) = \frac{1}{Z(\theta)}q(x\mid \theta)\\
\text{where: } Z(\theta) = \int q(x\mid \theta) dx
\end{aligned}
$$&lt;p&gt;但是归一化系数 $Z(\theta) = \int q(x\mid \theta) dx$ 对于高维分布几乎是不可能直接计算的。我们希望能找到一些办法，避免对归一化系数的直接计算，&lt;code&gt;Noise Contrastive Estimation&lt;/code&gt; 就是为此而提出的一种方法。（当然，其他方法还可以参考 &lt;a href="https://wangjv0812.github.io/WangJV-Blog-Pages/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 和 &lt;a href="https://wangjv0812.github.io/WangJV-Blog-Pages/2024/12/dreamfusion/#1-%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90"&gt;使用神经网路进行数据生成&lt;/a&gt;）&lt;/p&gt;
&lt;h2 id="2-通过比较来估计密度density-estimation-by-comparison"&gt;2. 通过比较来估计密度（Density Estimation by Comparison）&lt;/h2&gt;
&lt;p&gt;当我们希望从一系列离散的，从一个未知分布的采样 $\{X\}_n$ 估计出具体的分布时，我们几乎一定会去提取分布的中特征。或者换句话说，我们是通过刻画分布的性质来估计未知分布的密度的。甚至可以说刻画其性质是第一性的。因此一个显然的思路是，如果我们提供一个已知的、与目标分布显著不同的噪声分布 $p_n(x)$。之后训练一个神经网络，用于区分数据来自于数据分布 $p_d(x)$ 还是噪声分布 $p_n(x)$，神经网络自然而然的就学习到了数据分布的特征。这个过程实质上就实现了数据生成的目标，即学习数据的潜在分布。而这个过程则将原本的无监督学习转换为一个简单的，有负样本的，简单的而分类问题。换句话说，&lt;strong&gt;NCE不直接估计概率密度，而是通过学习区分“真实数据”和“人工产生的噪声”来间接地学习模型参数。&lt;/strong&gt;&lt;/p&gt;</description></item><item><title>Sliced Score Matching</title><link>https://wangjv0812.cn/2025/08/sliced-score-matching/</link><pubDate>Mon, 11 Aug 2025 17:02:00 +0800</pubDate><guid>https://wangjv0812.cn/2025/08/sliced-score-matching/</guid><description>&lt;h2 id="1-目的和动机"&gt;1. 目的和动机&lt;/h2&gt;
&lt;p&gt;在之前的关于 &lt;a href="https://wangjv0812.github.io/WangJV-Blog-Pages/2025/08/scorematching/"&gt;Score Matching&lt;/a&gt; 的文章中，介绍了 Score Matching 的基本概念和方法。Score Matching 巧妙的引入了 Score Function，避免了直接计算高维随机向量的归一化系数，让估计一个高维分布成为了可能。其 Loss Function 可以写作：&lt;/p&gt;
$$
\begin{aligned}
J(\theta)
&amp;= \text{E}_{\xi \sim p_X(\xi)}\left[
\text{tr} \left(\nabla^2_\xi \log p(\xi, \theta)\right)+ \frac 1 2\left\| \nabla_\xi \log p(\xi, \theta)\right\|^2
\right] \\
&amp;= \text{E}_{\xi \sim p_X(\xi)}\left[
\text{tr} \left(\nabla_\xi \psi(\xi, \theta)\right)+ \frac 1 2\left\| \psi(\xi, \theta)\right \|^2
\right] \\
\end{aligned}
$$&lt;p&gt;但是成为可能不代表它好算。Score Matching 引入了对原始分布的 Hessian Matrix 的迹 $\nabla^2_\xi \log p(\xi, \theta)$ 的计算。显然，这比直接计算归一化系数简单了不少，但是对于一个维度为 $d$ 的随机向量的估计，需要进行 $d$ 次反向传播，者仍然十分困难。更可怕的是，在反向传播的过程中需要计算：&lt;/p&gt;
$$
\frac{\partial}{\partial \theta} \big[ \text{tr} \left(\nabla_\xi \psi(\xi, \theta)\right)\big] = \text{tr} \left(\frac{\partial^2}{\partial \theta \xi} \psi(\xi, \theta)\right)
$$&lt;p&gt;这一项对于数值计算而言就是灾难。在实践中，需要找到一个真实可行的简化方法。人们常常使用的方法有：&lt;/p&gt;</description></item><item><title>ScoreMatching</title><link>https://wangjv0812.cn/2025/08/scorematching/</link><pubDate>Wed, 06 Aug 2025 16:29:40 +0800</pubDate><guid>https://wangjv0812.cn/2025/08/scorematching/</guid><description>&lt;h2 id="1-为什么要用-score-matching"&gt;1. 为什么要用 Score Matching&lt;/h2&gt;
&lt;p&gt;很多是否，我们希望从大量的数据 $x_1, x_2, \cdots x_n$（或者换句话说，从一个随机变量 $X$ 的大量抽象）还原回分布 $p(x)$ 本身。一个很显然的想法是通过一个带有可优化参数 $\theta$ 的函数 $q(x \mid \theta)$ 来还原/近似真实的数据分布。但是优化过程中，想要保证分布的归一化性质并不容易。一个很显然思路时优化完成后通过归一化系数来保证归一化性质：&lt;/p&gt;
$$
\begin{array}{c}
p(x\mid \theta) = \frac{1}{Z(\theta)}q(x\mid \theta)\\
\text{where: } Z(\theta) = \int q(x\mid \theta) dx
\end{array}
$$&lt;p&gt;但是在很多情况下，生成模型需要处理一个极高维度随机向量的概率分布的积分。此时归一化系数 $Z(\theta)$ 的计算几乎是不可能的。（如果实在希望直接计算，可以用数值方法或者 MCMC，但是这类方法同样很难直接计算。）&lt;/p&gt;
&lt;p&gt;要解决归一化问题的办法其实很多，事实上这在随机分布估计中是一个很常见的问题。我们不妨举一些显然的方案，例如 Flow Module、Bolzemann Machine、Variational Autoencoder 等等。那么如果归一化的分布不好处理，我们是否可以找到一个与归一化的概率分布等价的，不需要归一换的形式？答案是肯定的，就是我们后面要介绍的 Score Function 和对应的估计的方法 Score Matching。&lt;/p&gt;
&lt;h2 id="2-score-function"&gt;2. Score Function&lt;/h2&gt;
&lt;p&gt;对于一个受到参数 $\boldsymbol{\theta}$ 控制的，关于随机向量 $\boldsymbol{\xi}$ 的随机分布 $p(\boldsymbol{\xi}, \boldsymbol{\theta})$。我们定义其对数梯度为其 Score Function。形式化的，可以写作：&lt;/p&gt;
$$
\psi (\boldsymbol{\xi}, \boldsymbol{\theta}) =
\begin{pmatrix}
\frac{\partial p(\boldsymbol{\xi}, \boldsymbol{\theta})}{\partial \boldsymbol{\xi}_1}\\
\frac{\partial p(\boldsymbol{\xi}, \boldsymbol{\theta})}{\partial \boldsymbol{\xi}_2}\\
\vdots\\
\frac{\partial p(\boldsymbol{\xi}, \boldsymbol{\theta})}{\partial \boldsymbol{\xi}_n}
\end{pmatrix} =
\begin{pmatrix}
\psi_1(\boldsymbol{\xi}, \boldsymbol{\theta})\\
\psi_2(\boldsymbol{\xi}, \boldsymbol{\theta})\\
\vdots\\
\psi_n(\boldsymbol{\xi}, \boldsymbol{\theta})
\end{pmatrix} =
\nabla_{\boldsymbol{\xi}} \log p(\boldsymbol{\xi}, \boldsymbol{\theta})
$$&lt;p&gt;我们不难发现：&lt;/p&gt;</description></item><item><title>Naive Group Theory</title><link>https://wangjv0812.cn/2025/06/naive-group-theory/</link><pubDate>Wed, 25 Jun 2025 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/06/naive-group-theory/</guid><description>&lt;p&gt;我们知道，李群实质上是在一个微分流形性质的群。可以看到，李群实质上是 &lt;strong&gt;群&lt;/strong&gt; 和 &lt;strong&gt;微分流形&lt;/strong&gt; 的交集。想要搞明白微分流形是什么并不容易，这需要学习关于微分几何的知识。但是幸运的是，李群研究研究并没有那么依赖于微分流形的知识（事实上这样说并不准确，但是我们尽量不涉及）。和微分几何相比，群的知识就简单的多了。只要捋清概念，即便是中学生也可以明白。&lt;/p&gt;
&lt;p&gt;事实上群被描述为一个带有一个运算（或者说二元关系）的集合，这个集合和其上定义的二元运算需要满足四个基本性质。我们将集合标记为 $C$，二元运算为 $[\cdot\ ,\ \cdot]$。需要满足的性质为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;封闭性：$\forall c_1, c_2 \in C, [c_1, c_2] \in C$&lt;/li&gt;
&lt;li&gt;结合律：$\forall c_1, c_2, c_3 \in C, [[c_1, c_2], c_3 ] = [c_1, [c_2, c_3]]$&lt;/li&gt;
&lt;li&gt;单位元：$\forall c \in C, \exists e \in C, \text{ s.t. } ce = ec = c$&lt;/li&gt;
&lt;li&gt;逆元：$\forall c_1 \in C, \exists c_2 \in C, c_1c_2 = c_2c_1 = e$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在研究群的性质时，我们需要清晰的认识到 &lt;strong&gt;集合&lt;/strong&gt; 和 定义在集合上的 &lt;strong&gt;二元运算&lt;/strong&gt; 是同样重要的。最初提出群这个概念是诶了解决对称性问题，这种对称性关系实质上是研究一种数学结构上的 “&lt;strong&gt;操作不变形&lt;/strong&gt;”。即在一个元素操作前后的结果是完全相同的，我们就称这两个元素在操作上是 “对称” 的。例如对于一个球，在任意元素在球心上做“旋转” 操作，球本身是完全不变的，我们可以称“球”构成的集合 在 “过圆心旋转” 这样操作下，是对称的。&lt;/p&gt;
&lt;p&gt;另一个很经典的例子是用群来描述等边三角形的旋转不变形。但是这个例子我们后面再补充&lt;/p&gt;
&lt;h2 id="1-群的结构和基本操作"&gt;1. 群的结构和基本操作&lt;/h2&gt;
&lt;h3 id="11-子群"&gt;1.1. 子群&lt;/h3&gt;
&lt;p&gt;对于集合 $G$ 的一个子集 $H \subset G$，在群 $G$ 定义的运算律 $\cdot$ 上满足群的性质，就称 $H$ 为 $G$ 的子群。不难察觉到，单位元 $e$ 一定在 $H$ 上。例如任意通过坐标原点的直线都可以看作定义在加法上的对 $R(2)$ 的子群。&lt;/p&gt;</description></item><item><title>3D Kinematics and Dynamics</title><link>https://wangjv0812.cn/2025/06/3d-kinematics-and-dynamics/</link><pubDate>Fri, 20 Jun 2025 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/06/3d-kinematics-and-dynamics/</guid><description>深入探讨三维空间中的运动学和动力学基础，包括旋转矩阵、四元数和李群理论</description></item><item><title>From Transformer to VGGT</title><link>https://wangjv0812.cn/2025/04/from-transformer-to-vggt/</link><pubDate>Fri, 18 Apr 2025 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/04/from-transformer-to-vggt/</guid><description>&lt;h2 id="1-preliminary-attention-and-vit"&gt;1. Preliminary: Attention and ViT&lt;/h2&gt;
&lt;p&gt;我们先来回顾一下经典的 Transformer 结构，之后从 Transformer 的角度来理解 ViT，这样大家能更好的理解 VGGT 和 MASt3R、DUSt3R 之类工作的苦恼之处。&lt;/p&gt;
&lt;p&gt;&lt;img alt="VGGT Attention pipline" loading="lazy" src="https://wangjv0812.cn/2025/04/from-transformer-to-vggt/Images/VGGT%20Attention%20pipline.png"&gt;&lt;/p&gt;
&lt;h3 id="11-encoder-and-decoder"&gt;1.1. Encoder and Decoder&lt;/h3&gt;
&lt;p&gt;深度学习名著 《&lt;a href="https://arxiv.org/pdf/1706.03762"&gt;Attention is all you need&lt;/a&gt;》 提出的古典派 Attention（这么说是因为由于 Transformer 的大火，Attention 机制的变种已经太多了，我们只关注最经典的架构就好，其他都大同小异）。最经典的 Transformer 致力于解决翻译问题，是一个十分经典的 nlp 问题，采用了最经典的 Encoder-Decoder 结构。&lt;/p&gt;
&lt;p&gt;Encoder 由 6 个完全相同的层堆叠而成，每个层由两个子层组成。第一个层负责实现 multi-head self-attention 机制；第二个层是一个简单的全连接前馈网络。为了避免在训练中出现梯度消失的问题，Transformer 在子层间采用了残差链接，之后对子层的输出做归一化（即 Add&amp;amp;Norm）那个块。因此，每个子层的输出可以表示为：&lt;/p&gt;
$$
\text{LayerNorm}(x+\text{Sublayer}(x))
$$&lt;p&gt;其中 $\text{Sublayer}(x)$ 是每个子层具体的实现。为了方便残差链接，每层的输出和输入（包括 embedding layers）都被约定为 $d_{module} = 512$。（至少 Attention is all you need 是这样的）。&lt;/p&gt;
&lt;p&gt;Decoder 也是由完全相同的层堆叠而成，和 Encoder 不同的是 Decoder 的每个层也有三个子层。Decoder 的第一个子层是 masked multi-head self-attention，负责对输入的 embedding 做自注意力机制，之后将输入的 embedding 和 encoder 编码的结合结合起来。后面的层和 Encoder 一样，都是 multi-head self-attention 和前馈网络的组合。&lt;/p&gt;</description></item><item><title>DUSt3R and MUSt3R</title><link>https://wangjv0812.cn/2025/03/dust3r-and-must3r/</link><pubDate>Mon, 10 Mar 2025 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2025/03/dust3r-and-must3r/</guid><description>&lt;h2 id="1-dust3r"&gt;1. DUSt3R&lt;/h2&gt;
&lt;h3 id="11-introduction"&gt;1.1. Introduction&lt;/h3&gt;
&lt;p&gt;一般而言，现代的 MVS 和 SFM 的流程总是可以总结为以下几个子问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征点匹配&lt;/li&gt;
&lt;li&gt;寻找本质矩阵&lt;/li&gt;
&lt;li&gt;对点进行三角测量&lt;/li&gt;
&lt;li&gt;对场景进行稀疏重建&lt;/li&gt;
&lt;li&gt;估计相机参数，&lt;/li&gt;
&lt;li&gt;密集重建&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是在这个复杂的过程中，每个子问题都对原始问题做了简化，无法完美解决，为后面的步骤引入了噪声，从而导致整个系统显的“精致而脆弱”。在这方面，每个子问题之间缺乏沟通就很能说明问题：如果能将这些缓解紧耦合到一起，将噪声统一的，全局的考虑，可以很大程度上解决应为过度简化和解耦导致的种种问题。此外，这个流程中的关键步骤很脆弱，在很多情况下容易出错。例如，很多 SFM 方法都依赖于相机参数的估计，但是如果遇到观察比较少、非漫反射表面或者相机姿态运动较为单一时，相机参数估计可能失效，导致整个 SFM 过程都会失效。归根结底：&lt;strong&gt;一个多视图立体视觉（MVS）算法的性能仅取决于输入图像和相机参数的质量&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;事实上，单张图或者多张图哦通过深度学习的方式提取深度并不罕有。但是在不引入额外的先验信息时，这个问题往往是&lt;strong&gt;不适定的&lt;/strong&gt;，所以这些方法利用神经网络从大量数据中学习巨量的三维先验知识来解决模糊性问题。这些方法可以分为两类。第一类利用类别级别的物体先验知识，事实上 DreamFusion 就属于这类工作，可以从单张照片或者一句自然语言描述生成三纬结构。另一种与 DUSt3R 较为类似，系统的学习一般的场景来实现单目深度估计。但是一般而言，例如 SuperGlue 之类，在训练和推理过程中，都没有显然的引入三维结构的信息，也没有扔掉相机矩阵的桎梏。可以说，DUSt3R 是一种基于深度学习的 ALL in One 的深度估计方法，入了点图（Point Map）表示，使网络能够在规范框架中预测三维形状。&lt;/p&gt;
&lt;h3 id="12-method-and-forward"&gt;1.2. Method and forward&lt;/h3&gt;
&lt;h4 id="121-ponit-map"&gt;1.2.1. Ponit Map&lt;/h4&gt;
&lt;p&gt;接下来，我们将图片中每个像素对应的三维点构成的集合称为 &lt;strong&gt;点图&lt;/strong&gt;（point map） $X \in R^{W×H×3}$。与分辨率为 $W×H$的对应RGB图像 $I$相关联，&lt;strong&gt;点图 $X$ 在图像像素与三维点之间形成一一映射&lt;/strong&gt;，即对于所有像素坐标 $(i, j) \in \{1...W\}×\{1...H\}$，都有 $I_{i,j} \leftrightarrow X_{i,j}$。此处每个像素点对应于一个三维点实事丧引入了一个简化假设，即假设观测的场景全部是不透明且漫反射的，不存在透过某个物体并观察到另一个物体的情况。&lt;/p&gt;
&lt;h4 id="122-相机和场景"&gt;1.2.2. 相机和场景&lt;/h4&gt;
&lt;p&gt;相机与场景。给定相机内参 $K \in \mathbb{R}^{3 ×3}$ ，所观测场景的点图 $X$ 可以直接从真实深度图：&lt;/p&gt;
$$
D \in \mathbb{R}^{W ×H}
$$&lt;p&gt;中获取，其公式为&lt;/p&gt;
$$
\begin{aligned}
X_{i, j}
&amp;= K^{-1}D_{i,j}[i , j , 1]^{\top}\\
\end{aligned}
$$&lt;p&gt;其中， $X$ 是在相机坐标系中表示的。接下来，我们将在相机 $m$ 的坐标系中表示的、来自相机 $\pi$ 的点图 $X^{n}$ 记为 $X^{n, m}$，表示第 $n$ 帧在 第 $m$ 帧的坐标系下的点图为 ：&lt;/p&gt;</description></item><item><title>homography</title><link>https://wangjv0812.cn/2025/02/homography/</link><pubDate>Fri, 14 Feb 2025 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2025/02/homography/</guid><description>&lt;p&gt;假设我们有两个坐标系 $\mathcal F_a, \ \mathcal F_b$，有一个点 $P$ 在一个平面上。在两个坐标系下，这个点可以描述为 $\rho_a, \rho_b$；对应的平面可以通过法向量和截距来描述：$\{n_a. d_a\}, \{n_b. d_b\}$。&lt;/p&gt;
&lt;p&gt;此外，该点有在图像坐标系下的描述 $p_a, p_b$ 和对应的相机矩阵 $K_a, K_b$。那么可以写出：&lt;/p&gt;
$$
\begin{aligned}
p_a = \frac{1}{z_a} K_a \rho_a \\
p_b = \frac{1}{z_b} K_b \rho_b
\end{aligned}
$$&lt;p&gt;此外， 由于该点在对应的平面上，有平面约束：&lt;/p&gt;
$$
\begin{aligned}
n^T_a \rho_a + d_a = 0 \\
n^T_b \rho_b + d_b = 0
\end{aligned}
$$&lt;p&gt;那么，将平面约束中的 $\rho$ 通过投影矩阵转换为像素坐标，有：&lt;/p&gt;
$$
\begin{aligned}
&amp;z_a n^T_a K_a^{-1} p_a + d_a = 0\\
&amp;z_a = -\frac{d_a}{n^T_a K_a^{-1} p_a}\\
&amp;z_b n^T_b K_b^{-1} p_b + d_b = 0\\
&amp;z_b = -\frac{d_b}{n^T_b K_b^{-1} p_b}\\
\end{aligned}
$$&lt;p&gt;带入到 $\rho_a, \rho_b$ 的表达式中，有：&lt;/p&gt;</description></item><item><title>DreamFusion</title><link>https://wangjv0812.cn/2024/12/dreamfusion/</link><pubDate>Wed, 18 Dec 2024 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2024/12/dreamfusion/</guid><description>&lt;h2 id="1-使用神经网路进行数据生成"&gt;1. 使用神经网路进行数据生成&lt;/h2&gt;
&lt;p&gt;使用神经网络生成一个高维度数据是机器学习中非常重要的一个工作。我们假设数据集 $\left\{\boldsymbol{x_1}, \boldsymbol{x_2}, \dots, \boldsymbol{x_n}\right\}$ 为一个大小为$n$的数据集，该数据集统一的服从一个概率分布 $p_{data}(\boldsymbol{x})$ 。我们假设对数据集的抽样都是独立同分布的，即：&lt;/p&gt;
$$
\left\{\boldsymbol{x_1}, \boldsymbol{x_2}, \dots, \boldsymbol{x_n}\right\} \sim p_{data}(\boldsymbol{x})
$$&lt;p&gt;那么丛现有数据生成新的数据的核心就是使用神经网络学习这个概率分布。不妨假设学习的概率分布为 $\hat p_\theta(\boldsymbol x)$。我们会希望 $\hat p_\theta(\boldsymbol x)$ 尽可能的接近 $p_{data}(\boldsymbol(x))$ 。为了衡量真是分布和我们学习的分布之间的差距，我们需要定义一个距离函数 $D(\cdot \mid \cdot)$ 我们可以定义优化目标：&lt;/p&gt;
$$
\hat \theta = \arg \min_{\theta} D\left(p_{data}(\boldsymbol{x}) \mid \hat p_\theta(\boldsymbol x) \right)
$$&lt;p&gt;关于距离函数，我们可以定义 $D(\cdot \mid \cdot)$ 为 f-divergence 定义为：&lt;/p&gt;
$$
D_f(p_{data}(\boldsymbol(x)) \mid \hat p_\theta(\boldsymbol x)) = \int p_\theta(\boldsymbol x) f \left(\frac{p_{data}(\boldsymbol x)}{p_\theta(\boldsymbol x)}\right) d\boldsymbol x
$$&lt;p&gt;不妨取 $f(x) = x\log x$ ，我们可以得到 KL 散度：&lt;/p&gt;
$$
\begin{aligned}
D_{KL}(p_{data}(\boldsymbol(x)) \mid \hat p_\theta(\boldsymbol x))
&amp;= \int p_{data}(\boldsymbol x) \log \frac{p_{data}(\boldsymbol x)}{p_\theta(\boldsymbol x)} d\boldsymbol x\\
&amp;= \mathbb E_{p_{data}(\boldsymbol x)}\left[ \log \frac{p_{data}(\boldsymbol x)}{p_\theta(\boldsymbol x)} \right]
\end{aligned}
$$&lt;p&gt;我们可以用抽样的均值来代替期望，有：&lt;/p&gt;</description></item><item><title>Kalman_filter</title><link>https://wangjv0812.cn/2024/11/kalman_filter/</link><pubDate>Mon, 04 Nov 2024 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2024/11/kalman_filter/</guid><description>&lt;p&gt;ps: 为了更快的写出来这个文档，我不会很注意公式的粗细体，请见谅。&lt;/p&gt;
&lt;h2 id="1-最大后验估计"&gt;1. 最大后验估计&lt;/h2&gt;
&lt;h3 id="11-状态估计问题描述"&gt;1.1. 状态估计问题描述&lt;/h3&gt;
&lt;p&gt;我们假设有一个线性系统，其噪声可以用高斯函数来描述。这个线性系统可以如下描述：&lt;/p&gt;
$$
\begin{array}{l}
x_k = A_{k-1}x_{k-1} + v_k + \omega_k\\
y_k = Cx_k + n_k
\end{array}
$$&lt;p&gt;其中，有：&lt;/p&gt;
$$
\begin{array}{ll}
\text{初始噪声} &amp; x_0 \sim \mathcal G (x \mid 0, P_0) \\
\text{过程噪声} &amp; x_k \sim \mathcal G (x \mid 0, Q_k) \\
\text{观测噪声} &amp; \omega_k \sim \mathcal G (x \mid 0, R_k)
\end{array}
$$&lt;p&gt;我们认为除了系统的输入 $v_k$ 之外，其余所有变量皆为随机变量。此外我们称 $A_k$ 为状态转移矩阵，$C_k$ 为观测矩阵。对于这个系统而言，系统的初始状态 $x_0$、系统输入 $v_k$ 和 系统输出是已知的。状态估计的目标就是通过这些已知的参数，估计出系统的状态 $x_k$。&lt;/p&gt;
&lt;h3 id="12-最大后验估计"&gt;1.2. 最大后验估计&lt;/h3&gt;
&lt;p&gt;最大后验估计需要完成如下一个优化问题：&lt;/p&gt;
$$
\hat x = \arg \max_{x} p(x \mid y, v)
$$&lt;p&gt;对于上面这个问题，通过贝叶斯定理，可以变形得：&lt;/p&gt;</description></item><item><title>BARF: Bundle Adjusting Neural Radiance Field</title><link>https://wangjv0812.cn/2024/10/barf-bundle-adjusting-neural-radiance-field/</link><pubDate>Thu, 17 Oct 2024 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2024/10/barf-bundle-adjusting-neural-radiance-field/</guid><description>探讨 BARF 方法：如何在未知相机位姿的情况下优化神经辐射场，实现图像对齐和3D重建</description></item><item><title>Hierarchical Gaussian Splatting</title><link>https://wangjv0812.cn/2024/07/hierarchical-gaussian-splatting/</link><pubDate>Mon, 08 Jul 2024 16:40:25 +0800</pubDate><guid>https://wangjv0812.cn/2024/07/hierarchical-gaussian-splatting/</guid><description>&lt;h2 id="1-abstract--introduction"&gt;1. Abstract &amp;amp; Introduction&lt;/h2&gt;
&lt;p&gt;3D Gaussian Splatting 面临着一个几乎看起来无法规避的问题，就是我们需要给每个高斯函数分配一定的存储空间，并在训练时对其优化；并且在训练和渲染时需要同时将所有的高斯函数加载到设备的现存中，这导致训练和渲染在计算上是十分昂贵的。这导致我们总是要在渲染、重建质量和速度之间作出权衡，甚至很多时候是没办法训练的。这制约了 Splatting 在大场景的工作（例如城市级）上的应用。&lt;/p&gt;
&lt;p&gt;那么一个很显然的想法，就是在较远时提供一个较低的分辨率，实现一个分层级的渲染和训练，并且只加载视角可见的部分。那么需要的方法有两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;引入结构层次（Hierarchy），使用一种高效细节级别解决方案（Level of Detial）。&lt;/li&gt;
&lt;li&gt;引入分置策略（divide-and-conquer），让我们可以在独立的训练和渲染每一个小块。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;同时，通过不同层级的结构（Guassian Function）可以用来优化中间层的高斯函数。这篇文章所提出的策略可以实时的渲染非常大的场景，覆盖长达几公里的轨迹，持续长达一小时。&lt;/p&gt;
&lt;p&gt;&lt;img alt="db286f9b0b818bd938a3ef6ea35d1c7a_0_Figure_1_-1273433434" loading="lazy" src="https://wangjv0812.cn/2024/07/hierarchical-gaussian-splatting/images/db286f9b0b818bd938a3ef6ea35d1c7a_0_Figure_1_-1273433434.png"&gt;&lt;/p&gt;
&lt;h2 id="2-概述和背景"&gt;2. 概述和背景&lt;/h2&gt;
&lt;h3 id="21-背景"&gt;2.1. 背景&lt;/h3&gt;
&lt;p&gt;3DGS 提供了一种基于体积基元的空间场景表达方法，每个体积基元含有如下特征：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;位置（或者说均值$\mu$）&lt;/li&gt;
&lt;li&gt;协方差矩阵$\Sigma$&lt;/li&gt;
&lt;li&gt;透明度（$o$）&lt;/li&gt;
&lt;li&gt;球谐系数（$SH$）用于表达与视角相关的颜色，或者直接使用颜色&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;三维基元可以投影到二维屏幕空间上，并且通过 $\alpha\text{-blander}$ 来实现光栅化。 $\alpha\text{-blander}$ 的权重为：
&lt;/p&gt;
$$
\begin{aligned}
\alpha &amp;= \text{oG}\\
G(x,y) &amp;= \exp
\left\{
-\frac 12 ([x,y]^T-\mu')^T\Sigma'^{-1}([x,y]^T-\mu')
\right\}
\end{aligned}
$$&lt;p&gt;
其中 $\mu'$ 是三维空间基元投影到二维相机平面上基元的均值，$\Sigma'$ 投影的二维基元的协方差。&lt;/p&gt;
&lt;h2 id="3-3dgaussian-的结构化-hierarchy-的细节层次-lod"&gt;3. 3DGaussian 的结构化 (hierarchy) 的细节层次 (LOD)&lt;/h2&gt;
&lt;p&gt;在处理大型场景以允许有效渲染大量内容时，细节级别 (LOD) 解决方案至关重要；因此，我们的目标是创建一个层次结构，表示原始 3DGS 优化生成的原语。遵循图形中的传统LOD方法，我们需要&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;找到候选3DGS基元，并定义如何将它们合并到中间节点&lt;/li&gt;
&lt;li&gt;提供一种有效的方法来确定层次结构中的切割，从而在质量和速度之间提供良好的折衷&lt;/li&gt;
&lt;li&gt;层次结构级别之间的平滑过渡策略&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="31-生成不同分辨率的高斯球"&gt;3.1. 生成不同分辨率的高斯球&lt;/h3&gt;
&lt;p&gt;我们为每个块创建一个具有内部节点和叶节点的基于树的层次结构。每个节点都与一个 3D 高斯相关联，该高斯要么是来自原始优化的叶节点，要么是合并的内部节点。我们对中间节点的要求是它们应该：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;保持与叶节点相同的快速光栅化例程&lt;/li&gt;
&lt;li&gt;尽可能准确地表示子节点的外观。因此，我们需要定义具有 3DGS 原语所有属性的 3D 高斯的中间节点。例如保持它原本所有的特征：均值$\mu$、协方差$\Sigma$、透明度 $o$ 等等。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于均值和协方差，有很多文献详尽的描述了这个混合过程。可以通过如下公式混合 $N$ 个在第 $l$ 级的均值为 $\mu_i^l$，协方差为 $\Sigma_i^l$ 高斯函数。我们可以通过评估这 $N$ 个高斯函数和待估计的高斯函数之间的 3D Kullback-Leibler divergence。3DKL 散度描述了两个高斯函数之间的相关性。那么显然的，假设$f = \sum_{i=1}^{N}\mathcal \alpha_i N(\mu_i, \Sigma_i)$，g为我们所需要新的高斯函数，应该有：
&lt;/p&gt;</description></item><item><title>Mathematics In 3DGS 2</title><link>https://wangjv0812.cn/2024/05/mathematics-in-3dgs-2/</link><pubDate>Fri, 17 May 2024 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2024/05/mathematics-in-3dgs-2/</guid><description>&lt;h2 id="1-矩阵求导的常用方法"&gt;1. 矩阵求导的常用方法&lt;/h2&gt;
&lt;h3 id="11-矩阵求导的一般方法"&gt;1.1. 矩阵求导的一般方法&lt;/h3&gt;
&lt;p&gt;在矩阵论的课程中，我们学习过如下几种分析相关的知识，分别是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;向量对标量求导&lt;/li&gt;
&lt;li&gt;向量对向量求导&lt;/li&gt;
&lt;li&gt;向量对矩阵求导&lt;/li&gt;
&lt;li&gt;矩阵对标量求导&lt;/li&gt;
&lt;li&gt;矩阵对向量求导&lt;/li&gt;
&lt;li&gt;矩阵对矩阵求导&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;事实上不难发现，我们只需要搞明白了矩阵对标量求导和矩阵对矩阵求导的方法，其他问题均可从这个两个原则推理开去。因此我们叙述的重点放在这两个问题上。&lt;/p&gt;
&lt;h4 id="111-矩阵对标量求导"&gt;1.1.1. 矩阵对标量求导&lt;/h4&gt;
&lt;p&gt;假设我们有矩阵 $\mathbf A$ 和标量 $k$，其中矩阵 $\mathbf A$ 的展开形式为：&lt;/p&gt;
$$
\mathbf A=
\left[
\begin{matrix}
a_{11}&amp; a_{12}&amp; \cdots&amp; \ a_{1n} \\
a_{21}&amp; a_{22}&amp; \cdots&amp; \ a_{2n} \\
\vdots&amp; \vdots&amp; \ddots&amp; \vdots \\
a_{n1}&amp; a_{n2}&amp; \cdots&amp; \ a_{nn} \\
\end{matrix}
\right]
$$&lt;p&gt;那么，$\frac{d \mathbf A}{d k}$被定义为：&lt;/p&gt;
$$
\frac{d \mathbf A}{d k} =
\left[
\begin{matrix}
\frac{d a_{11}}{d k}&amp; \frac{d a_{12}}{d k}&amp; \cdots&amp; \ \frac{d a_{1n}}{d k}&amp; \\
\frac{d a_{21}}{d k}&amp; \frac{d a_{22}}{d k}&amp; \cdots&amp; \ \frac{d a_{2n}}{d k}&amp; \\
\vdots&amp; \vdots&amp; \ddots&amp; \vdots \\
\frac{d a_{n1}}{d k}&amp; \frac{d a_{n2}}{d k}&amp; \cdots&amp; \ \frac{d a_{nn}}{d k}&amp; \\
\end{matrix}
\right]
$$&lt;p&gt;于上述定义类似，如果标量对矩阵求导，即$\frac{d k}{d \mathbf A}$，其定义为：&lt;/p&gt;</description></item><item><title>Mathematics In 3DGS 1</title><link>https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/</link><pubDate>Wed, 01 May 2024 17:13:56 +0800</pubDate><guid>https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/</guid><description>&lt;h2 id="1-体渲染"&gt;1. 体渲染&lt;/h2&gt;
&lt;p&gt;体渲染的提出时为了解决如云、烟等非刚体的光学行为。可以理解为用于解决对光学&lt;strong&gt;不是完全反射&lt;/strong&gt;，有复杂&lt;strong&gt;透射&lt;/strong&gt;的光学行为。为了对这个光学行为建模，我们将云团（为了叙述方便，我们后面统一将被渲染物体称为云团）视为一团飘忽不定的粒子。光沿直线方向穿过一堆粒子 (粉色部分)，如果能计算出每根光线从最开始发射，到最终打到成像平面上的辐射强度，我们就可以渲染出投影图像。而渲染要做的就是对这个过程进行建模。为了简化计算，我们就假设光子只跟它附近的粒子发生作用，这个范围就是图中圆柱体大小的区间。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Volumn Rendering" loading="lazy" src="https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/Images/image-20240125001336326.png"&gt;&lt;/p&gt;
&lt;h3 id="11-渲染行为分析"&gt;1.1. 渲染行为分析&lt;/h3&gt;
&lt;p&gt;光线与粒子发生发生的作用有如下几类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;吸收 (absorption)&lt;/strong&gt;：光子被粒子吸收，会导致入射光的辐射强度减弱&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;放射 (emission)&lt;/strong&gt;：粒子本身可能发光，这会进一步增大辐射强度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;外散射 (out-scattering)&lt;/strong&gt;：光子在撞击到粒子后，可能会发生弹射，导致方向发生偏移，会减弱入射光强度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;内散射 (in-scattering)&lt;/strong&gt;：其他方向的光子在撞到粒子后，可能和当前方向上的光子重合，从而增强当前光路上的辐射强度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Volumn Rendering" loading="lazy" src="https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/Images/image-20240125001538229.png"&gt;&lt;/p&gt;
&lt;p&gt;那么对于任意一个云团块而言，出射光与入射光之间的变化量，可以表示为这四个过程的叠加。我们假设入射光线的强度为$I_i$，出射光线为$I_o$，那么有：&lt;/p&gt;
$$
l_o-\mathrm{I}_i= dL(x,\omega) =emission+inscattering-outscatting-absorption
$$&lt;p&gt;下面针对吸收、发射、内散射、外散射四个环节进行分析。&lt;/p&gt;
&lt;h4 id="111-吸收"&gt;1.1.1 吸收&lt;/h4&gt;
&lt;p&gt;我们假设半透明物体中的每个粒子的半径为$r$， 每个粒子的投影面积为$A=$ $\pi r^2$， 并假设圆柱体中粒子的密度为$\rho$，圆柱体的底面积是$E$，并且圆柱体的厚度足够薄。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Volumn Rendering" loading="lazy" src="https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/Images/image-20240125003153333.png"&gt;&lt;/p&gt;
&lt;p&gt;假定这个厚度是$\Delta s$，那么在这个厚度内，圆柱体体积为$E\Delta s$，粒子总数为$\rho E \Delta s$。这些粒子遮挡的面积为$\rho E \Delta s A$，占整个底面积的比例为$\rho E\Delta sA/E=\rho A\Delta s_{\mathrm{o}}$。也就是说，当一束光通过这个圆柱体的时候，有$\rho A\Delta s$的概率会被遮挡。&lt;/p&gt;
&lt;p&gt;换句话说，如果我们在圆柱体的一端发射无数光线 (假设都朝相同的方向)，在另一端接收，会发现有些光线安然通过，有些则被粒子遮挡 (吸收)。但可以确定的是，这些接受到的光线总强度，相比入射光线总强度而言，会有$\rho A\Delta s$比例的衰减，即接受到的光的强度均值是入射光的$\rho A\Delta s$倍。其数学形式可以写作：&lt;/p&gt;
$$
I_0 - I_i = \Delta I = -\rho(s)AI(s)\Delta s
$$&lt;p&gt;这是一个关于粒子密法$\rho$和$s$的函数，在空间中每个位置的密度是不同的。我们将上面的薄的圆柱体仍为时一个微元，那么可以将其转化为微分方程：&lt;/p&gt;
$$
\frac{dI}{ds}=-\rho(s)AI(s)=-\tau_{a}(s)I(s)
$$&lt;p&gt;那么有：&lt;/p&gt;
$$
I(s)=I_{0}\exp(-\int_{0}^{s}\tau_{a}(t)dt)
$$&lt;p&gt;其中$I_o$时表示了光线的起始点。那么针对出射光而言有：&lt;/p&gt;
$$
I_{o}=I_{i}\exp(-\int_{i}^{o}\tau_{a}(t)dt)_{0}
$$&lt;p&gt;此式的物理含义是显而易见的：如果离子云是均匀的，那么射入粒子云的光线会指数衰减，这被称为：比尔-朗伯吸收定律 (Beer-Lambert law)。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Volumn Rendering" loading="lazy" src="https://wangjv0812.cn/2024/05/mathematics-in-3dgs-1/Images/image-20240125003622660.png"&gt;&lt;/p&gt;</description></item></channel></rss>