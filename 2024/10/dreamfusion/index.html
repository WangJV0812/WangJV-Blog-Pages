<!doctype html><html><!doctype html><html><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>DreamFusion - WangJV Blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="DreamFusion"><meta itemprop=description content="1. 图像平面对齐 思考我们有一系列位姿$x$不准确的图像。我们希望找到一个图像的扭曲变换（warp transform）$\mathcal W(x,p)$ 我们知道，两个平面之间的投影变换可以通过homography变换来描述。变换 $\mathcal W(x,p)$ 受到参数 $p$控制。我们希望能通过不断修正参数 $p = p + \Delta p$来修正图像的位姿，实现最终的图像对齐。假设我们有两张照片 $\mathcal I_1(x), \mathcal I_2(x)$这个对齐过程可以通过下面这样一个最优指标来描述：
$$ \min \sum_x \lvert \mathcal I_1(\mathcal W(x,p)) - \mathcal I_2(x) \rvert^2 $$我们讲对 $p$ 的修正 $\mathcal W(x,p)$ 带入$\mathcal I_1(x)$中，并做泰勒展开：
$$ \mathcal I_1(W(x,p+\Delta p)) = \mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p $$带入误差形式中，有：
$$ E(\Delta p) = \lvert \sum_x(\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x))\rvert^2 $$使用梯度下降，有：
$$ \frac{\partial E(\Delta p)}{\partial \Delta p} =2\sum_x J^T(x,p) [\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x)] = 0 $$重新整理，有："><meta itemprop=datePublished content="2024-10-17T16:40:25+08:00"><meta itemprop=dateModified content="2024-10-17T16:40:25+08:00"><meta itemprop=wordCount content="157"><meta property="og:url" content="https://wangjv0812.github.io/WangJV-Blog-Pages/2024/10/dreamfusion/"><meta property="og:site_name" content="WangJV Blog"><meta property="og:title" content="DreamFusion"><meta property="og:description" content="1. 图像平面对齐 思考我们有一系列位姿$x$不准确的图像。我们希望找到一个图像的扭曲变换（warp transform）$\mathcal W(x,p)$ 我们知道，两个平面之间的投影变换可以通过homography变换来描述。变换 $\mathcal W(x,p)$ 受到参数 $p$控制。我们希望能通过不断修正参数 $p = p + \Delta p$来修正图像的位姿，实现最终的图像对齐。假设我们有两张照片 $\mathcal I_1(x), \mathcal I_2(x)$这个对齐过程可以通过下面这样一个最优指标来描述：
$$ \min \sum_x \lvert \mathcal I_1(\mathcal W(x,p)) - \mathcal I_2(x) \rvert^2 $$我们讲对 $p$ 的修正 $\mathcal W(x,p)$ 带入$\mathcal I_1(x)$中，并做泰勒展开：
$$ \mathcal I_1(W(x,p+\Delta p)) = \mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p $$带入误差形式中，有：
$$ E(\Delta p) = \lvert \sum_x(\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x))\rvert^2 $$使用梯度下降，有：
$$ \frac{\partial E(\Delta p)}{\partial \Delta p} =2\sum_x J^T(x,p) [\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x)] = 0 $$重新整理，有："><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-17T16:40:25+08:00"><meta property="article:modified_time" content="2024-10-17T16:40:25+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="DreamFusion"><meta name=twitter:description content="1. 图像平面对齐 思考我们有一系列位姿$x$不准确的图像。我们希望找到一个图像的扭曲变换（warp transform）$\mathcal W(x,p)$ 我们知道，两个平面之间的投影变换可以通过homography变换来描述。变换 $\mathcal W(x,p)$ 受到参数 $p$控制。我们希望能通过不断修正参数 $p = p + \Delta p$来修正图像的位姿，实现最终的图像对齐。假设我们有两张照片 $\mathcal I_1(x), \mathcal I_2(x)$这个对齐过程可以通过下面这样一个最优指标来描述：
$$ \min \sum_x \lvert \mathcal I_1(\mathcal W(x,p)) - \mathcal I_2(x) \rvert^2 $$我们讲对 $p$ 的修正 $\mathcal W(x,p)$ 带入$\mathcal I_1(x)$中，并做泰勒展开：
$$ \mathcal I_1(W(x,p+\Delta p)) = \mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p $$带入误差形式中，有：
$$ E(\Delta p) = \lvert \sum_x(\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x))\rvert^2 $$使用梯度下降，有：
$$ \frac{\partial E(\Delta p)}{\partial \Delta p} =2\sum_x J^T(x,p) [\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x)] = 0 $$重新整理，有："><link rel=stylesheet type=text/css media=screen href=https://wangjv0812.github.io/WangJV-Blog-Pages/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://wangjv0812.github.io/WangJV-Blog-Pages/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://wangjv0812.github.io/WangJV-Blog-Pages/css/dark.css><script src=https://wangjv0812.github.io/WangJV-Blog-Pages/js/main.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]},loader:{load:["ui/safe"]}}</script></head><body><div class="container wrapper"><div class=header><h1 class=site-title><a href=https://wangjv0812.github.io/WangJV-Blog-Pages/>WangJV Blog</a></h1><div class=site-description><nav class="nav social"><ul class=flat></ul></nav></div><nav class=nav><ul class=flat></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>17</span>
<span class=rest>Oct 2024</span></div></div><div class=matter><h1 class=title>DreamFusion</h1></div></div><h2 id=1-图像平面对齐>1. 图像平面对齐</h2><p>思考我们有一系列位姿$x$不准确的图像。我们希望找到一个图像的扭曲变换（warp transform）$\mathcal W(x,p)$ 我们知道，两个平面之间的投影变换可以通过homography变换来描述。变换 $\mathcal W(x,p)$ 受到参数 $p$控制。我们希望能通过不断修正参数 $p = p + \Delta p$来修正图像的位姿，实现最终的图像对齐。假设我们有两张照片 $\mathcal I_1(x), \mathcal I_2(x)$这个对齐过程可以通过下面这样一个最优指标来描述：</p>$$
\min \sum_x \lvert
\mathcal I_1(\mathcal W(x,p)) -
\mathcal I_2(x)
\rvert^2
$$<p>我们讲对 $p$ 的修正 $\mathcal W(x,p)$ 带入$\mathcal I_1(x)$中，并做泰勒展开：</p>$$
\mathcal I_1(W(x,p+\Delta p)) = \mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p
$$<p>带入误差形式中，有：</p>$$
E(\Delta p) = \lvert \sum_x(\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x))\rvert^2
$$<p>使用梯度下降，有：</p>$$
\frac{\partial E(\Delta p)}{\partial \Delta p} =2\sum_x J^T(x,p) [\mathcal I_1(\mathcal W(x,p)) + J(x,p) \Delta p - \mathcal I_2(x)] = 0
$$<p>重新整理，有：</p>$$
\sum_x J^T(x,p)J(x,p)\Delta p = -\sum_x J^T(x,p)\left[\mathcal I_1(\mathcal W(x,p)) - \mathcal I_2(x)\right]
$$<p>我们令 $A(x,p) = J^T(x,p)J(x,p)$，有：</p>$$
\Delta p = -A(x,p)^{-1}\sum_x J^T(x,p)\left[\mathcal I_1(\mathcal W(x,p)) - \mathcal I_2(x)\right]
$$<p>那么很显然的，只要我们能计算出对 warp transform 的参数的修正的导数 $J^T(x,p)$，就可以求出对位姿的修正。我们用链式法则，有：</p>$$
\mathbf{J(x;p)}=\frac{\partial\mathcal{I}_1(\mathcal{W}(\mathbf{x};\mathbf{p}))}{\partial\mathcal{W}(\mathbf{x};\mathbf{p})}\frac{\partial\mathcal{W}(\mathbf{x};\mathbf{p})}{\partial\mathbf{p}}
$$<p>我们主要需要求取 $\frac{\partial\mathcal{I}_1(\mathcal{W}(\mathbf{x};\mathbf{p}))}{\partial\mathcal{W}(\mathbf{x};\mathbf{p})}$。那么后面的工作就很显然来，要想办法求出由 Nerf 渲染出的图像对位置的梯度。</p><h2 id=2-nerf-的表达和优化>2. Nerf 的表达和优化</h2><p>我们假设nerf训练出的 mlp 可以表达为一个函数 $\mathbf{y}=[\mathbf{c};\sigma]^\top=f(\mathbf{x};\boldsymbol{\Theta})$，其中$\boldsymbol{\Theta}$ 是待训练的参数。我们简单的将一个像素点上计算出的参数（该像素点对应的射线上的体渲染参数）记作: $\{\mathbf{y}_1,\ldots,\mathbf{y}_N\}$。体渲染方程我们记作一个函数:</p>$$
\hat{\mathcal{I}}(\mathbf{u})=g\left(\mathbf{y}_1,\ldots,\mathbf{y}_N\right)
$$<p>那么整个形式我们可以写出：</p>$$
\hat{\mathcal{I}}(\mathbf{u};\mathbf{p})=g\Big(f(\mathcal{W}(z_1\bar{\mathbf{u}};\mathbf{p});\mathbf{\Theta}),\ldots,f(\mathcal{W}(z_N\bar{\mathbf{u}};\mathbf{p});\mathbf{\Theta})\Big)
$$<p>梯度可以写出：</p>$$
\mathbf{J}(\mathbf{u};\mathbf{p})=\sum_{i=1}^N\frac{\partial g(\mathbf{y}_1,\ldots,\mathbf{y}_N)}{\partial\mathbf{y}_i}\frac{\partial\mathbf{y}_i(\mathbf{p})}{\partial\mathbf{x}_i(\mathbf{p})}\frac{\partial\mathcal{W}(z_i\bar{\mathbf{u}};\mathbf{p})}{\partial\mathbf{p}}
$$<p>我们总结一下：</p><ol><li>$\frac{\partial g}{\partial y}$ 是体渲染过程的梯度</li><li>$\frac{\partial y}{\partial x}$ 是nerf的mlp神经网络的梯度</li><li>$\frac{\partial W}{\partial p}$ 是wrap变换的梯度</li></ol><hr class=footer-separator><div class=tags></div><div class=back><a href=https://wangjv0812.github.io/WangJV-Blog-Pages/><span aria-hidden=true>← Back</span></a></div><div class=back></div></div></div><div class="footer wrapper"><nav class=nav><div>2024</div></nav></div></body></html>